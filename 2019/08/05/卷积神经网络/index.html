<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="卷积神经网络原理"><meta name="keywords" content="deeplearning,卷积神经网络,cv"><meta name="author" content="pangzibo243"><meta name="copyright" content="pangzibo243"><title>卷积神经网络原理 | pangzibo243's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#人类视觉原理"><span class="toc-text">人类视觉原理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#卷积神经网络"><span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积（Convolution）"><span class="toc-text">卷积（Convolution）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#池化（Pooling）"><span class="toc-text">池化（Pooling）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#反向传播"><span class="toc-text">反向传播</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#卷积神经网络中的trick"><span class="toc-text">卷积神经网络中的trick</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#分组卷积Group-convolution"><span class="toc-text">分组卷积Group convolution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3卷积核"><span class="toc-text">3*3卷积核</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#每层使用多个尺寸的卷积核"><span class="toc-text">每层使用多个尺寸的卷积核</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#减少卷积层参数量–Bottleneck"><span class="toc-text">减少卷积层参数量–Bottleneck</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#训练更深的网络–ResNet"><span class="toc-text">训练更深的网络–ResNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DepthWise操作"><span class="toc-text">DepthWise操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分组卷积对通道进行随机分组–-ShuffleNet"><span class="toc-text">分组卷积对通道进行随机分组– ShuffleNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#给通道加上权重–SENet"><span class="toc-text">给通道加上权重–SENet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#更大的接受域–Dilated-convolution"><span class="toc-text">更大的接受域–Dilated convolution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#可变的卷积核–Deformable-convolutuin"><span class="toc-text">可变的卷积核–Deformable convolutuin</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#启发"><span class="toc-text">启发</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#上采样"><span class="toc-text">上采样</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_faceQ.png"></div><div class="author-info__name text-center">pangzibo243</div><div class="author-info__description text-center">pangzibo243's blog</div><div class="follow-button"><a href="https://github.com/litianbo243">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">17</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">26</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">7</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/chen_sir_1.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">pangzibo243's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">卷积神经网络原理</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-05</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/deeplearning-cv/">deeplearning_cv</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="人类视觉原理"><a href="#人类视觉原理" class="headerlink" title="人类视觉原理"></a>人类视觉原理</h1><p>人类的视觉原理如下：从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。下面是人脑进行人脸识别的一个示例：<br><img src="https://uploader.shimo.im/f/XLoCW79b3MAspRng.png!thumbnail" alt="图片"><br>对于不同的物体，人类视觉也是通过这样逐层分级，来进行认知的：<br><img src="https://uploader.shimo.im/f/cj0ezRSqCJM9K6Mz.png!thumbnail" alt="图片"><br>我们可以看到，在最底层特征基本上是类似的，就是各种边缘，越往上，越能提取出此类物体的一些特征（轮子、眼睛、躯干等），到最上层，不同的高级特征最终组合成相应的图像，从而能够让人类准确的区分不同的物体。</p>
<p>那么我们可以很自然的想到：可以不可以模仿人类大脑的这个特点，构造多层的神经网络，较低层的识别初级的图像特征，若干底层特征组成更上一层特征，最终通过多个层级的组合，最终在顶层做出分类呢？答案是肯定的，这也是许多深度学习算法（包括CNN）的灵感来源。</p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p><img src="https://uploader.shimo.im/f/UHHjYKRWm1EgQ15N.png!thumbnail" alt="图片"><br>这是一个最典型的卷积网络，由卷积层、池化层、全连接层组成。其中卷积层与池化层配合，组成多个卷积组，逐层提取特征，最终通过若干个全连接层完成分类。<br>卷积层完成的操作，可以认为是受<strong>局部感受野</strong>概念的启发，而池化层，主要是为了降低数据维度。<br>综合起来说，CNN通过卷积来模拟特征区分，并且通过卷积的权值共享及池化，来降低网络参数的数量级，最后通过传统神经网络完成分类等任务。</p>
<h2 id="卷积（Convolution）"><a href="#卷积（Convolution）" class="headerlink" title="卷积（Convolution）"></a>卷积（Convolution）</h2><p>卷积运算的定义如下图所示：<br><img src="https://uploader.shimo.im/f/l5EwZDJd9zMTJZlr.png!thumbnail" alt="图片"><br>在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是24种不同的卷积核的示例：<br><img src="https://uploader.shimo.im/f/oliyvDm4iQkaurUl.png!thumbnail" alt="图片"></p>
<h2 id="池化（Pooling）"><a href="#池化（Pooling）" class="headerlink" title="池化（Pooling）"></a>池化（Pooling）</h2><p>池化听起来很高深，其实简单的说就是下采样。池化的过程如下图所示：<br><img src="https://uploader.shimo.im/f/PEK7huWTx0Uhgba7.png!thumbnail" alt="图片"><br>上图中，我们可以看到，原始图片是20x20的，我们对其进行下采样，采样窗口为10x10，最终将其下采样成为一个2x2大小的特征图。<br>之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。<br>之所以能这么做，是因为即使减少了许多数据，特征的统计属性仍能够描述图像，而且由于降低了数据维度，有效地避免了过拟合。<br>在实际应用中，池化根据下采样的方法，分为最大值下采样（Max-Pooling）与平均值下采样（Mean-Pooling）。</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>原理为多元函数微积分中的链式求导规则，通过链式求导规则，把梯度传给每个卷积核的权重，更新使他们学习到新的特征。</p>
<p>pooling层的反向求导为：<br>对于mean pooling，假设pooling的窗大小是2x2, 在forward的时候啊，就是在前面卷积完的输出上依次不重合的取2x2的窗平均，得到一个值就是当前mean pooling之后的值。backward的时候，把一个值分成四等分放到前面2x2的格子里面就好了。如下</p>
<p>forward: [1 3; 2 2] -&gt; [2]<br>backward: [2] -&gt; [0.5 0.5; 0.5 0.5]</p>
<p>max pooling就稍微复杂一点，forward的时候你只需要把2x2窗子里面那个最大的拿走就好了，backward的时候你要把当前的值放到之前那个最大的位置，其他的三个位置都弄成0。如下</p>
<p>forward: [1 3; 2 2] -&gt; 3<br>backward: [3] -&gt; [0 3; 0 0]</p>
<h1 id="卷积神经网络中的trick"><a href="#卷积神经网络中的trick" class="headerlink" title="卷积神经网络中的trick"></a>卷积神经网络中的trick</h1><h2 id="分组卷积Group-convolution"><a href="#分组卷积Group-convolution" class="headerlink" title="分组卷积Group convolution"></a>分组卷积Group convolution</h2><p><img src="https://uploader.shimo.im/f/ovy9juEEbt4N9RVf.png!thumbnail" alt="图片"><br>在AlexNet中被提出，可以节省显存，减少参数量。</p>
<h2 id="3-3卷积核"><a href="#3-3卷积核" class="headerlink" title="3*3卷积核"></a>3*3卷积核</h2><p><img src="https://uploader.shimo.im/f/ivQeqEdaxE43VAPy.png!thumbnail" alt="图片"><br>在VGGNet中被提出，使用3*3的卷积核可以减少参数数量，效果比大卷积核更好。</p>
<h2 id="每层使用多个尺寸的卷积核"><a href="#每层使用多个尺寸的卷积核" class="headerlink" title="每层使用多个尺寸的卷积核"></a>每层使用多个尺寸的卷积核</h2><p><img src="https://uploader.shimo.im/f/EodDBtSKNcEQzAjY.png!thumbnail" alt="图片"><br>在Inception中被提出，事实上，同一层feature map可以分别使用多个不同尺寸的卷积核，以获得不同尺度的特征，再把这些特征结合起来，得到的特征往往比使用单一卷积核的要好。</p>
<h2 id="减少卷积层参数量–Bottleneck"><a href="#减少卷积层参数量–Bottleneck" class="headerlink" title="减少卷积层参数量–Bottleneck"></a>减少卷积层参数量–Bottleneck</h2><p><img src="https://uploader.shimo.im/f/UdhrXzxKj7EXjZVJ.png!thumbnail" alt="图片"><br>在Inception中被提出，如果仅仅引入多个尺寸的卷积核，会带来大量的额外的参数，受到Network In Network中1×1卷积核的启发，为了解决这个问题，他们往Inception结构中加入了一些1×1的卷积核。<br><img src="https://uploader.shimo.im/f/TMtxIb9u0b4BR2xz.png!thumbnail" alt="图片"><br>1×1卷积核也被认为是影响深远的操作，往后大型的网络为了降低参数量都会应用上1×1卷积核。</p>
<h2 id="训练更深的网络–ResNet"><a href="#训练更深的网络–ResNet" class="headerlink" title="训练更深的网络–ResNet"></a>训练更深的网络–ResNet</h2><p><img src="https://uploader.shimo.im/f/jEa0IHU1FKI0s5p2.png!thumbnail" alt="图片"><br>传统的卷积层层叠网络会遇到一个问题，当层数加深时，网络的表现越来越差，很大程度上的原因是因为当层数加深时，梯度消散得越来越严重，以至于反向传播很难训练到浅层的网络。为了解决这个问题，何凯明大神想出了一个“残差网络”，使得梯度更容易地流动到浅层的网络当中去。</p>
<h2 id="DepthWise操作"><a href="#DepthWise操作" class="headerlink" title="DepthWise操作"></a>DepthWise操作</h2><p><img src="https://uploader.shimo.im/f/1g4HpIr3ti8hh3XV.png!thumbnail" alt="图片"><br>标准的卷积过程可以看上图，一个2×2的卷积核在卷积时，对应图像区域中的所有通道均被同时考虑，问题在于，为什么一定要同时考虑图像区域和通道？我们为什么不能把通道和空间区域分开考虑？<br><img src="https://uploader.shimo.im/f/rPO9Qe5JWek5o0Uk.png!thumbnail" alt="图片"><br>Xception网络就是基于以上的问题发明而来。我们首先对每一个通道进行各自的卷积操作，有多少个通道就有多少个过滤器。得到新的通道feature maps之后，这时再对这批新的通道feature maps进行标准的1×1跨通道卷积操作。这种操作被称为 “DepthWise convolution” ，缩写“DW”。<br>这种操作是相当有效的，在imagenet 1000类分类任务中已经超过了InceptionV3的表现，而且也同时减少了大量的参数。<br>因此，一个depthwise操作比标准的卷积操作降低不少的参数量，同时论文中指出这个模型得到了更好的分类效果。</p>
<h2 id="分组卷积对通道进行随机分组–-ShuffleNet"><a href="#分组卷积对通道进行随机分组–-ShuffleNet" class="headerlink" title="分组卷积对通道进行随机分组– ShuffleNet"></a>分组卷积对通道进行随机分组– ShuffleNet</h2><p>在AlexNet的Group Convolution当中，特征的通道被平均分到不同组里面，最后再通过两个全连接层来融合特征，这样一来，就只能在最后时刻才融合不同组之间的特征，对模型的泛化性是相当不利的。为了解决这个问题，ShuffleNet在每一次层叠这种Group conv层前，都进行一次channel shuffle，shuffle过的通道被分配到不同组当中。进行完一次group conv之后，再一次channel shuffle，然后分到下一层组卷积当中，以此循环。<br><img src="https://uploader.shimo.im/f/ifLbL32zb0IOXq4D.png!thumbnail" alt="图片"><br>经过channel shuffle之后，Group conv输出的特征能考虑到更多通道，输出的特征自然代表性就更高。另外，AlexNet的分组卷积，实际上是标准卷积操作，而在ShuffleNet里面的分组卷积操作是depthwise卷积，因此结合了通道洗牌和分组depthwise卷积的ShuffleNet，能得到超少量的参数以及超越mobilenet、媲美AlexNet的准确率！</p>
<h2 id="给通道加上权重–SENet"><a href="#给通道加上权重–SENet" class="headerlink" title="给通道加上权重–SENet"></a>给通道加上权重–SENet</h2><p>无论是在Inception、DenseNet或者ShuffleNet里面，我们对所有通道产生的特征都是不分权重直接结合的，那为什么要认为所有通道的特征对模型的作用就是相等的呢？ 这是一个好问题，于是，ImageNet2017 冠军SEnet就出来了。<br><img src="https://uploader.shimo.im/f/xypuD7ZJRjgTP5Rk.png!thumbnail" alt="图片"><br>一组特征在上一层被输出，这时候分两条路线，第一条直接通过，第二条首先进行Squeeze操作（Global Average Pooling），把每个通道2维的特征压缩成一个1维，从而得到一个特征通道向量（每个数字代表对应通道的特征）。然后进行Excitation操作，把这一列特征通道向量输入两个全连接层和sigmoid，建模出特征通道间的相关性，得到的输出其实就是每个通道对应的权重，把这些权重通过Scale乘法通道加权到原来的特征上（第一条路），这样就完成了特征通道的权重分配。</p>
<h2 id="更大的接受域–Dilated-convolution"><a href="#更大的接受域–Dilated-convolution" class="headerlink" title="更大的接受域–Dilated convolution"></a>更大的接受域–Dilated convolution</h2><p>标准的3×3卷积核只能看到对应区域3×3的大小，但是为了能让卷积核看到更大的范围，dilated conv使其成为了可能。<br><img src="https://uploader.shimo.im/f/aWNbgLa0lT8ds8JO.png!thumbnail" alt="图片"><br>上图b可以理解为卷积核大小依然是3×3，但是每个卷积点之间有1个空洞，也就是在绿色7×7区域里面，只有9个红色点位置作了卷积处理，其余点权重为0。这样即使卷积核大小不变，但它看到的区域变得更大了。</p>
<h2 id="可变的卷积核–Deformable-convolutuin"><a href="#可变的卷积核–Deformable-convolutuin" class="headerlink" title="可变的卷积核–Deformable convolutuin"></a>可变的卷积核–Deformable convolutuin</h2><p><img src="https://uploader.shimo.im/f/TYhXdfeB0XgWIr5c.png!thumbnail" alt="图片"><br>传统的卷积核一般都是长方形或正方形，但MSRA提出了一个相当反直觉的见解，认为卷积核的形状可以是变化的，变形的卷积核能让它只看感兴趣的图像区域 ，这样识别出来的特征更佳。<br><img src="https://uploader.shimo.im/f/0wRgZyGTuFYD6pFv.png!thumbnail" alt="图片"><br>要做到这个操作，可以直接在原来的过滤器前面再加一层过滤器，这层过滤器学习的是下一层卷积核的位置偏移量（offset），这样只是增加了一层过滤器，或者直接把原网络中的某一层过滤器当成学习offset的过滤器，这样实际增加的计算量是相当少的，但能实现可变形卷积核，识别特征的效果更好。</p>
<h2 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h2><p>现在越来越多的CNN模型从巨型网络到轻量化网络一步步演变，模型准确率也越来越高。现在工业界追求的重点已经不是准确率的提升（因为都已经很高了），都聚焦于速度与准确率的trade off，都希望模型又快又准。因此从原来AlexNet、VGGnet，到体积小一点的Inception、Resnet系列，到目前能移植到移动端的mobilenet、ShuffleNet（体积能降低到0.5mb！），我们可以看到这样一些趋势：</p>
<p><strong>卷积核方面：</strong><br>大卷积核用多个小卷积核代替；<br>单一尺寸卷积核用多尺寸卷积核代替；<br>固定形状卷积核趋于使用可变形卷积核；<br>使用1×1卷积核（bottleneck结构）。</p>
<p><strong>卷积层通道方面：</strong><br>标准卷积用depthwise卷积代替；<br>使用分组卷积；<br>分组卷积前使用channel shuffle；<br>通道加权计算。</p>
<p><strong>卷积层连接方面：</strong><br>使用skip connection，让模型更深；<br>densely connection，使每一层都融合上其它层的特征输出（DenseNet）</p>
<p><strong>启发</strong><br>类比到通道加权操作，卷积层跨层连接能否也进行加权处理？bottleneck + Group conv + channel shuffle + depthwise的结合会不会成为以后降低参数量的标准配置？</p>
<h1 id="上采样"><a href="#上采样" class="headerlink" title="上采样"></a>上采样</h1><p>在看图像语义分割方面的论文时，发现在网络解码器结构中有的时候使用反卷积、而有的时候使用unpooling或或者unsampling，查了下资料，发现三者还是有不同的。这里记录一下。<br><img src="https://uploader.shimo.im/f/uTR7gGlFLPUWv40T.png!thumbnail" alt="图片"><br>图（a）表示UnPooling的过程，特点是在Maxpooling的时候保留最大值的位置信息，之后在unPooling阶段使用该信息扩充Feature Map，除最大值位置以外，其余补0。与之相对的是图（b），两者的区别在于UnSampling阶段没有使用MaxPooling时的位置信息，而是直接将内容复制来扩充Feature Map。从图中即可看到两者结果的不同。图（c）为反卷积的过程，反卷积是卷积的逆过程，又称作转置卷积。最大的区别在于反卷积过程是有参数要进行学习的（类似卷积过程），理论是反卷积可以实现UnPooling和unSampling，只要卷积核的参数设置的合理。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">pangzibo243</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://litianbo243.github.io/2019/08/05/卷积神经网络/">https://litianbo243.github.io/2019/08/05/卷积神经网络/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/deeplearning/">deeplearning</a><a class="post-meta__tags" href="/tags/卷积神经网络/">卷积神经网络</a><a class="post-meta__tags" href="/tags/cv/">cv</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_zhifubao_code.png"><div class="post-qr-code__desc">支付宝打赏</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_weixin_code.png"><div class="post-qr-code__desc">微信打赏</div></div></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/08/05/专利申请/"><i class="fa fa-chevron-left">  </i><span>专利申请指南</span></a></div><div class="next-post pull-right"><a href="/2019/08/05/Git使用指南/"><span>Git使用指南</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/chen_sir_1.png)"><div class="layout" id="footer"><div class="copyright">&copy;2019 By pangzibo243</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>