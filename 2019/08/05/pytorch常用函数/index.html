<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="pytorch常用函数"><meta name="keywords" content="deeplearning,pytorch"><meta name="author" content="pangzibo243"><meta name="copyright" content="pangzibo243"><title>pytorch常用函数 | pangzibo243's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Tensor基本操作"><span class="toc-text">Tensor基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor的基本数据类型"><span class="toc-text">Tensor的基本数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor数据类型的转化"><span class="toc-text">Tensor数据类型的转化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor与numpy的相互转换"><span class="toc-text">Tensor与numpy的相互转换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor拼接操作"><span class="toc-text">Tensor拼接操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor进行分块"><span class="toc-text">Tensor进行分块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor进行挤压"><span class="toc-text">Tensor进行挤压</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor进行扩增"><span class="toc-text">Tensor进行扩增</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor进行转置"><span class="toc-text">Tensor进行转置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor进行view操作"><span class="toc-text">Tensor进行view操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机种子"><span class="toc-text">随机种子</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#序列化"><span class="toc-text">序列化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是state-dict"><span class="toc-text">什么是state_dict</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#保存读取模型"><span class="toc-text">保存读取模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数学操作"><span class="toc-text">数学操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#夹紧操作"><span class="toc-text">夹紧操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#沿维度计算和"><span class="toc-text">沿维度计算和</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计算元素均值"><span class="toc-text">计算元素均值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计算元素标准差"><span class="toc-text">计算元素标准差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计算所有元素和"><span class="toc-text">计算所有元素和</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torch-nn"><span class="toc-text">torch.nn</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#容器"><span class="toc-text">容器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#时序容器"><span class="toc-text">时序容器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积层"><span class="toc-text">卷积层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Conv2d"><span class="toc-text">Conv2d</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ConvTranspose2d"><span class="toc-text">ConvTranspose2d</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#池化层"><span class="toc-text">池化层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Maxpool2d"><span class="toc-text">Maxpool2d</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MaxUnpool2d"><span class="toc-text">MaxUnpool2d</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AvgPool2d"><span class="toc-text">AvgPool2d</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaptiveMaxPool2d"><span class="toc-text">AdaptiveMaxPool2d</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#非线性激活函数"><span class="toc-text">非线性激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU"><span class="toc-text">ReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ELU"><span class="toc-text">ELU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PReLU"><span class="toc-text">PReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LeakyReLU"><span class="toc-text">LeakyReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Threshold"><span class="toc-text">Threshold</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid"><span class="toc-text">Sigmoid</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tanh"><span class="toc-text">Tanh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax"><span class="toc-text">Softmax</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#标准化层"><span class="toc-text">标准化层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BatchNorm2d"><span class="toc-text">BatchNorm2d</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#线性层"><span class="toc-text">线性层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Linear"><span class="toc-text">Linear</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dropout层"><span class="toc-text">Dropout层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout"><span class="toc-text">Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Droupout2d"><span class="toc-text">Droupout2d</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#损失函数"><span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MSELoss"><span class="toc-text">MSELoss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CrossEntropyLoss"><span class="toc-text">CrossEntropyLoss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BCELoss"><span class="toc-text">BCELoss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#视觉函数"><span class="toc-text">视觉函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PixelShuffle"><span class="toc-text">PixelShuffle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UpsamplingNearest2d"><span class="toc-text">UpsamplingNearest2d</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UpsamplingBilinear2d"><span class="toc-text">UpsamplingBilinear2d</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#多GPU模块并行"><span class="toc-text">多GPU模块并行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DataParallel"><span class="toc-text">DataParallel</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#优化器"><span class="toc-text">优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Optimizer"><span class="toc-text">Optimizer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adam"><span class="toc-text">Adam</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SGD"><span class="toc-text">SGD</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据集抽象类"><span class="toc-text">数据集抽象类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-utils-data"><span class="toc-text">torch.utils.data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataLoader"><span class="toc-text">DataLoader</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torchvision"><span class="toc-text">torchvision</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#torchvision-datasets"><span class="toc-text">torchvision.datasets</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MNIST"><span class="toc-text">MNIST</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#COCO"><span class="toc-text">COCO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ImageFolder"><span class="toc-text">ImageFolder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torchvision-models"><span class="toc-text">torchvision.models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tansform"><span class="toc-text">tansform</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Compose"><span class="toc-text">Compose</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CenterCrop"><span class="toc-text">CenterCrop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RandomCrop"><span class="toc-text">RandomCrop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RandomHorizantalFlip"><span class="toc-text">RandomHorizantalFlip</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RandomSizeCrop"><span class="toc-text">RandomSizeCrop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pad"><span class="toc-text">Pad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Normalize"><span class="toc-text">Normalize</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ToTensor与ToPILImage"><span class="toc-text">ToTensor与ToPILImage</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_faceQ.png"></div><div class="author-info__name text-center">pangzibo243</div><div class="author-info__description text-center">pangzibo243's blog</div><div class="follow-button"><a href="https://github.com/litianbo243">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">42</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">36</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">10</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/chen_sir_1.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">pangzibo243's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">pytorch常用函数</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-05</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/pytorch/">pytorch</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Tensor基本操作"><a href="#Tensor基本操作" class="headerlink" title="Tensor基本操作"></a>Tensor基本操作</h1><h2 id="Tensor的基本数据类型"><a href="#Tensor的基本数据类型" class="headerlink" title="Tensor的基本数据类型"></a>Tensor的基本数据类型</h2><p>torch.Tensor是一种包含单一数据类型元素的多维矩阵。<br>Torch定义了七种CPU tensor类型和八种GPU tensor类型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Data type</th>
<th style="text-align:center">CPU tensor</th>
<th style="text-align:center">GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">32-bit floating point</td>
<td style="text-align:center">torch.FloatTensor</td>
<td style="text-align:center">torch.cuda.FloatTensor</td>
</tr>
<tr>
<td style="text-align:center">64-bit floating point</td>
<td style="text-align:center">torch.DoubleTensor</td>
<td style="text-align:center">torch.cuda.DoubleTensor</td>
</tr>
<tr>
<td style="text-align:center">16-bit floating point</td>
<td style="text-align:center">N/A</td>
<td style="text-align:center">torch.cuda.HalfTensor</td>
</tr>
<tr>
<td style="text-align:center">8-bit integer (unsigned)</td>
<td style="text-align:center">torch.ByteTensor</td>
<td style="text-align:center">torch.cuda.ByteTensor</td>
</tr>
<tr>
<td style="text-align:center">8-bit integer (signed)</td>
<td style="text-align:center">torch.CharTensor</td>
<td style="text-align:center">torch.cuda.CharTensor</td>
</tr>
<tr>
<td style="text-align:center">16-bit integer (signed)</td>
<td style="text-align:center">torch.ShortTensor</td>
<td style="text-align:center">torch.cuda.ShortTensor</td>
</tr>
<tr>
<td style="text-align:center">32-bit integer (signed)</td>
<td style="text-align:center">torch.IntTensor</td>
<td style="text-align:center">torch.cuda.IntTensor</td>
</tr>
<tr>
<td style="text-align:center">64-bit integer (signed)</td>
<td style="text-align:center">torch.LongTensor</td>
<td style="text-align:center">torch.cuda.LongTensor</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">t = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">print(t.dtype)</span><br><span class="line">print(t)</span><br></pre></td></tr></table></figure>
<h2 id="Tensor数据类型的转化"><a href="#Tensor数据类型的转化" class="headerlink" title="Tensor数据类型的转化"></a>Tensor数据类型的转化</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 默认数据类型为float32</span><br><span class="line">t = torch.Tensor(3, 5)</span><br><span class="line">print(t.dtype, &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># t.double()将该tensor投射为double类型</span><br><span class="line">double_t = t.double()</span><br><span class="line">print(double_t.dtype)</span><br><span class="line"></span><br><span class="line"># t.float()将该tensor投射为float类型</span><br><span class="line">float_t = t.float()</span><br><span class="line">print(float_t.dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># t.half()将tensor投射为半精度浮点类型</span><br><span class="line">half_t = t.half()</span><br><span class="line">print(half_t.dtype)</span><br><span class="line"></span><br><span class="line"># t.long() 将tensor投射为long类型</span><br><span class="line">long_t = t.long()</span><br><span class="line">print(long_t.dtype)</span><br><span class="line"></span><br><span class="line"># t.int()将该tensor投射为int类型</span><br><span class="line">int_t = t.int()</span><br><span class="line">print(int_t.dtype)</span><br><span class="line"></span><br><span class="line"># t.short()将该tensor投射为short类型</span><br><span class="line">short_t = t.short()</span><br><span class="line">print(short_t.dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># t.char()将该tensor投射为char类型</span><br><span class="line">char_t = t.char()</span><br><span class="line">print(char_t.dtype)</span><br><span class="line"></span><br><span class="line"># t.byte()将该tensor投射为byte类型</span><br><span class="line">byte_t = t.byte()</span><br><span class="line">print(byte_t.dtype)</span><br><span class="line"></span><br><span class="line"># t.cuda()将tensor投射为gputensor</span><br><span class="line">gpu_t = t.cuda()</span><br><span class="line">print(gpu_t.dtype)</span><br></pre></td></tr></table></figure>
<h2 id="Tensor与numpy的相互转换"><a href="#Tensor与numpy的相互转换" class="headerlink" title="Tensor与numpy的相互转换"></a>Tensor与numpy的相互转换</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([1, 2, 3])</span><br><span class="line">t = torch.from_numpy(a)</span><br><span class="line">print(t)</span><br><span class="line">t[0] = -1</span><br><span class="line">a = t.numpy()</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<h2 id="Tensor拼接操作"><a href="#Tensor拼接操作" class="headerlink" title="Tensor拼接操作"></a>Tensor拼接操作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = torch.randn(2, 3)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x0 = torch.cat((x, x, x), 0)</span><br><span class="line">print(x0)</span><br><span class="line"></span><br><span class="line">x1 = torch.cat((x, x, x), 1)</span><br><span class="line">print(x1)</span><br></pre></td></tr></table></figure>
<h2 id="Tensor进行分块"><a href="#Tensor进行分块" class="headerlink" title="Tensor进行分块"></a>Tensor进行分块</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = torch.randn(2, 3)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.chunk(x, 2, 0)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<h2 id="Tensor进行挤压"><a href="#Tensor进行挤压" class="headerlink" title="Tensor进行挤压"></a>Tensor进行挤压</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = torch.zeros(2, 1, 2, 1, 2)</span><br><span class="line">print(x.size())</span><br><span class="line"></span><br><span class="line">y = torch.squeeze(x)</span><br><span class="line">print(y.size())</span><br><span class="line"></span><br><span class="line">y = torch.squeeze(x, 0)</span><br><span class="line">print(y.size())</span><br><span class="line"></span><br><span class="line">y = torch.squeeze(x, 1)</span><br><span class="line">print(y.size())</span><br></pre></td></tr></table></figure>
<h2 id="Tensor进行扩增"><a href="#Tensor进行扩增" class="headerlink" title="Tensor进行扩增"></a>Tensor进行扩增</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = torch.zeros(2, 2, 2)</span><br><span class="line">print(x.size())</span><br><span class="line"></span><br><span class="line">y = torch.unsqueeze(x, 0)</span><br><span class="line">print(y.size())</span><br><span class="line"></span><br><span class="line">y = torch.unsqueeze(x, 1)</span><br><span class="line">print(y.size())</span><br></pre></td></tr></table></figure>
<h2 id="Tensor进行转置"><a href="#Tensor进行转置" class="headerlink" title="Tensor进行转置"></a>Tensor进行转置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = torch.randn(2, 3)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">x = torch.transpose(x, 0, 1)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<h2 id="Tensor进行view操作"><a href="#Tensor进行view操作" class="headerlink" title="Tensor进行view操作"></a>Tensor进行view操作</h2><p>相当于numpy中resize（）的功能，但是用法可能不太一样。</p>
<p>把原先tensor中的数据按照行优先的顺序排成一个一维的数据（这里应该是因为要求地址是连续存储的），然后按照参数组合成其他维度的tensor。比如说是不管你原先的数据是[[[1,2,3],[4,5,6]]]还是[1,2,3,4,5,6]，因为它们排成一维向量都是6个元素，所以只要view后面的参数一致，得到的结果都是一样的。比如，<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=torch.Tensor([[[1,2,3],[4,5,6]]])</span><br><span class="line">b=torch.Tensor([1,2,3,4,5,6])</span><br><span class="line"></span><br><span class="line">print(a.view(1,6))</span><br><span class="line">print(b.view(1,6))</span><br></pre></td></tr></table></figure></p>
<p>得到的结果都是tensor([[1., 2., 3., 4., 5., 6.]])<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a=torch.Tensor([[[1,2,3],[4,5,6]]])</span><br><span class="line">print(a.view(3,2))</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.],</span><br><span class="line">        [5., 6.]])&apos;&apos;&apos;</span><br></pre></td></tr></table></figure></p>
<h2 id="随机种子"><a href="#随机种子" class="headerlink" title="随机种子"></a>随机种子</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">torch.manual_seed(0)</span><br><span class="line"></span><br><span class="line">x = torch.randn(2, 3)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">torch.manual_seed(0)</span><br><span class="line"></span><br><span class="line">x = torch.randn(2, 3)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>
<h1 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h1><p>当提到保存和加载模型时，有三个核心功能需要熟悉：<br>1.<a href="https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save" target="_blank" rel="noopener">torch.save</a>：将序列化的对象保存到disk。这个函数使用Python的pickle实用程序进行序列化。使用这个函数可以保存各种对象的模型、张量和字典。<br>2.<a href="https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load" target="_blank" rel="noopener">torch.load</a>：使用pickle unpickle工具将pickle的对象文件反序列化为内存。<br>3.<a href="https://pytorch.org/docs/stable/nn.html?highlight=load_state_dict#torch.nn.Module.load_state_dict" target="_blank" rel="noopener">torch.nn.Module.load_state_dict</a>:使用反序列化状态字典加载model’s参数字典。</p>
<h2 id="什么是state-dict"><a href="#什么是state-dict" class="headerlink" title="什么是state_dict"></a>什么是state_dict</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"># Define model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TheModelClass(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(TheModelClass, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 6, 5)</span><br><span class="line">        self.pool = nn.MaxPool2d(2, 2)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">        self.fc1 = nn.Linear(16 * 5 * 5, 120)</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def farward(self, x):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-1, 16 * 5 * 5)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Initialize model</span><br><span class="line">model = TheModelClass()</span><br><span class="line"># Initialize optimizer</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)</span><br><span class="line"></span><br><span class="line">print(&quot;Model&apos;s state_dict:&quot;)</span><br><span class="line"># Print model&apos;s state_dict</span><br><span class="line">for param_tensor in model.state_dict():</span><br><span class="line">    print(param_tensor, &quot;\t&quot;, model.state_dict()[param_tensor].size())</span><br><span class="line">print(&quot;optimizer&apos;s state_dict:&quot;)</span><br><span class="line"># Print optimizer&apos;s state_dict</span><br><span class="line">for var_name in optimizer.state_dict():</span><br><span class="line">    print(var_name, &quot;\t&quot;, optimizer.state_dict()[var_name])</span><br></pre></td></tr></table></figure>
<h2 id="保存读取模型"><a href="#保存读取模型" class="headerlink" title="保存读取模型"></a>保存读取模型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#　只保存模型的学习参数</span><br><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line"></span><br><span class="line">#　读取模型的可学习参数</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 保存整个模型</span><br><span class="line">torch.save(the_model, PATH)</span><br><span class="line"></span><br><span class="line"># 读取整个模型</span><br><span class="line">the_model = torch.load(PATH)</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 序列化字典</span><br><span class="line"></span><br><span class="line"># save</span><br><span class="line">torch.save(&#123;</span><br><span class="line">        &apos;epoch&apos;: epoch,</span><br><span class="line">        &apos;model_state_dict&apos;: model.state_dict(),</span><br><span class="line">        &apos;optimizer_state_dict&apos;: optimizer.state_dict(),</span><br><span class="line">        &apos;loss&apos;: loss,</span><br><span class="line">        ...</span><br><span class="line">        &#125;, PATH)</span><br><span class="line">        </span><br><span class="line"># load</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">optimizer = TheOptimizerClass(*args, **kwargs)</span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[&apos;model_state_dict&apos;])</span><br><span class="line">optimizer.load_state_dict(checkpoint[&apos;optimizer_state_dict&apos;])</span><br><span class="line">epoch = checkpoint[&apos;epoch&apos;]</span><br><span class="line">loss = checkpoint[&apos;loss&apos;]</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line"># - or -</span><br><span class="line">model.train()</span><br></pre></td></tr></table></figure>
<h1 id="数学操作"><a href="#数学操作" class="headerlink" title="数学操作"></a>数学操作</h1><h2 id="夹紧操作"><a href="#夹紧操作" class="headerlink" title="夹紧操作"></a>夹紧操作</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = torch.randn(4)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">a = torch.clamp(a, min=-0.5, max=0.5)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<h2 id="沿维度计算和"><a href="#沿维度计算和" class="headerlink" title="沿维度计算和"></a>沿维度计算和</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = torch.randn(10)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">a = torch.cumsum(a, dim=0)</span><br><span class="line">print(a)</span><br><span class="line">print(a[-1])</span><br></pre></td></tr></table></figure>
<h2 id="计算元素均值"><a href="#计算元素均值" class="headerlink" title="计算元素均值"></a>计算元素均值</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = torch.randn(2, 2)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = torch.mean(a)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">b = torch.mean(a, 1)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<h2 id="计算元素标准差"><a href="#计算元素标准差" class="headerlink" title="计算元素标准差"></a>计算元素标准差</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = torch.randn(2, 2)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = torch.std(a)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">b = torch.std(a, dim=1)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<h2 id="计算所有元素和"><a href="#计算所有元素和" class="headerlink" title="计算所有元素和"></a>计算所有元素和</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = torch.randn(2, 2)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = torch.sum(a)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">b = torch.sum(a, dim=0)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<h1 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h1><h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><p>torch.nn.Module是所有网络的基类。你的模型也应该继承这个类。<br>Modules也可以包含其它Modules,允许使用树结构嵌入他们。<br>你可以将子模块赋值给模型属性。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Model(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(1, 20, 5)  # submodule: Conv2d</span><br><span class="line">        self.conv2 = nn.Conv2d(20, 20, 5)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        return F.relu(self.conv2(x))</span><br></pre></td></tr></table></figure></p>
<h2 id="时序容器"><a href="#时序容器" class="headerlink" title="时序容器"></a>时序容器</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Example of using Sequential</span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(1,20,5),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(20,64,5),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line"># Example of using Sequential with OrderedDict</span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (&apos;conv1&apos;, nn.Conv2d(1,20,5)),</span><br><span class="line">          (&apos;relu1&apos;, nn.ReLU()),</span><br><span class="line">          (&apos;conv2&apos;, nn.Conv2d(20,64,5)),</span><br><span class="line">          (&apos;relu2&apos;, nn.ReLU())</span><br><span class="line">        ]))</span><br></pre></td></tr></table></figure>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><p>class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</p>
<p>参数kernel_size，stride,padding，dilation也可以是一个int的数据，此时卷积height和width值相同;也可以是一个tuple数组，tuple的第一维度表示height的数值，tuple的第二维度表示width的数值。</p>
<ul>
<li>in_channels(int) – 输入信号的通道</li>
<li>out_channels(int) – 卷积产生的通道</li>
<li>kerner_size(int or tuple) - 卷积核的尺寸</li>
<li>stride(int or tuple, optional) - 卷积步长</li>
<li>padding(int or tuple, optional) - 输入的每一条边补充0的层数</li>
<li>dilation(int or tuple, optional) – 卷积核元素之间的间距</li>
<li>groups(int, optional) – 从输入通道到输出通道的阻塞连接数</li>
<li>bias(bool, optional) - 如果bias=True，添加偏置<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">m = nn.Conv2d(16, 33, 3, stride=2)</span><br><span class="line"></span><br><span class="line">m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span><br><span class="line"></span><br><span class="line">m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(20, 16, 50, 100))</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 空洞卷积</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">m = nn.Conv2d(16, 33, kernel_size=3, stride=1, padding=2, dilation=2)</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(20, 16, 100, 100))</span><br><span class="line">print(input.size())</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure>
<h3 id="ConvTranspose2d"><a href="#ConvTranspose2d" class="headerlink" title="ConvTranspose2d"></a>ConvTranspose2d</h3><p>class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)</p>
<p>注意，这上面的stride、padding是争对于与原始卷积上的stride和padding<br>2维的转置卷积操作（transposed convolution operator，注意改视作操作可视作解卷积操作，但并不是真正的解卷积操作） 该模块可以看作是Conv2d相对于其输入的梯度，有时（但不正确地）被称为解卷积操作。</p>
<p>由于内核的大小，输入的最后的一些列的数据可能会丢失。因为输入和输出是不是完全的互相关。因此，用户可以进行适当的填充（padding操作）。</p>
<ul>
<li>in_channels(int) – 输入信号的通道数</li>
<li>out_channels(int) – 卷积产生的通道数</li>
<li>kerner_size(int or tuple) - 卷积核的大小</li>
<li>stride(int or tuple,optional) - 卷积步长</li>
<li>padding(int or tuple, optional) - 输入的每一条边补充0的层数</li>
<li>output_padding(int or tuple, optional) - 输出的每一条边补充0的层数</li>
<li>dilation(int or tuple, optional) – 卷积核元素之间的间距</li>
<li>groups(int, optional) – 从输入通道到输出通道的阻塞连接数</li>
<li>bias(bool, optional) - 如果bias=True，添加偏置<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(20, 16, 64, 64))</span><br><span class="line">print(input.size())</span><br><span class="line"></span><br><span class="line">m = nn.ConvTranspose2d(16, 33, 3, stride=2, padding=1, output_padding=1)</span><br><span class="line">output = m(input)</span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><h3 id="Maxpool2d"><a href="#Maxpool2d" class="headerlink" title="Maxpool2d"></a>Maxpool2d</h3><p>class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</p>
<ul>
<li>kernel_size(int or tuple) - max pooling的窗口大小</li>
<li>stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size</li>
<li>padding(int or tuple, optional) - 输入的每一条边补充0的层数</li>
<li>dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数</li>
<li>return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助</li>
<li>ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(20, 16, 64, 64))</span><br><span class="line">print(input.size())</span><br><span class="line"></span><br><span class="line">m = nn.MaxPool2d(3, stride=2)</span><br><span class="line">output = m(input)</span><br><span class="line">print(output.size())</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 4, 4))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.MaxPool2d(2, stride=2, return_indices=True)</span><br><span class="line">output, indices = m(input)</span><br><span class="line">print(indices)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="MaxUnpool2d"><a href="#MaxUnpool2d" class="headerlink" title="MaxUnpool2d"></a>MaxUnpool2d</h3><p>class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)</p>
<p>Maxpool2d的逆过程，不过并不是完全的逆过程，因为在maxpool2d的过程中，一些最大值的已经丢失。 MaxUnpool2d的输入是MaxPool2d的输出，包括最大值的索引，并计算所有maxpool2d过程中非最大值被设置为零的部分的反向。</p>
<ul>
<li>kernel_size(int or tuple) - max pooling的窗口大小</li>
<li>stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size</li>
<li>padding(int or tuple, optional) - 输入的每一条边补充0的层数<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pool = nn.MaxPool2d(2, stride=2, return_indices=True)</span><br><span class="line">unpool = nn.MaxUnpool2d(2, stride=2)</span><br><span class="line"></span><br><span class="line">input = torch.Tensor([[[[1, 2, 3, 4],</span><br><span class="line">                        [5, 6, 7, 8],</span><br><span class="line">                        [9, 10, 11, 12],</span><br><span class="line">                        [13, 14, 15, 16]]]])</span><br><span class="line"></span><br><span class="line">output, indices = pool(input)</span><br><span class="line">result = unpool(output, indices)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="AvgPool2d"><a href="#AvgPool2d" class="headerlink" title="AvgPool2d"></a>AvgPool2d</h3><p>class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)</p>
<ul>
<li>kernel_size(int or tuple) - 池化窗口大小</li>
<li>stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size</li>
<li>padding(int or tuple, optional) - 输入的每一条边补充0的层数</li>
<li>dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数</li>
<li>ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</li>
<li>count_include_pad - 如果等于True，计算平均池化时，将包括padding填充的0<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 9, 9))</span><br><span class="line"></span><br><span class="line">m = nn.AvgPool2d(3, stride=2)</span><br><span class="line">output = m(input)</span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="AdaptiveMaxPool2d"><a href="#AdaptiveMaxPool2d" class="headerlink" title="AdaptiveMaxPool2d"></a>AdaptiveMaxPool2d</h3><p>class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)</p>
<p>对输入信号，提供2维的自适应最大池化操作 对于任何输入大小的输入，可以将输出尺寸指定为H*W，但是输入和输出特征的数目不会变化。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 9, 9))</span><br><span class="line"></span><br><span class="line">m = nn.AdaptiveMaxPool2d((3, 3))</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure></p>
<h2 id="非线性激活函数"><a href="#非线性激活函数" class="headerlink" title="非线性激活函数"></a>非线性激活函数</h2><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>class torch.nn.ReLU(inplace=False) </p>
<p>对输入运用修正线性单元函数<br>{ReLU}(x)= max(0, x)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 3, 3))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.ReLU(inplace=True)</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(input)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></p>
<h3 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h3><p>class torch.nn.ELU(alpha=1.0, inplace=False)</p>
<p>对输入的每一个元素运用函数<br>f(x) = max(0,x) + min(0, alpha * (e^x - 1))<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 3, 3))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.ELU(inplace=False)</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></p>
<h3 id="PReLU"><a href="#PReLU" class="headerlink" title="PReLU"></a>PReLU</h3><p>class torch.nn.PReLU(num_parameters=1, init=0.25)<br>对输入的每一个元素运用函数<br>PReLU(x) = max(0,x) + a * min(0,x)<br>a是一个可学习参数。当没有声明时，nn.PReLU()在所有的输入中只有一个参数a；如果是nn.PReLU(nChannels)，a将应用到每个输入。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 3, 3))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.PReLU()</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></p>
<h3 id="LeakyReLU"><a href="#LeakyReLU" class="headerlink" title="LeakyReLU"></a>LeakyReLU</h3><p>class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)</p>
<p>对输入的每一个元素运用<br>f(x) = max(0, x) + {negative_slope} * min(0, x)<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 3, 3))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.LeakyReLU()</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></p>
<h3 id="Threshold"><a href="#Threshold" class="headerlink" title="Threshold"></a>Threshold</h3><p>class torch.nn.Threshold(threshold, value, inplace=False)</p>
<p><em>y</em>=<em>x</em>,<em>if</em> <em>x</em>&gt;=<em>threshold</em> <em>y</em>=<em>value</em>,<em>if</em> <em>x</em>&lt;<em>threshold</em><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 3, 3))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.Threshold(0.1, 20)</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></p>
<h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p>class torch.nn.Sigmoid<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 3, 3))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></p>
<h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>class torch.nn.Tanh<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 3, 3))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.Tanh()</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></p>
<h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>class torch.nn.Softmax<br>对n维输入张量运用Softmax函数，将张量的每个元素缩放到（0,1）区间且和为1。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 7))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.Softmax(dim=1)</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></p>
<h2 id="标准化层"><a href="#标准化层" class="headerlink" title="标准化层"></a>标准化层</h2><h3 id="BatchNorm2d"><a href="#BatchNorm2d" class="headerlink" title="BatchNorm2d"></a>BatchNorm2d</h3><p>class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)</p>
<p>对小批量(mini-batch)3d数据组成的4d输入进行批标准化(Batch Normalization)操作<br>在每一个小批量（mini-batch）数据中，计算输入各个维度的均值和标准差。gamma与beta是可学习的大小为C的参数向量（C为输入大小）<br>在训练时，该层计算每次输入的均值与方差，并进行移动平均。移动平均默认的动量值为0.1。<br>在验证时，训练求得的均值/方差将用于标准化验证数据。</p>
<ul>
<li><strong>num_features：</strong> 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features x height x width’</li>
<li><strong>eps：</strong> 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。</li>
<li><strong>momentum：</strong> 动态均值和动态方差所使用的动量。默认为0.1。</li>
<li><strong>affine：</strong> 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(20, 100, 3, 3))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.BatchNorm2d(100)</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="线性层"><a href="#线性层" class="headerlink" title="线性层"></a>线性层</h2><h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><p>class torch.nn.Linear(in_features, out_features, bias=True)<br>对输入数据做线性变换：<em>y</em>=<em>Ax</em>+<em>b</em></p>
<ul>
<li><strong>in_features</strong> - 每个输入样本的大小</li>
<li><strong>out_features</strong> - 每个输出样本的大小</li>
<li><strong>bias</strong> - 若设置为False，这层不会学习偏置。默认值：True<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(20, 20))</span><br><span class="line">print(input.size())</span><br><span class="line"></span><br><span class="line">m = nn.Linear(20, 30)</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h2><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>class torch.nn.Dropout(p=0.5, inplace=False)</p>
<p>随机将输入张量中部分元素设置为0。对于每次前向调用，被置0的元素都是随机的。</p>
<ul>
<li><strong>p</strong> - 将元素置0的概率。默认值：0.5</li>
<li><strong>in-place</strong> - 若设置为True，会在原地执行操作。默认值：False<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(3, 6))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.Dropout(p=0.5)</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="Droupout2d"><a href="#Droupout2d" class="headerlink" title="Droupout2d"></a>Droupout2d</h3><p>class torch.nn.Dropout2d(p=0.5, inplace=False)</p>
<p>随机将输入张量中整个通道设置为0。对于每次前向调用，被置0的通道都是随机的。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 3, 3, 3))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">m = nn.Dropout2d(p=0.5)</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>基本用法：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">criterion = LossCriterion() #构造函数有自己的参数</span><br><span class="line">loss = criterion(x, y) #调用标准时也有参数</span><br></pre></td></tr></table></figure></p>
<p>计算出来的结果已经对mini-batch取了平均。</p>
<h3 id="MSELoss"><a href="#MSELoss" class="headerlink" title="MSELoss"></a>MSELoss</h3><p>class torch.nn.MSELoss(size_average=True)<br>创建一个衡量输入x(模型预测输出)和目标y之间均方误差标准。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">input1 = torch.Tensor(torch.randn(1, 8))</span><br><span class="line">print(input1)</span><br><span class="line"></span><br><span class="line">input2 = torch.Tensor(torch.rand(1, 8))</span><br><span class="line">print(input2)</span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(input1, input2)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure></p>
<h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><p>class torch.nn.CrossEntropyLoss(weight=None, size_average=True)</p>
<p>此标准将LogSoftMax和NLLLoss集成到一个类中。不需要再使用softmax。</p>
<ul>
<li><p>weight(tensor): 1-D tensor，n个元素，分别代表n类的权重，如果你的训练样本很不均衡的话，是非常有用的。默认值为None。</p>
</li>
<li><p>input : 包含每个类的得分，３-D tensor,shape为 batch<em>n</em>样例数</p>
</li>
<li>target: 大小为 n 的 ２-D tensor，包含类别的索引(0到 n-1)，long型。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># batchsize=3, 二分类, 8个样例</span><br><span class="line">input = torch.Tensor(torch.randn(2, 3, 3, 3))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line"># target为long型</span><br><span class="line">target = torch.Tensor(torch.zeros(2, 3, 3)).long()</span><br><span class="line">target[0, 0, 0] = 2</span><br><span class="line">target[1, 1, 1] = 1</span><br><span class="line">target[1, 2, 2] = 1</span><br><span class="line">print(target)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(input, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h3><p>class torch.nn.BCELoss(weight=None, size_average=True)</p>
<p>计算 target 与 output 之间的二进制交叉熵。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line"># 二进制交叉商，用0和1来表示类别</span><br><span class="line">criterion = nn.BCELoss(reduction=&quot;none&quot;)</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(2, 3, 3))</span><br><span class="line">m = nn.Sigmoid()</span><br><span class="line">input = m(input)</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">target = torch.Tensor(torch.ones(2, 3, 3))</span><br><span class="line">target[0, 0, 0] = 0</span><br><span class="line">target[1, 1, 1] = 0</span><br><span class="line">target[1, 2, 2] = 0</span><br><span class="line">print(target)</span><br><span class="line"></span><br><span class="line">loss = criterion(input, target)</span><br><span class="line">print(loss.mean())</span><br></pre></td></tr></table></figure></p>
<h2 id="视觉函数"><a href="#视觉函数" class="headerlink" title="视觉函数"></a>视觉函数</h2><h3 id="PixelShuffle"><a href="#PixelShuffle" class="headerlink" title="PixelShuffle"></a>PixelShuffle</h3><p>class torch.nn.PixelShuffle(upscale_factor)</p>
<p>将shape为$[N, C<em>r^2, H, W]$的Tensor重新排列为shape为$[N, C, H</em>r, W*r]$的Tensor。 当使用stride=1/r 的sub-pixel卷积的时候，这个方法是非常有用的。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">ps = nn.PixelShuffle(3)</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 9, 4, 4))</span><br><span class="line">print(input.size())</span><br><span class="line"></span><br><span class="line">output = ps(input)</span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure></p>
<h3 id="UpsamplingNearest2d"><a href="#UpsamplingNearest2d" class="headerlink" title="UpsamplingNearest2d"></a>UpsamplingNearest2d</h3><p>class torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)</p>
<p>对于多channel 输入 进行 2-D 最近邻上采样。<br>可以通过size或者scale_factor来指定上采样后的图片大小。<br>当给定size时，size的值将会是输出图片的大小。</p>
<ul>
<li>size (tuple, optional) – 一个包含两个整数的元组 (H_out, W_out)指定了输出的长宽</li>
<li>scale_factor (int, optional) – 长和宽的一个乘子<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">m = nn.UpsamplingNearest2d(scale_factor=2)</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 1, 2, 2))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="UpsamplingBilinear2d"><a href="#UpsamplingBilinear2d" class="headerlink" title="UpsamplingBilinear2d"></a>UpsamplingBilinear2d</h3><p>class torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">m = nn.UpsamplingBilinear2d(scale_factor=2)</span><br><span class="line"></span><br><span class="line">input = torch.Tensor(torch.randn(1, 1, 2, 2))</span><br><span class="line">print(input)</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
<h2 id="多GPU模块并行"><a href="#多GPU模块并行" class="headerlink" title="多GPU模块并行"></a>多GPU模块并行</h2><h3 id="DataParallel"><a href="#DataParallel" class="headerlink" title="DataParallel"></a>DataParallel</h3><p>class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)</p>
<p>在模块级别上实现数据并行。<br>此容器通过将mini-batch划分到不同的设备上来实现给定module的并行。在forward过程中，module会在每个设备上都复制一遍，每个副本都会处理部分输入。在backward过程中，副本上的梯度会累加到原始module上。<br>batch的大小应该大于所使用的GPU的数量。还应当是GPU个数的整数倍，这样划分出来的每一块都会有相同的样本数量。</p>
<ul>
<li>module – 要被并行的module</li>
<li>device_ids – CUDA设备，默认为所有设备。</li>
<li>output_device – 输出设备（默认为device_ids[0]）<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])</span><br><span class="line">output = net(input_var)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>为了构建一个Optimizer，你需要给它一个包含了需要优化的参数（必须都是Variable对象）的iterable。然后，你可以设置optimizer的参 数选项，比如学习率，权重衰减，等等。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)</span><br><span class="line">optimizer = optim.Adam([var1, var2], lr = 0.0001)</span><br></pre></td></tr></table></figure></p>
<p>当我们想指定每一层的学习率时<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">optim.SGD([</span><br><span class="line">                &#123;&apos;params&apos;: model.base.parameters()&#125;,</span><br><span class="line">                &#123;&apos;params&apos;: model.classifier.parameters(), &apos;lr&apos;: 1e-3&#125;</span><br><span class="line">            ], lr=1e-2, momentum=0.9)</span><br></pre></td></tr></table></figure></p>
<p>这意味着model.base的参数将会使用1e-2的学习率，model.classifier的参数将会使用1e-3的学习率，并且0.9的momentum将会被用于所有的参数。</p>
<p>所有的optimizer都实现了step()方法，这个方法会更新所有的参数。</p>
<p><strong>optimizer.step()</strong><br>这是大多数optimizer所支持的简化版本。一旦梯度被如backward()之类的函数计算好后，我们就可以调用这个函数。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">for input, target in dataset:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(input)</span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></p>
<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><p>class torch.optim.Optimizer(params, defaults) [source]</p>
<p>Base class for all optimizers.</p>
<ul>
<li>params (iterable) —— Variable 或者 dict的iterable。指定了什么参数应当被优化。</li>
<li><p>defaults —— (dict)：包含了优化选项默认值的字典（一个参数组没有指定的参数选项将会使用默认值）。</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>class torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)[source]</p>
</li>
<li><p>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict</p>
</li>
<li>lr (float, 可选) – 学习率（默认：1e-3）</li>
<li>betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999）</li>
<li>eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）</li>
<li>weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）<h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2>class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)[source]</li>
</ul>
<p>实现随机梯度下降算法（momentum可选）</p>
<ul>
<li>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict</li>
<li>lr (float) – 学习率</li>
<li>momentum (float, 可选) – 动量因子（默认：0）</li>
<li>weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认：0）</li>
<li>dampening (float, 可选) – 动量的抑制因子（默认：0）</li>
<li>nesterov (bool, 可选) – 使用Nesterov动量（默认：False）<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss_fn(model(input), target).backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="数据集抽象类"><a href="#数据集抽象类" class="headerlink" title="数据集抽象类"></a>数据集抽象类</h1><h2 id="torch-utils-data"><a href="#torch-utils-data" class="headerlink" title="torch.utils.data"></a>torch.utils.data</h2><p>class torch.utils.data.Dataset<br>表示Dataset的抽象类。<br>所有其他数据集都应该进行子类化。所有子类应该override<strong>len</strong>和<strong>getitem</strong>，前者提供了数据集的大小，后者支持整数索引，范围从0到len(self)。</p>
<h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><p>class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=<function default_collate>, pin_memory=False, drop_last=False)</function></p>
<p>数据加载器。组合数据集和采样器，并在数据集上提供单进程或多进程迭代器。</p>
<ul>
<li><strong>dataset</strong> (<em>Dataset</em>) – 加载数据的数据集。</li>
<li><strong>batch_size</strong> (<em>int</em>, optional) – 每个batch加载多少个样本(默认: 1)。</li>
<li><strong>shuffle</strong> (<em>bool</em>, optional) – 设置为True时会在每个epoch重新打乱数据(默认: False).</li>
<li><strong>sampler</strong> (<em>Sampler</em>, optional) – 定义从数据集中提取样本的策略。如果指定，则忽略shuffle参数。</li>
<li><strong>num_workers</strong> (<em>int</em>, optional) – 用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0)</li>
<li><strong>collate_fn</strong> (<em>callable</em>, optional) –</li>
<li><strong>pin_memory</strong> (<em>bool</em>, optional) –</li>
<li><strong>drop_last</strong> (<em>bool</em>, optional) – 如果数据集大小不能被batch size整除，则设置为True后可删除最后一个不完整的batch。如果设为False并且数据集的大小不能被batch size整除，则最后一个batch将更小。(默认: False)<h1 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h1><h2 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h2>torchvision.datasets中包含了以下数据集</li>
<li>MNIST</li>
<li>COCO（用于图像标注和目标检测）(Captioning and Detection)</li>
<li>LSUN Classification</li>
<li>ImageFolder</li>
<li>Imagenet-12</li>
<li>CIFAR10 and CIFAR100</li>
<li>STL10</li>
</ul>
<p>Datasets 拥有以下API:<br><strong>getitem</strong> <strong>len</strong><br>由于以上Datasets都是 torch.utils.data.Dataset的子类，所以，他们也可以通过torch.utils.data.DataLoader使用多线程（python的多进程）。</p>
<h3 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision import datasets</span><br><span class="line"></span><br><span class="line">root = &quot;/home/ltb/MNIST&quot;</span><br><span class="line">datasets.MNIST(root, train=True, transform=None, target_transform=None, download=True)</span><br></pre></td></tr></table></figure>
<p>参数说明：</p>
<ul>
<li><p>root : processed/training.pt 和 processed/test.pt 的主目录</p>
</li>
<li><p>train : True = 训练集, False = 测试集 </p>
</li>
<li><p>download : True = 从互联网上下载数据集，并把数据集放在root目录下. 如果数据集之前下载过，将处理过的数据（minist.py中有相关函数）放在processed文件夹下。</p>
</li>
</ul>
<h3 id="COCO"><a href="#COCO" class="headerlink" title="COCO"></a>COCO</h3><p>需要安装<a href="https://github.com/pdollar/coco/tree/master/PythonAPI" target="_blank" rel="noopener">COCO API</a><br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 图像标注</span><br><span class="line">dset.CocoCaptions(root=&quot;dir where images are&quot;, annFile=&quot;json annotation file&quot;, [transform, target_transform])</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision import datasets</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">cap = datasets.CocoCaptions(root=&apos;dir where images are&apos;,</span><br><span class="line">                            annFile=&apos;json annotation file&apos;,</span><br><span class="line">                            transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">print(&apos;Number of samples: &apos;, len(cap))</span><br><span class="line">img, target = cap[3]  # load 4th sample</span><br><span class="line"></span><br><span class="line">print(&quot;Image Size: &quot;, img.size())</span><br><span class="line">print(target)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 检测</span><br><span class="line">dset.CocoDetection(root=&quot;dir where images are&quot;, annFile=&quot;json annotation file&quot;, [transform, target_transform])</span><br></pre></td></tr></table></figure>
<h3 id="ImageFolder"><a href="#ImageFolder" class="headerlink" title="ImageFolder"></a>ImageFolder</h3><p>一个通用的数据加载器，数据集中的数据以以下方式组织。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root/dog/xxx.png</span><br><span class="line">root/dog/xxy.png</span><br><span class="line">root/dog/xxz.png</span><br><span class="line"></span><br><span class="line">root/cat/123.png</span><br><span class="line">root/cat/nsdf3.png</span><br><span class="line">root/cat/asd932_.png</span><br></pre></td></tr></table></figure></p>
<p>既其默认你的数据集已经自觉按照要分配的类型分成了不同的文件夹，一种类型的文件夹下面只存放一种类型的图片<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torchvision.datasets as dset</span><br><span class="line">dset.ImageFolder(root=&quot;root folder path&quot;, [transform, target_transform])</span><br><span class="line"></span><br><span class="line"># root ： 指定图片存储的路径，在下面的例子中是&apos;./data/dogcat_2&apos;</span><br><span class="line">#　transform： 一个函数，原始图片作为输入，返回一个转换后的图片。</span><br><span class="line"># target_transform - 一个函数，输入为target，输出对其的转换。例子，输入的是图片标注的string，输出为word的索引。</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torchvision.datasets as dset</span><br><span class="line">dataset = dset.ImageFolder(&apos;./data/dogcat_2&apos;) #没有transform，先看看取得的原始图像数据</span><br><span class="line">print(dataset.classes)  #根据分的文件夹的名字来确定的类别</span><br><span class="line">print(dataset.class_to_idx) #按顺序为这些类别定义索引为0,1...</span><br><span class="line">print(dataset.imgs) #返回从所有文件夹中得到的图片的路径以及其类别</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[&apos;cat&apos;, &apos;dog&apos;]</span><br><span class="line">&#123;&apos;cat&apos;: 0, &apos;dog&apos;: 1&#125;</span><br><span class="line">[(&apos;./data/dogcat_2/cat/cat.12484.jpg&apos;, 0), (&apos;./data/dogcat_2/cat/cat.12485.jpg&apos;, 0), (&apos;./data/dogcat_2/cat/cat.12486.jpg&apos;, 0), (&apos;./data/dogcat_2/cat/cat.12487.jpg&apos;, 0), (&apos;./data/dogcat_2/dog/dog.12496.jpg&apos;, 1), (&apos;./data/dogcat_2/dog/dog.12497.jpg&apos;, 1), (&apos;./data/dogcat_2/dog/dog.12498.jpg&apos;, 1), (&apos;./data/dogcat_2/dog/dog.12499.jpg&apos;, 1</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<h2 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h2><p>torchvision.models模块的 子模块中包含以下模型结构。</p>
<ul>
<li>AlexNet</li>
<li>VGG</li>
<li>ResNet</li>
<li>SqueezeNet</li>
<li>DenseNet </li>
</ul>
<p>你可以使用随机初始化的权重来创建这些模型。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision import models</span><br><span class="line">resnet18 = models.resnet18()</span><br><span class="line">alexnet = models.alexnet()</span><br><span class="line">squeezenet = models.squeezenet1_0()</span><br><span class="line">densenet = models.densenet161()</span><br></pre></td></tr></table></figure></p>
<p>对于ResNet variants和AlexNet，我们也提供了预训练(pre-trained)的模型。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torchvision.models as models</span><br><span class="line"># pretrained=True就可以使用预训练的模型</span><br><span class="line">resnet18 = models.resnet18(pretrained=True)</span><br><span class="line">alexnet = models.alexnet(pretrained=True)</span><br></pre></td></tr></table></figure></p>
<h2 id="tansform"><a href="#tansform" class="headerlink" title="tansform"></a>tansform</h2><h3 id="Compose"><a href="#Compose" class="headerlink" title="Compose"></a>Compose</h3><p>将多个transform组合起来使用。<br>transforms： 由transform构成的列表. 例子：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from PIL import Image</span><br><span class="line">from torchvision import transforms</span><br><span class="line"></span><br><span class="line">transforms.Compose([</span><br><span class="line">    transforms.CenterCrop(10),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">### class torchvision.transforms.Scale(size, interpolation=2)</span><br><span class="line"></span><br><span class="line">将输入的`PIL.Image`重新改变大小成给定的`size`，`size`是最小边的边长。举个例子，如果原图的`height&gt;width`,那么改变大小后的图片大小是`(size*height/width, size)`。</span><br><span class="line">**用例:**</span><br><span class="line">​```python</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">crop = transforms.Scale(12)</span><br><span class="line">img = Image.open(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;)</span><br><span class="line"></span><br><span class="line">print(type(img))</span><br><span class="line">print(img.size)</span><br><span class="line"></span><br><span class="line">croped_img = crop(img)</span><br><span class="line">print(type(croped_img))</span><br><span class="line">print(croped_img.size)</span><br></pre></td></tr></table></figure></p>
<h3 id="CenterCrop"><a href="#CenterCrop" class="headerlink" title="CenterCrop"></a>CenterCrop</h3><p>class torchvision.transforms.CenterCrop(size)<br>将给定的PIL.Image进行中心切割，得到给定的size，size可以是tuple，(target_height, target_width)。size也可以是一个Integer，在这种情况下，切出来的图片的形状是正方形。</p>
<h3 id="RandomCrop"><a href="#RandomCrop" class="headerlink" title="RandomCrop"></a>RandomCrop</h3><p>class torchvision.transforms.RandomCrop(size, padding=0)<br>切割中心点的位置随机选取。size可以是tuple也可以是Integer。</p>
<h3 id="RandomHorizantalFlip"><a href="#RandomHorizantalFlip" class="headerlink" title="RandomHorizantalFlip"></a>RandomHorizantalFlip</h3><p>class torchvision.transforms.RandomHorizontalFlip<br>随机水平翻转给定的PIL.Image,概率为0.5。即：一半的概率翻转，一半的概率不翻转。</p>
<h3 id="RandomSizeCrop"><a href="#RandomSizeCrop" class="headerlink" title="RandomSizeCrop"></a>RandomSizeCrop</h3><p>class torchvision.transforms.RandomSizedCrop(size, interpolation=2)<br>先将给定的PIL.Image随机切，然后再resize成给定的size大小。</p>
<h3 id="Pad"><a href="#Pad" class="headerlink" title="Pad"></a>Pad</h3><p>class torchvision.transforms.Pad(padding, fill=0)<br>将给定的PIL.Image的所有边用给定的pad value填充。 padding：要填充多少像素 fill：用什么值填充 例子：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from torchvision import transforms</span><br><span class="line">from PIL import Image</span><br><span class="line">padding_img = transforms.Pad(padding=10, fill=0)</span><br><span class="line">img = Image.open(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;)</span><br><span class="line"></span><br><span class="line">print(type(img))</span><br><span class="line">print(img.size)</span><br><span class="line"></span><br><span class="line">padded_img = padding_img(img)</span><br><span class="line">print(type(padded_img))</span><br><span class="line">print(padded_img.size)</span><br></pre></td></tr></table></figure></p>
<h3 id="Normalize"><a href="#Normalize" class="headerlink" title="Normalize"></a>Normalize</h3><p>class torchvision.transforms.Normalize(mean, std)<br>给定均值：(R,G,B) 方差：（R，G，B），将会把Tensor正则化。即：Normalized_image=(image-mean)/std。</p>
<h3 id="ToTensor与ToPILImage"><a href="#ToTensor与ToPILImage" class="headerlink" title="ToTensor与ToPILImage"></a>ToTensor与ToPILImage</h3><p>把一个取值范围是[0,255]的PIL.Image或者shape为(H,W,C)的numpy.ndarray，转换成形状为[C,H,W]，取值范围是[0,1.0]的torch.FloadTensor<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from PIL import Image</span><br><span class="line">import cv2</span><br><span class="line">import torch</span><br><span class="line">from torchvision import transforms</span><br><span class="line"></span><br><span class="line"># opencv和PIL读取图片</span><br><span class="line">img1 = Image.open(&quot;/home/ltb/图片/cv2-tutorial/000001.jpg&quot;)</span><br><span class="line">img2 = cv2.imread(&quot;/home/ltb/图片/cv2-tutorial/000001.jpg&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img1.show()</span><br><span class="line">cv2.imshow(&quot;img2&quot;, img2)</span><br><span class="line">cv2.waitKey(0)</span><br><span class="line"></span><br><span class="line">print(img1)</span><br><span class="line">print(img2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line"># ToTensor,转化为tensor,[C, H, W],RGB,[0,1.0]</span><br><span class="line">t = transforms.ToTensor()</span><br><span class="line"></span><br><span class="line">img1_tensor = t(img1)</span><br><span class="line">img2_tensor = t(img2)</span><br><span class="line"></span><br><span class="line">print(img1_tensor)</span><br><span class="line">print(&quot;\n&quot;*10)</span><br><span class="line">print(img2_tensor)</span><br><span class="line"></span><br><span class="line"># tensor转化为PILImage</span><br><span class="line">t = transforms.ToPILImage()</span><br><span class="line"></span><br><span class="line">img1 = t(img1_tensor)</span><br><span class="line">img2 = t(img2_tensor)</span><br><span class="line"></span><br><span class="line">img1.show()</span><br><span class="line">img2.show()</span><br></pre></td></tr></table></figure></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">pangzibo243</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://litianbo243.github.io/2019/08/05/pytorch常用函数/">https://litianbo243.github.io/2019/08/05/pytorch常用函数/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/deeplearning/">deeplearning</a><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_zhifubao_code.jpg"><div class="post-qr-code__desc">支付宝打赏</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_weixin_code.jpg"><div class="post-qr-code__desc">微信打赏</div></div></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/08/05/opencv-python常用函数及介绍/"><i class="fa fa-chevron-left">  </i><span>opencv-python常用函数及其介绍</span></a></div><div class="next-post pull-right"><a href="/2019/08/05/Tkinter使用指南/"><span>tkinter使用指南</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/chen_sir_1.png)"><div class="layout" id="footer"><div class="copyright">&copy;2019 By pangzibo243</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>