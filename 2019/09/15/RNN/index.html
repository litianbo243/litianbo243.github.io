<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="RNN"><meta name="keywords" content="学习深度学习,deeplearning"><meta name="author" content="pangzibo243"><meta name="copyright" content="pangzibo243"><title>RNN | pangzibo243's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#RNN"><span class="toc-text">RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN的前向输出流程"><span class="toc-text">RNN的前向输出流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN的训练方法"><span class="toc-text">RNN的训练方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch中的RNN"><span class="toc-text">pytorch中的RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM"><span class="toc-text">LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#长期依赖（Long-Term-Dependencies）问题"><span class="toc-text">长期依赖（Long-Term Dependencies）问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM-网络"><span class="toc-text">LSTM 网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM-的核心思想"><span class="toc-text">LSTM 的核心思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#逐步理解-LSTM"><span class="toc-text">逐步理解 LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch中的LSTM"><span class="toc-text">pytorch中的LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Gated-Recurrent-Unit-GRU"><span class="toc-text">Gated Recurrent Unit (GRU)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch中的GRU"><span class="toc-text">pytorch中的GRU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-text">参考</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_faceQ.png"></div><div class="author-info__name text-center">pangzibo243</div><div class="author-info__description text-center">pangzibo243's blog</div><div class="follow-button"><a href="https://github.com/litianbo243">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">36</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">34</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">9</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/chen_sir_1.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">pangzibo243's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">RNN</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-09-15</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/deeplearning/">deeplearning</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p><strong>RNN（Recurrent Neural Network）</strong>是一类用于处理序列数据的神经网络。</p>
<p>我们从基础的神经网络中知道，神经网络包含<strong>输入层、隐层、输出层</strong>，通过激活函数控制输出，层与层之间通过权值连接。激活函数是事先确定好的，那么神经网络模型通过训练“学“到的东西就蕴含在“权值“中。<br>基础的神经网络只在层与层之间建立了权连接，RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。如图。<br><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170306142253375-175971779.png" alt></p>
<p>这是一个标准的RNN结构图，图中每个箭头代表做一次变换，也就是说箭头连接带有权值。左侧是折叠起来的样子，右侧是展开的样子，左侧中h旁边的箭头代表此结构中的“循环“体现在隐层。</p>
<p>在展开结构中我们可以观察到，在标准的RNN结构中，隐层的神经元之间也是带有权值的。也就是说，随着序列的不断推进，前面的隐层将会影响后面的隐层。图中O代表输出，y代表样本给出的确定值，L代表损失函数，我们可以看到，“损失“也是随着序列的推荐而不断积累的。</p>
<p>除上述特点之外，标准RNN的还有以下特点：<br>1、权值共享，图中的W全是相同的，U和V也一样。<br>2、每一个输入值都只与它本身的那条路线建立权连接，不会和别的神经元连接。</p>
<p>以上是RNN的标准结构，然而在实际中这一种结构并不能解决所有问题，例如我们输入为一串文字，输出为分类类别，那么输出就不需要一个序列，只需要单个输出。如图。</p>
<p><img src="http://p0.ifengimg.com/pmop/2017/0901/394A46EE152702FF3D5641147917A317A1E6B732_size29_w600_h460.jpeg" alt></p>
<p>同样的，我们有时候还需要单输入但是输出为序列的情况。那么就可以使用如下结构：</p>
<p><img src="http://p0.ifengimg.com/pmop/2017/0901/70BA9389E63EC316AB0C89C4ED15FAA1C5E5DB1C_size23_w600_h470.jpeg" alt></p>
<p>还有一种结构是输入虽是序列，但不随着序列变化，就可以使用如下结构：</p>
<p><img src="http://p0.ifengimg.com/pmop/2017/0901/51F23D45210EF7F730A927DCF32F2BFF0DBB7A65_size30_w600_h468.jpeg" alt></p>
<p>原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。<br>下面我们来介绍RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。</p>
<p><img src="http://p0.ifengimg.com/pmop/2017/0901/72615AC83D11EBD6E73208FA8CE0932D3D0E5FE9_size25_w600_h391.jpeg" alt></p>
<p>从名字就能看出，这个结构的原理是先编码后解码。左侧的RNN用来编码得到c，拿到c后再用右侧的RNN进行解码。得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。</p>
<p><img src="http://p0.ifengimg.com/pmop/2017/0901/CB886FFD55D48B4F6E183D4794E1F305FFC25FC9_size17_w600_h241.jpeg" alt></p>
<p>除了以上这些结构以外RNN还有很多种结构，用于应对不同的需求和解决不同的问题。</p>
<h2 id="RNN的前向输出流程"><a href="#RNN的前向输出流程" class="headerlink" title="RNN的前向输出流程"></a>RNN的前向输出流程</h2><p><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170306142253375-175971779.png" alt></p>
<p>来介绍一下各个符号的含义：<strong>x</strong>是输入，<strong>h</strong>是隐层单元，<strong>o</strong>为输出，<strong>L</strong>为损失函数，<strong>y</strong>为训练集的标签。这些元素右上角带的<strong>t代表t时刻的状态</strong>，其中需要注意的是，因策单元h在t时刻的表现不仅由此刻的输入决定，还受t时刻之前时刻的影响。<strong>V、W、U是权值，同一类型的权连接权值相同</strong>。</p>
<p>有了上面的理解，前向传播算法其实非常简单，对于t时刻：</p>
<script type="math/tex; mode=display">
h^{t} = \phi (Ux^{(t)} + Wh^{(t-1)} + b)</script><p>其中ϕ()为激活函数，一般来说会选择tanh函数，b为偏置。</p>
<p>t时刻的输出就更为简单：</p>
<script type="math/tex; mode=display">
o{(t)} = Vh^{(t)} + c</script><p>最终模型的预测输出为：</p>
<script type="math/tex; mode=display">
\hat y^{(t)} = \sigma(o^{(t)})</script><p>其中σ 为激活函数，通常RNN用于分类，故这里一般用softmax函数。</p>
<h2 id="RNN的训练方法"><a href="#RNN的训练方法" class="headerlink" title="RNN的训练方法"></a>RNN的训练方法</h2><p><strong>BPTT（back-propagation through time）算法</strong>是常用的训练RNN的方法，其实本质还是BP算法，只不过RNN处理时间序列数据，所以要基于时间反向传播，故叫随时间反向传播。BPTT的中心思想和BP算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。综上所述，BPTT算法本质还是BP算法，BP算法本质还是梯度下降法，那么求各个参数的梯度便成了此算法的核心。<br><img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170306142253375-175971779.png" alt></p>
<p>再次拿出这个结构图观察，需要寻优的参数有三个，分别是<strong>U、V、W</strong>。与BP算法不同的是，其中W和U两个参数的寻优过程需要追溯之前的历史数据，参数V相对简单只需关注目前，那么我们就来先求解参数V的偏导数。</p>
<script type="math/tex; mode=display">
\frac {\partial L^{(t)}} {\partial V} = \frac {\partial L^{(t)}} {\partial o^{(t)}} \cdot \frac {\partial o^{(t)}} {\partial V}</script><p>这个式子看起来简单但是求解起来很容易出错，因为其中嵌套着激活函数函数，是复合函数的求道过程。</p>
<p>RNN的损失也是会随着时间累加的，所以不能只求t时刻的偏导。</p>
<script type="math/tex; mode=display">
L = \sum_{t=1}^n L^{(t)}</script><script type="math/tex; mode=display">
\frac {\partial L} {\partial V} = \sum_{t=1}^n \frac {\partial L^{(t)}} {\partial o^{(t)}} \cdot \frac {\partial o^{(t)}} {\partial V}</script><p>W和U的偏导的求解由于需要涉及到历史数据，其偏导求起来相对复杂，我们先假设只有三个时刻，那么<strong>在第三个时刻</strong> L对W的偏导数为：</p>
<script type="math/tex; mode=display">
\frac {\partial L^{(3)}} {\partial W} = \frac {\partial L^{(3)}} {\partial o^{(3)}} \cdot \frac {\partial o^{(3)}} {\partial h^{(3)}} \cdot \frac {\partial h^{(3)}} {\partial W} + \frac {\partial L^{(3)}} {\partial o^{(3)}} \cdot \frac {\partial o^{(3)}} {\partial h^{(3)}} \cdot \frac {\partial h^{(3)}} {\partial h^{(2)}} \cdot \frac {\partial h^{(2)}} {\partial W} + \frac {\partial L^{(3)}} {\partial o^{(3)}} \cdot \frac {\partial o^{(3)}} {\partial h^{(3)}} \cdot \frac {\partial h^{(3)}} {\partial h^{(2)}} \cdot \frac {\partial h^{(2)}} {\partial h^{(1)}} \cdot \frac {\partial h^{(1)}} {\partial W}</script><p>相应的，L在第三个时刻对U的偏导数为：</p>
<script type="math/tex; mode=display">
\frac {\partial L^{(3)}} {\partial U} = \frac {\partial L^{(3)}} {\partial o^{(3)}} \cdot \frac {\partial o^{(3)}} {\partial h^{(3)}} \cdot \frac {\partial h^{(3)}} {\partial U} + \frac {\partial L^{(3)}} {\partial o^{(3)}} \cdot \frac {\partial o^{(3)}} {\partial h^{(3)}} \cdot \frac {\partial h^{(3)}} {\partial h^{(2)}} \cdot \frac {\partial h^{(2)}} {\partial U} + \frac {\partial L^{(3)}} {\partial o^{(3)}} \cdot \frac {\partial o^{(3)}} {\partial h^{(3)}} \cdot \frac {\partial h^{(3)}} {\partial h^{(2)}} \cdot \frac {\partial h^{(2)}} {\partial h^{(1)}} \cdot \frac {\partial h^{(1)}} {\partial U}</script><p>可以观察到，在某个时刻的对W或是U的偏导数，需要追溯这个时刻之前所有时刻的信息，这还仅仅是一个时刻的偏导数，上面说过损失也是会累加的，那么整个损失函数对W和U的偏导数将会非常繁琐。虽然如此但好在规律还是有迹可循，我们根据上面两个式子可以写出L在t时刻对W和U偏导数的通式：</p>
<script type="math/tex; mode=display">
\frac {\partial L^{(t)}} {\partial W} =\sum_{k=0}^t \frac {\partial L^{(t)}} {\partial o^{(t)}} \cdot \frac {\partial o^{(t)}} {\partial h^{(t)}} \cdot (\prod_{j=k+1}^t \frac {\partial h^{(j)}} {\partial h^{(j-1)}}) \cdot \frac {\partial h^{(3)}} {\partial W} \\
\frac {\partial L^{(t)}} {\partial U} =\sum_{k=0}^t \frac {\partial L^{(t)}} {\partial o^{(t)}} \cdot \frac {\partial o^{(t)}} {\partial h^{(t)}} \cdot (\prod_{j=k+1}^t \frac {\partial h^{(j)}} {\partial h^{(j-1)}}) \cdot \frac {\partial h^{(3)}} {\partial U}</script><p>整体的偏导公式就是将其按时刻再一一加起来。</p>
<p>前面说过激活函数是嵌套在里面的，如果我们把激活函数放进去，拿出中间累乘的那部分：</p>
<script type="math/tex; mode=display">
\prod_{j=k+1}^t \frac {\partial h^j} {\partial h^{j-1}} = \prod_{j=k+1}^t tanh^{'} \cdot W_s \\
\prod_{j=k+1}^t \frac {\partial h^j} {\partial h^{j-1}} = \prod_{j=k+1}^t sigmoid^{'} \cdot W_s</script><p>我们会发现累乘会导致激活函数导数的累乘，进而会导致“<strong>梯度消失</strong>“和“<strong>梯度爆炸</strong>“现象的发生。</p>
<p>至于为什么，我们先来看看这两个激活函数的图像。</p>
<p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1530282863766&amp;di=93faa1d6596719af61de7f3cb8405687&amp;imgtype=jpg&amp;src=http://img0.imgtn.bdimg.com/it/u=4198225085,3524733900&amp;fm=214&amp;gp=0.jpg" alt></p>
<p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b10000_10000&amp;sec=1530272904&amp;di=1c5f0934784e1b7ca2af983dda79b066&amp;src=http://img.zhiding.cn/5/578/lizKbcu27KiVg.jpg?rand=177" alt></p>
<p>它们二者是何其的相似，都把输出压缩在了一个范围之内。他们的导数图像也非常相近，我们可以从中观察到，sigmoid函数的导数范围是(0,0.25]，tach函数的导数范围是(0,1]，他们的导数最大都不大于1。</p>
<p>这就会导致一个问题，在上面式子累乘的过程中，如果取sigmoid函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。其实RNN的时间序列与深层神经网络很像，在较为深层的神经网络中使用sigmoid函数做激活函数也会导致反向传播时梯度消失，梯度消失就意味消失那一层的参数再也不更新，那么那一层隐层就变成了单纯的映射层，毫无意义了，所以在深层神经网络中，有时候多加神经元数量可能会比多家深度好。</p>
<p>你可能会提出异议，RNN明明与深层神经网络不同，RNN的参数都是共享的，而且某时刻的梯度是此时刻和之前时刻的累加，即使传不到最深处那浅层也是有梯度的。这当然是对的，但如果我们根据有限层的梯度来更新更多层的共享的参数一定会出现问题的，因为将有限的信息来作为寻优根据必定不会找到所有信息的最优解。<br>之前说过我们多用tanh函数作为激活函数，那tanh函数的导数最大也才1啊，而且又不可能所有值都取到1，那相当于还是一堆小数在累乘，还是会出现“梯度消失“，那为什么还要用它做激活函数呢？原因是tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢。</p>
<p>还有一个原因是sigmoid函数还有一个缺点，Sigmoid函数输出不是零中心对称。sigmoid的输出均大于0，这就使得输出不是0均值，称为偏移现象，这将导致后一层的神经元将上一层输出的非0均值的信号作为输入。关于原点对称的输入和中心对称的输出，网络会收敛地更好。</p>
<p>RNN的特点本来就是能“追根溯源“利用历史数据，现在告诉我可利用的历史数据竟然是有限的，这就令人非常难受，解决“梯度消失“是非常必要的。解决“梯度消失“的方法主要有：<br>1、选取更好的激活函数<br>2、改变传播结构</p>
<p>关于第一点，一般选用ReLU函数作为激活函数，ReLU函数的图像为：</p>
<p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1530283046193&amp;di=7bde8f4ad7fca6cb0812bf7c1835874e&amp;imgtype=0&amp;src=http://static.leiphone.com/uploads/new/article/740_740/201702/58a14c6da8f44.png?imageMogr2/format/jpg/quality/90" alt></p>
<p>ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。还有一点就是如果左侧横为0的导数有可能导致把神经元学死，不过设置合适的步长（学习旅）也可以有效避免这个问题的发生。</p>
<p>关于第二点，LSTM结构可以解决这个问题。</p>
<p>总结一下，sigmoid函数的缺点：<br>1、导数值范围为(0,0.25]，反向传播时会导致“梯度消失“。tanh函数导数值范围更大，相对好一点。<br>2、sigmoid函数不是0中心对称，tanh函数是，可以使网络收敛的更好。</p>
<h2 id="pytorch中的RNN"><a href="#pytorch中的RNN" class="headerlink" title="pytorch中的RNN"></a>pytorch中的RNN</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># input_size, hidden_size, num_layers</span></span><br><span class="line"><span class="comment"># num_layers为RNN的堆叠层数，由上一层每个时间节点的输出作为下一层每个时间节点的输入</span></span><br><span class="line">rnn = nn.RNN(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># input of shape (seq_len, batch, input_size)</span></span><br><span class="line">input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># h_0 of shape (num_layers * num_directions, batch, hidden_size)</span></span><br><span class="line">h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">output, hn = rnn(input, h0)</span><br></pre></td></tr></table></figure>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>下面来了解一下<strong>LSTM（long short-term memory）</strong>。长短期记忆网络是RNN的一种变体，<strong>RNN由于梯度消失的原因只能有短期记忆</strong>，<strong>LSTM网络通过精妙的门控制将短期记忆与长期记忆结合起来，并且一定程度上解决了梯度消失的问题</strong>。</p>
<h2 id="长期依赖（Long-Term-Dependencies）问题"><a href="#长期依赖（Long-Term-Dependencies）问题" class="headerlink" title="长期依赖（Long-Term Dependencies）问题"></a>长期依赖（Long-Term Dependencies）问题</h2><p>RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。</p>
<p>有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。.</p>
<p><img src="https://img-blog.csdnimg.cn/20190613125339273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt></p>
<p>但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。</p>
<p>不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。</p>
<p><img src="https://img-blog.csdnimg.cn/20190613125403928.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt></p>
<p>在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。</p>
<p>然而，幸运的是，LSTM 并没有这个问题</p>
<h2 id="LSTM-网络"><a href="#LSTM-网络" class="headerlink" title="LSTM 网络"></a>LSTM 网络</h2><p>Long Short Term 网络—— 一般就叫做 LSTM ——是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 由Hochreiter &amp; Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。<br>LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！</p>
<p>所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。<br><img src="https://img-blog.csdnimg.cn/20190701145548129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt></p>
<p>LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，整体上除了h在随时间流动，细胞状态c也在随时间流动，细胞状态c就代表着长期记忆。</p>
<p><img src="https://img-blog.csdnimg.cn/20190701145646333.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt></p>
<p>不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。</p>
<p><img src="https://img-blog.csdnimg.cn/20190701145707658.png" alt></p>
<ul>
<li>黄色的矩形是学习得到的神经网络层</li>
<li>粉色的圆形表示一些运算操作，诸如加法乘法</li>
<li>黑色的单箭头表示向量的传输</li>
<li>两个箭头合成一个表示向量的连接</li>
<li>一个箭头分开表示向量的复制</li>
</ul>
<h2 id="LSTM-的核心思想"><a href="#LSTM-的核心思想" class="headerlink" title="LSTM 的核心思想"></a>LSTM 的核心思想</h2><p>LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。</p>
<p>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。</p>
<p><img src="https://img-blog.csdnimg.cn/20190701145744799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt></p>
<p>LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。</p>
<p><img src="https://img-blog.csdnimg.cn/2019070114584830.png" alt></p>
<p>Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！</p>
<p>LSTM 拥有三个门，来保护和控制细胞状态。</p>
<h2 id="逐步理解-LSTM"><a href="#逐步理解-LSTM" class="headerlink" title="逐步理解 LSTM"></a>逐步理解 LSTM</h2><p>在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门完成。该门会读取ht-1和xt，输出一个在 0 到 1 之间的数值给每个在细胞状态Ct−1中的数字。1 表示“完全保留”，0 表示“完全舍弃”。</p>
<p>让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。<br><img src="https://img-blog.csdnimg.cn/2019070114590464.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt></p>
<p>这里可以抛出两个问题：这个门怎么做到“遗忘“的呢？怎么理解？既然是遗忘旧的内容，为什么这个门还要接收新的xt?</p>
<p>对于第一个问题，“遗忘“可以理解为“之前的内容记住多少“，其精髓在于只能输出（0，1）小数的sigmoid函数和粉色圆圈的乘法，LSTM网络经过学习决定让网络记住以前百分之多少的内容。对于第二个问题就更好理解，决定记住什么遗忘什么，其中新的输入肯定要产生影响。</p>
<p>下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，Ct，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。<br><img src="https://img-blog.csdn.net/20160731214331118" alt></p>
<p>现在是更新旧细胞状态的时间了，Ct−1更新为 Ct。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。</p>
<p>我们把旧状态与f_t 相乘，丢弃掉我们确定需要丢弃的信息。接着加上相乘，丢弃掉我们确定需要丢弃的信息。接着加上 i_t <em> Ct。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。<br>有了上面的理解基础输入门，输入门理解起来就简单多了。<em>*sigmoid函数选择更新内容，tanh函数创建更新候选。</em></em></p>
<p><img src="https://img-blog.csdn.net/20160731215529417" alt></p>
<p>最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<p><img src="https://img-blog.csdn.net/20160731220103283" alt></p>
<p>这三个门虽然功能上不同，但在执行任务的操作上是相同的。他们都是使用sigmoid函数作为选择工具，tanh函数作为变换工具，这两个函数结合起来实现三个门的功能。</p>
<h2 id="pytorch中的LSTM"><a href="#pytorch中的LSTM" class="headerlink" title="pytorch中的LSTM"></a>pytorch中的LSTM</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># input_size, hidden_size, num_layers</span></span><br><span class="line"><span class="comment"># num_layers为RNN的堆叠层数，由上一层每个时间节点的输出作为下一层每个时间节点的输入</span></span><br><span class="line">rnn = nn.LSTM(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># input of shape (seq_len, batch, input_size)</span></span><br><span class="line">input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># h_0 and c0 of shape (num_layers * num_directions, batch, hidden_size)</span></span><br><span class="line">h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">c0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">output, (hn, cn) = rnn(input, (h0, c0))</span><br></pre></td></tr></table></figure>
<h2 id="Gated-Recurrent-Unit-GRU"><a href="#Gated-Recurrent-Unit-GRU" class="headerlink" title="Gated Recurrent Unit (GRU)"></a>Gated Recurrent Unit (GRU)</h2><p>它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p>
<p><img src="https://img-blog.csdnimg.cn/20190613125456537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW9qYzE5OTU=,size_16,color_FFFFFF,t_70" alt></p>
<h2 id="pytorch中的GRU"><a href="#pytorch中的GRU" class="headerlink" title="pytorch中的GRU"></a>pytorch中的GRU</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># input_size, hidden_size, num_layers</span></span><br><span class="line"><span class="comment"># num_layers为RNN的堆叠层数，由上一层每个时间节点的输出作为下一层每个时间节点的输入</span></span><br><span class="line">rnn = nn.GRU(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># input of shape (seq_len, batch, input_size)</span></span><br><span class="line">input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment"># h_0 of shape (num_layers * num_directions, batch, hidden_size)</span></span><br><span class="line">h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line">output, hn = rnn(input, h0)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/zhaojc1995/article/details/80572098" target="_blank" rel="noopener">https://blog.csdn.net/zhaojc1995/article/details/80572098</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">pangzibo243</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://litianbo243.github.io/2019/09/15/RNN/">https://litianbo243.github.io/2019/09/15/RNN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/学习深度学习/">学习深度学习</a><a class="post-meta__tags" href="/tags/deeplearning/">deeplearning</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_zhifubao_code.jpg"><div class="post-qr-code__desc">支付宝打赏</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_weixin_code.jpg"><div class="post-qr-code__desc">微信打赏</div></div></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/09/16/优化器-介绍/"><i class="fa fa-chevron-left">  </i><span>优化器 介绍</span></a></div><div class="next-post pull-right"><a href="/2019/09/14/Dropout/"><span>Dropout</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/chen_sir_1.png)"><div class="layout" id="footer"><div class="copyright">&copy;2019 By pangzibo243</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>