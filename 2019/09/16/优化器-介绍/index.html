<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="优化器 介绍"><meta name="keywords" content="学习深度学习,deeplearning"><meta name="author" content="pangzibo243"><meta name="copyright" content="pangzibo243"><title>优化器 介绍 | pangzibo243's blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#优化器介绍"><span class="toc-text">优化器介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SGD"><span class="toc-text">SGD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Gradient-Descent"><span class="toc-text">Batch Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent"><span class="toc-text">Stochastic Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-batch-Gradient-Descent"><span class="toc-text">Mini-batch Gradient Descent</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Momentum"><span class="toc-text">Momentum</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Nesterov-Momentum"><span class="toc-text">Nesterov Momentum</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adagrad"><span class="toc-text">Adagrad</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RMSprop"><span class="toc-text">RMSprop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adam"><span class="toc-text">Adam</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#表现"><span class="toc-text">表现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#调整学习率"><span class="toc-text">调整学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-text">参考</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_faceQ.png"></div><div class="author-info__name text-center">pangzibo243</div><div class="author-info__description text-center">pangzibo243's blog</div><div class="follow-button"><a href="https://github.com/litianbo243">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">45</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">36</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">10</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/chen_sir_1.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">pangzibo243's blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">优化器 介绍</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-09-16</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/deeplearning/">deeplearning</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="优化器介绍"><a href="#优化器介绍" class="headerlink" title="优化器介绍"></a>优化器介绍</h1><p>要使用<code>torch.optim</code>，您必须构造一个<code>optimizer</code>对象。这个对象能保存当前的参数状态并且基于计算梯度更新参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr = <span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">optimizer = optim.Adam([var1, var2], lr = <span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(input)</span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><h3 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a>Batch Gradient Descent</h3><p>在每一轮的训练过程中，Batch Gradient Descent算法用整个训练集的数据计算cost fuction的梯度，并用该梯度对模型参数进行更新：</p>
<script type="math/tex; mode=display">
\Theta = \Theta - \alpha \cdot \bigtriangledown_{\Theta} J(\Theta)</script><p><strong>优点:</strong></p>
<ul>
<li>cost fuction若为凸函数，能够保证收敛到全局最优值；若为非凸函数，能够收敛到局部最优值</li>
</ul>
<p><strong>缺点:</strong></p>
<ul>
<li>由于每轮迭代都需要在整个数据集上计算一次，所以批量梯度下降可能非常慢</li>
<li>训练数较多时，需要较大内存</li>
<li>批量梯度下降不允许在线更新模型，例如新增实例。</li>
</ul>
<h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>和批梯度下降算法相反，Stochastic gradient descent 算法每读入一个数据，便立刻计算cost fuction的梯度来更新参数：</p>
<script type="math/tex; mode=display">
\Theta = \Theta - \alpha \cdot \bigtriangledown_{\Theta} J(\Theta; x^{(i)}, y^{(i)})</script><p><strong>优点:</strong></p>
<ul>
<li>算法收敛速度快(在Batch Gradient Descent算法中, 每轮会计算很多相似样本的梯度, 这部分是冗余的)</li>
<li>可以在线更新</li>
<li>有几率跳出一个比较差的局部最优而收敛到一个更好的局部最优甚至是全局最优</li>
</ul>
<p><strong>缺点:</strong></p>
<ul>
<li>容易收敛到局部最优，并且容易被困在鞍点</li>
</ul>
<h3 id="Mini-batch-Gradient-Descent"><a href="#Mini-batch-Gradient-Descent" class="headerlink" title="Mini-batch Gradient Descent"></a>Mini-batch Gradient Descent</h3><p>mini-batch Gradient Descent的方法是在上述两个方法中取折衷, 每次从所有训练数据中取一个子集（mini-batch） 用于计算梯度：</p>
<script type="math/tex; mode=display">
\Theta = \Theta - \alpha \cdot \bigtriangledown_{\Theta} J(\Theta; x^{(i:i+n)}, y^{(i:i+n)})</script><p>Mini-batch Gradient Descent在每轮迭代中仅仅计算一个mini-batch的梯度，不仅计算效率高，而且收敛较为稳定。该方法是目前深度学训练中的主流方法</p>
<p>Mini-batch Gradient Descent在每轮迭代中仅仅计算一个mini-batch的梯度，不仅计算效率高，而且收敛较为稳定。该方法是目前深度学训练中的主流方法</p>
<p>上述三个方法面临的主要<strong>挑战</strong>如下：</p>
<ul>
<li>选择适当的学习率α 较为困难。太小的学习率会导致收敛缓慢，而学习速度太块会造成较大波动，妨碍收敛。</li>
<li>目前可采用的方法是在训练过程中调整学习率大小，例如模拟<strong>退火算法</strong>：<strong>预先定义一个迭代次数m，每执行完m次训练便减小学习率，或者当cost function的值低于一个阈值时减小学习率。</strong>然而迭代次数和阈值必须事先定义，因此无法适应数据集的特点。</li>
<li>上述方法中, 每个参数的 learning rate 都是相同的，这种做法是不合理的：如果训练数据是稀疏的，并且不同特征的出现频率差异较大，那么比较合理的做法是对于出现频率低的特征设置较大的学习速率，对于出现频率较大的特征数据设置较小的学习速率。</li>
<li>近期的的研究表明，深层神经网络之所以比较难训练，并不是因为容易进入local minimum。相反，由于网络结构非常复杂，在绝大多数情况下即使是 local minimum 也可以得到非常好的结果。<strong>而之所以难训练是因为学习过程容易陷入到马鞍面中，即在坡面上，一部分点是上升的，一部分点是下降的。而这种情况比较容易出现在平坦区域，在这种区域中，所有方向的梯度值都几乎是 0。</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim.SGD(params, lr=, momentum=<span class="number">0</span>, dampening=<span class="number">0</span>, weight_decay=<span class="number">0</span>, nesterov=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss_fn(model(input), target).backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。<strong>Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。</strong>这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力：</p>
<script type="math/tex; mode=display">
v_t = \gamma \cdot v_{t-1} + \alpha \cdot \bigtriangledown_{\Theta} J(\Theta) \\
\Theta = \Theta - v_t</script><p>Momentum算法会观察历史梯度vt−1，若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯方向不一致，则梯度会衰减。<strong>一种形象的解释是：</strong>我们把一个球推下山，球在下坡时积聚动量，在途中变得越来越快，γ可视为空气阻力，若球的方向发生变化，则动量会衰减。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim.SGD(params, lr=, momentum=<span class="number">0</span>, dampening=<span class="number">0</span>, weight_decay=<span class="number">0</span>, nesterov=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss_fn(model(input), target).backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h2><p>在小球向下滚动的过程中，我们希望小球能够提前知道在哪些地方坡面会上升，这样在遇到上升坡面之前，小球就开始减速。这方法就是Nesterov Momentum，其在凸优化中有较强的理论保证收敛。并且，在实践中Nesterov Momentum也比单纯的 Momentum 的效果好：</p>
<script type="math/tex; mode=display">
v_t = \gamma \cdot v_{t-1} + \alpha \cdot \bigtriangledown_{\Theta} J(\Theta - \gamma v_{t-1}) \\
\Theta = \Theta -v_t</script><p>其核心思想是：注意到 momentum 方法，如果只看 γ <em> v 项，那么当前的 θ经过 momentum 的作用会变成 θ-γ </em> v。因此可以把 θ-γ <em> v这个位置看做是当前优化的一个”展望”位置。所以，可以在 θ-γ </em> v求导, 而不是原始的θ。</p>
<p><img src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/20170805212728775.jpeg" alt></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim.SGD(params, lr=, momentum=<span class="number">0</span>, dampening=<span class="number">0</span>, weight_decay=<span class="number">0</span>, nesterov=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss_fn(model(input), target).backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>上述方法中，对于每一个参数θi 的训练都使用了相同的学习率α。<strong>Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新；相反，对于出现频率较高的参数采用较小的α更新。</strong>因此，Adagrad非常适合处理稀疏数据。</p>
<p>我们设gt,为第t轮第i个参数的梯度，即gt,i=▽ΘJ(Θi)。因此，SGD中参数更新的过程可写为：</p>
<script type="math/tex; mode=display">
\Theta_{t+1,i} = \Theta_{t,i} - \alpha \cdot g_{t,i}</script><p>Adagrad在每轮训练中对每个参数θi 的学习率进行更新，参数更新公式如下：</p>
<script type="math/tex; mode=display">
\Theta_{t+1,i} = \Theta_{t,i} - \frac {\alpha} {\sqrt {G_{t,ii} + \epsilon}} \cdot g_{t,i}</script><p>其中，Gt∈ℝd×d 为对角矩阵，每个对角线位置i,i为对应参数θi 从第1轮到第t轮梯度的平方和。ϵ是平滑项，用于避免分母为0，一般取值1e−8。Adagrad的缺点是在训练的中后期，分母上梯度平方的累加将会越来越大，从而梯度趋近于0，使得训练提前结束。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim.Adagrad(params, lr=<span class="number">0.01</span>, lr_decay=<span class="number">0</span>, weight_decay=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adagrad(model.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss_fn(model(input), target).backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop是Geoff Hinton提出的一种自适应学习率方法。Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题。</p>
<script type="math/tex; mode=display">
E[g^2]_t = 0.9E[g^2]_{t-1} + 0.1g_t^2 \\
\Theta_{t+1} = \Theta_{t} - \frac {\alpha} {\sqrt {E[g^2]_t + \epsilon}} \cdot g_{t}</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.optim.RMSprop(params, lr=<span class="number">0.01</span>, alpha=<span class="number">0.99</span>, eps=<span class="number">1e-08</span>, weight_decay=<span class="number">0</span>, momentum=<span class="number">0</span>, centered=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.RMSprop(model.parameters(), lr=<span class="number">0.1</span>, alpha=<span class="number">0.9</span>)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss_fn(model(input), target).backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法。它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下：</p>
<script type="math/tex; mode=display">
m_t = \beta_1 m_{t-1} + (1 - \beta_1)g_t \\
v_t = \beta_2 v_{t-1} + (1 - \beta_2)g_t^2 \\
\hat m_t = \frac {m_t} {1 - \beta_1^t} \\
\hat v_t = \frac {v_t} {1 - \beta_2^t} \\
\Theta_{t+1} = \Theta_t - \frac {\alpha} {\sqrt {\hat v_t} + \epsilon} \hat m_t</script><p>其中，mt ，vt 分别是对梯度的一阶矩估计和二阶矩估计，可以看作对期望E[gt] ，E[g2t] 的近似;mt^，vt^是对mt ，vt 的校正，这样可以近似为对期望的无偏估计。Adam算法的提出者建议β1的默认值为0.9，β2 的默认值为.999， \epsilon 默认为10^−8 。 另外，在数据比较稀疏的时候，adaptive的方法能得到更好的效果，例如Adagrad，RMSprop, Adam 等。Adam 方法也会比 RMSprop方法收敛的结果要好一些, 所以在实际应用中 ，Adam为最常用的方法，可以比较快地得到一个预估结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim.Adam(params, lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>, weight_decay=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.1</span>, beta=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss_fn(model(input), target).backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="表现"><a href="#表现" class="headerlink" title="表现"></a>表现</h2><p>最后两张动图从直观上展现了算法的优化过程。第一张图为不同算法在损失平面等高线上随时间的变化情况，第二张图为不同算法在鞍点处的行为比较。</p>
<p><img src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/20170806000414235.gif" alt></p>
<p><img src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/20170806001149509.gif" alt></p>
<h2 id="调整学习率"><a href="#调整学习率" class="headerlink" title="调整学习率"></a>调整学习率</h2><p><code>torch.optim.lr_scheduler</code> 提供了几种方法来根据epoches的数量调整学习率。<code>torch.optim.lr_scheduler.ReduceLROnPlateau</code>允许基于一些验证测量来降低动态学习速率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim.lr_scheduler.StepLR(optimizer, step_size, gamma=<span class="number">0.1</span>, last_epoch=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming optimizer uses lr = 0.5 for all groups</span></span><br><span class="line"><span class="comment"># lr = 0.05     if epoch &lt; 30</span></span><br><span class="line"><span class="comment"># lr = 0.005    if 30 &lt;= epoch &lt; 60</span></span><br><span class="line"><span class="comment"># lr = 0.0005   if 60 &lt;= epoch &lt; 90</span></span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">scheduler = StepLR(optimizer, step_size=<span class="number">30</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    scheduler.step()</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=<span class="number">0.1</span>, last_epoch=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming optimizer uses lr = 0.5 for all groups</span></span><br><span class="line"><span class="comment"># lr = 0.05     if epoch &lt; 30</span></span><br><span class="line"><span class="comment"># lr = 0.005    if 30 &lt;= epoch &lt; 80</span></span><br><span class="line"><span class="comment"># lr = 0.0005   if epoch &gt;= 80</span></span><br><span class="line">scheduler = MultiStepLR(optimizer, milestones=[<span class="number">30</span>,<span class="number">80</span>], gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    scheduler.step()</span><br><span class="line">    train(...)</span><br><span class="line">    validate(...)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">'min'</span>, factor=<span class="number">0.1</span>, patience=<span class="number">10</span>, verbose=<span class="literal">False</span>, threshold=<span class="number">0.0001</span>, threshold_mode=<span class="string">'rel'</span>, cooldown=<span class="number">0</span>, min_lr=<span class="number">0</span>, eps=<span class="number">1e-08</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mode (str) – min, max中的一个. 在最小模式下，当监测量停止下降时，lr将减少; 在最大模式下，当监控量停止增加时，会减少。默认值：'min'。</span></span><br><span class="line"><span class="comment"># factor (float) – 使学习率降低的因素。 new_lr = lr * factor. 默认: 0.1.</span></span><br><span class="line"><span class="comment"># patience (int) –epochs没有改善后，学习率将降低。 默认: 10.</span></span><br><span class="line"><span class="comment"># verbose (bool) – 如果为True，则会向每个更新的stdout打印一条消息。 默认: False.</span></span><br><span class="line"><span class="comment"># threshold (float) – 测量新的最优值的阈值，只关注显着变化。 默认: 1e-4.</span></span><br><span class="line"><span class="comment"># threshold_mode (str) – rel, abs中的一个. 在rel模型, dynamic_threshold = best ( 1 + threshold ) in ‘max’ mode or best ( 1 - threshold ) 在最小模型. 在绝对值模型中, dynamic_threshold = best + threshold 在最大模式或最佳阈值最小模式. 默认: ‘rel’.</span></span><br><span class="line"><span class="comment"># cooldown (int) – 在lr减少后恢复正常运行之前等待的时期数。默认的: 0.</span></span><br><span class="line"><span class="comment"># min_lr (float or list) – 标量或标量的列表。对所有的组群或每组的学习速率的一个较低的限制。 默认: 0.</span></span><br><span class="line"><span class="comment"># eps (float) – 适用于lr的最小衰减。如果新旧lr之间的差异小于eps，则更新将被忽略。默认: 1e-8.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当指标停止改善时，降低学习率。当学习停滞不前时，模型往往会使学习速度降低2-10倍。这个调度程序读取一个指标量，如果没有提高epochs的数量，学习率就会降低。</span></span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">scheduler = torch.optim.ReduceLROnPlateau(optimizer, <span class="string">'min'</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">	train(...)</span><br><span class="line">	val_loss = validate(...)</span><br><span class="line">	<span class="comment"># Note that step should be called after validate()</span></span><br><span class="line">	scheduler.step(val_loss)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/u010089444/article/details/76725843" target="_blank" rel="noopener">https://blog.csdn.net/u010089444/article/details/76725843</a></p>
<p><a href="https://ptorch.com/docs/1/optim#algorithms" target="_blank" rel="noopener">https://ptorch.com/docs/1/optim#algorithms</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">pangzibo243</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://litianbo243.github.io/2019/09/16/优化器-介绍/">https://litianbo243.github.io/2019/09/16/优化器-介绍/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/学习深度学习/">学习深度学习</a><a class="post-meta__tags" href="/tags/deeplearning/">deeplearning</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_zhifubao_code.jpg"><div class="post-qr-code__desc">支付宝打赏</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/my_weixin_code.jpg"><div class="post-qr-code__desc">微信打赏</div></div></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/09/18/深度学习之GPU和显存分析/"><i class="fa fa-chevron-left">  </i><span>深度学习之GPU和显存分析</span></a></div><div class="next-post pull-right"><a href="/2019/09/16/loss-function-总结/"><span>loss function 总结</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://raw.githubusercontent.com/litianbo243/litianbo243.github.io/master/images/chen_sir_1.png)"><div class="layout" id="footer"><div class="copyright">&copy;2019 By pangzibo243</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>