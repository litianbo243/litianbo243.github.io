{"meta":{"title":"pangzibo243's blog","subtitle":"a man can be destroyed, but not defeated.","description":"pangzibo243's blog","author":"pangzibo243","url":"https://litianbo243.github.io","root":"/"},"pages":[{"title":"404","text":"","path":"404/index.html","date":"08-03","excerpt":""},{"title":"categories","text":"","path":"categories/index.html","date":"08-05","excerpt":""},{"title":"search","text":"","path":"search/index.html","date":"08-03","excerpt":""},{"title":"tags","text":"","path":"tags/index.html","date":"08-05","excerpt":""}],"posts":[{"title":"CSS 入门","text":"CSS 入门CSS 指层叠样式表 (Cascading Style Sheets) 样式表允许以多种方式规定样式信息。样式可以规定在单个的 HTML 元素中，在 HTML 页的头元素中，或在一个外部的 CSS 文件中。甚至可以在同一个 HTML 文档内部引用多个外部样式表。 一般而言，所有的样式会根据下面的规则层叠于一个新的虚拟样式表中，其中数字 4 拥有最高的优先权。 浏览器缺省设置 外部样式表 内部样式表（位于 标签内部） 内联样式（在 HTML 元素内部） 因此，内联样式（在 HTML 元素内部）拥有最高的优先权，这意味着它将优先于以下的样式声明： 标签中的样式声明，外部样式表中的样式声明，或者浏览器中的样式声明（缺省值）。 CSS 语法CSS 规则由两个主要的部分构成：选择器，以及一条或多条声明。 选择器通常是您需要改变样式的 HTML 元素。 每条声明由一个属性和一个值组成。 &lt;style type=\"text/css\"&gt; body &#123; background: #ff0000; &#125; h1 &#123; background: aqua; &#125; h2 &#123; background: aliceblue; &#125;&lt;/style&gt; 创建CSS的3种方式内联样式表 在标签内使用style属性指定css代码 &lt;a style=\"font-size: 40px; color: red\"&gt;Hello World!&lt;/a&gt; 文档内嵌样式表 在中使用style元素指定css代码 &lt;style type = \"text/css\"&gt; a&#123; font-size: 40px; color: #c7edcc; &#125;&lt;/style&gt; 外部样式表 外部css文件，需要在html文件中的中进行链接 /*a.css*/a&#123; font-size: 40px; color: aquamarine;&#125; &lt;!--xxx.html--&gt;&lt;link rel=\"stylesheet\" type=\"text/css\" href=\"a.css\"/&gt; CSS 选择器CSS选择器：就是指定CSS要作用的标签，那个标签的名称就是选择器。意为：选择哪个容器。 基本选择器： 标签选择器：针对一类标签 ID选择器：针对某一个特定的标签使用 类选择器：针对你想要的所有标签使用 通用选择器（通配符）：针对所有的标签都适用（不建议使用） 标签选择器 &lt;head&gt; &lt;style type=\"text/css\"&gt; p&#123; font-size:14px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;css&lt;/p&gt;&lt;/body&gt; id选择器 针对某一个特定的标签来使用，只能使用一次。css中的ID选择器以”#”来定义。 &lt;head&gt; &lt;title&gt;Document&lt;/title&gt; &lt;style type=\"text/css\"&gt; #mytitle&#123; border:3px dashed green; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h2 id=\"mytitle\"&gt;你好&lt;/h2&gt;&lt;/body&gt; id选择器的选择符是“#”。 另外，特别强调的是：HTML页面，不能出现相同的id，哪怕他们不是一个类型。比如页面上有一个id为pp的p，一个id为pp的div，是非法的！ 类选择器 规定用圆点.来定义 &lt;head&gt; &lt;style type=\"text/css\"&gt; /*定义类选择器*/ .oneclass&#123; width:800px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h2 class=\"oneclass\"&gt;你好&lt;/h2&gt;&lt;/body&gt; 和id非常相似，任何的标签都可以携带id属性和class属性。但是id属性只能被某一特定标签引用一次。 class属性的特点： 特性1：类选择器可以被多种标签使用。 特性2：同一个标签可以使用多个类选择器。用空格隔开 我们记住这句话：类上样式，id上行为。意思是说，class属性交给css使用，id属性交给js使用。 通用选择器 通用选择器，将匹配任何标签。不建议使用，IE有些版本不支持，大网站增加客户端负担。 &lt;style type=\"text/css\"&gt; /*定义通用选择器*，希望所有标签的上边距和左边距都为0*/ *&#123; margin-left:0px; margin-top:0px; &#125;&lt;/style&gt; 属性选择器 &lt;style&gt; [title] &#123; color:red; &#125; [title=\"W3School\"]&#123; border:5px solid blue; &#125; input [type=\"text\"] &#123; width:150px; display:block; margin-bottom:10px; background-color:yellow; font-family: Verdana, Arial; &#125;&lt;/style&gt; CSS 设置背景可以使用 background-color 属性为元素设置背景色。这个属性接受任何合法的颜色值。 p &#123; background-color: gray;&#125; 要把图像放入背景，需要使用 background-image 属性。background-image 属性的默认值是 none，表示背景上没有放置任何图像。 body &#123; background-image: url(\"https://www.w3school.com.cn/i/eg_bg_04.gif\");&#125; /*背景重复*/body &#123; background-image: url(\"https://www.w3school.com.cn/i/eg_bg_04.gif\"); background-repeat: repeat-y;&#125; /*背景定位*/body &#123; background-image: url(\"https://www.w3school.com.cn/i/eg_bg_04.gif\"); background-repeat: no-repeat; background-position: center;&#125; /*背景关联*//*通过这个属性，可以声明图像相对于可视区是固定的（fixed），因此不会受到滚动的影响*/body &#123; background-image: url(\"https://www.w3school.com.cn/i/eg_bg_04.gif\"); background-repeat: no-repeat; background-attachment: fixed;&#125; CSS 设置文本样式文本缩进 /*使所有段落的首行缩进5em*/p &#123; text-indent: 5em;&#125; 水平对齐 text-align 是一个基本的属性，它会影响一个元素中的文本行互相之间的对齐方式。 值 left、right 和 center 会导致元素中的文本分别左对齐、右对齐和居中。 h1 &#123;text-align: center&#125;h2 &#123;text-align: left&#125;h3 &#123;text-align: right&#125; 字间隔 word-spacing 属性可以改变字（单词）之间的标准间隔。其默认值 normal 与设置值为 0 是一样的。 p.spread &#123; word-spacing: 30px;&#125;p.tight &#123; word-spacing: -0.5em;&#125; 字母间隔 letter-spacing 属性与 word-spacing 的区别在于，字母间隔修改的是字符或字母之间的间隔。 h1 &#123; letter-spacing: -0.5em;&#125;h4 &#123; letter-spacing: 20px;&#125; 字符转换 h1 &#123;text-transform: uppercase&#125;p.uppercase &#123;text-transform: uppercase&#125;p.lowercase &#123;text-transform: lowercase&#125;p.capitalize &#123;text-transform: capitalize&#125; 文本装饰 text-decoration 有 5 个值： none underline overline line-through blink underline 会对元素加下划线，就像 HTML 中的 U 元素一样。 overline 的作用恰好相反，会在文本的顶端画一个上划线。 line-through 则在文本中间画一个贯穿线，等价于 HTML 中的 S 和 strike 元素。 blink 会让文本闪烁，类似于 Netscape 支持的颇招非议的 blink 标记。 none 值会关闭原本应用到一个元素上的所有装饰。 a &#123; text-decoration: underline overline;&#125;p &#123; text-decoration: line-through blink;&#125; 文本方向 /*从右到左*/h2 &#123; direction: rtl;&#125; 文本阴影 您能够规定水平阴影、垂直阴影、模糊距离，以及阴影的颜色 h1&#123; text-shadow: 5px 5px 5px #FF0000;&#125; CSS字体CSS 定义了 5 种通用字体系列： Serif 字体 Sans-serif 字体 Monospace 字体 Cursive 字体 Fantasy 字体 指定字体 body &#123; font-family: sans-serif;&#125;a &#123; font-family: Serif;&#125;h1 &#123; font-family: Monospaced;&#125; 字体风格 font-style 属性最常用于规定斜体文本。 该属性有三个值： normal - 文本正常显示 italic - 文本斜体显示 oblique - 文本倾斜显示 p.normal &#123;font-style:normal;&#125;p.italic &#123;font-style:italic;&#125;p.oblique &#123;font-style:oblique;&#125; 斜体（italic）是一种简单的字体风格，对每个字母的结构有一些小改动，来反映变化的外观。与此不同，倾斜（oblique）文本则是正常竖直文本的一个倾斜版本。 字体加粗 font-weight 属性设置文本的粗细。 使用 bold 关键字可以将文本设置为粗体。 关键字 100 ~ 900 为字体指定了 9 级加粗度。如果一个字体内置了这些加粗级别，那么这些数字就直接映射到预定义的级别，100 对应最细的字体变形，900 对应最粗的字体变形。数字 400 等价于 normal，而 700 等价于 bold。 如果将元素的加粗设置为 bolder，浏览器会设置比所继承值更粗的一个字体加粗。与此相反，关键词 lighter 会导致浏览器将加粗度下移而不是上移。 p.normal &#123;font-weight:normal;&#125;p.thick &#123;font-weight:bold;&#125;p.thicker &#123;font-weight:900;&#125; 字体大小 font-size 属性设置文本的大小。 请始终使用正确的 HTML 标题，比如使用 - 来标记标题，使用 来标记段落。 h1 &#123;font-size:60px;&#125;h2 &#123;font-size:40px;&#125;p &#123;font-size:14px;&#125; h1 &#123;font-size:3.75em;&#125; /* 60px/16=3.75em */h2 &#123;font-size:2.5em;&#125; /* 40px/16=2.5em */p &#123;font-size:0.875em;&#125; /* 14px/16=0.875em */ body &#123;font-size:100%;&#125;h1 &#123;font-size:3.75em;&#125;h2 &#123;font-size:2.5em;&#125;p &#123;font-size:0.875em;&#125; 在所有浏览器中均有效的方案是为 body 元素（父元素）以百分比设置默认的 font-size 值 CSS3 过渡通过 CSS3，我们可以在不使用 Flash 动画或 JavaScript 的情况下，当元素从一种样式变换为另一种样式时为元素添加效果。 CSS3 过渡是元素从一种样式逐渐改变为另一种的效果。 要实现这一点，必须规定两项内容： 规定您希望把效果添加到哪个 CSS 属性上 规定效果的时长 &lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style type=\"text/css\"&gt; div &#123; width: 100px; height: 100px; background: red; &#125; div:hover &#123; width: 300px; transition: width 2s linear 1s; -moz-transition: width 2s ease 1s, -moz-transform 2s ease-in-out 0.5s; -webkit-transition: width 2s ease-in 1s, -webkit-transform 2s ease-in-out 0.5s; -o-transition: width 2s ease-in-out 1s, -o-transform 2s ease-in-out 0.5s; -moz-transform: rotate(180deg); -webkit-transform: rotate(180deg); -o-transform: rotate(180deg); &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; CSS3 动画通过 CSS3，我们能够创建动画，这可以在许多网页中取代动画图片、Flash 动画以及 JavaScript。 @keyframes 规则用于创建动画。在 @keyframes 中规定某项 CSS 样式，就能创建由当前样式逐渐改为新样式的动画效果。 &lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style type=\"text/css\"&gt; div &#123; width: 100px; height: 100px; background: red; animation: test1 5s; &#125; @keyframes test1 &#123; from &#123; background: red; &#125; to&#123; background: yellow; &#125; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 请用百分比来规定变化发生的时间，或用关键词 “from” 和 “to”，等同于 0% 和 100%。 &lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style type=\"text/css\"&gt; div &#123; width: 100px; height: 100px; background: red; animation: test1 10s; &#125; @keyframes test1 &#123; 0% &#123;background: black&#125; 25% &#123;background: yellow&#125; 50% &#123;background: green&#125; 75% &#123;background: white&#125; 100% &#123;background: red&#125; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; &lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;style type=\"text/css\"&gt; div &#123; width: 100px; height: 100px; background: red; position: relative; animation: test 10s linear 1s infinite alternate running; &#125; @keyframes test &#123; 0% &#123;background:red; left:0px; top:0px;&#125; 25% &#123;background:yellow; left:200px; top:0px;&#125; 50% &#123;background:blue; left:200px; top:200px;&#125; 75% &#123;background:green; left:0px; top:200px;&#125; 100% &#123;background:red; left:0px; top:0px;&#125; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; CSS 变换通过 CSS3 转换，我们能够对元素进行移动、缩放、转动、拉长或拉伸。 translate 通过 translate() 方法，元素从其当前位置移动，根据给定的 left（x 坐标） 和 top（y 坐标） 位置参数： div&#123; width:100px; height:75px; background-color:yellow; border:1px solid black;&#125;div#div2&#123; transform:translate(50px,100px); -ms-transform:translate(50px,100px); /* IE 9 */ -moz-transform:translate(50px,100px); /* Firefox */ -webkit-transform:translate(50px,100px); /* Safari and Chrome */ -o-transform:translate(50px,100px); /* Opera */&#125; rotate 通过 rotate() 方法，元素顺时针旋转给定的角度。允许负值，元素将逆时针旋转。 div&#123; width:100px; height:75px; background-color:yellow; border:1px solid black;&#125;div#div2&#123; transform:rotate(30deg); -ms-transform:rotate(30deg); /* IE 9 */ -moz-transform:rotate(30deg); /* Firefox */ -webkit-transform:rotate(30deg); /* Safari and Chrome */ -o-transform:rotate(30deg); /* Opera */&#125; scale 通过 scale() 方法，元素的尺寸会增加或减少，根据给定的宽度（X 轴）和高度（Y 轴）参数： div&#123; width:100px; height:75px; background-color:yellow; border:1px solid black;&#125;div#div2&#123; margin:100px; transform:scale(2,4); -ms-transform:scale(2,4); /* IE 9 */ -moz-transform:scale(2,4); /* Firefox */ -webkit-transform:scale(2,4); /* Safari and Chrome */ -o-transform:scale(2,4); /* Opera */&#125; CSS 盒子模型CSS 框模型 (Box Model) 规定了元素框处理元素内容、内边距、边框 和 外边距 的方式。 元素框的最内部分是实际的内容，直接包围内容的是内边距。内边距呈现了元素的背景。内边距的边缘是边框。边框以外是外边距，外边距默认是透明的，因此不会遮挡其后的任何元素。 提示：背景应用于由内容和内边距、边框组成的区域。 在 CSS 中，width 和 height 指的是内容区域的宽度和高度。增加内边距、边框和外边距不会影响内容区域的尺寸，但是会增加元素框的总尺寸。 假设框的每个边上有 10 个像素的外边距和 5 个像素的内边距。如果希望这个元素框达到 100 个像素，就需要将内容的宽度设置为 70 像素，请看下图： #box &#123; width: 70px; margin: 10px; padding: 5px;&#125; 参考https://www.w3school.com.cn/ https://blog.csdn.net/qq_43539599/article/details/97820848 https://blog.csdn.net/DYD850804/article/details/80997251","path":"2019/10/22/CSS-入门/","date":"10-22","excerpt":"","tags":[{"name":"CSS","slug":"CSS","permalink":"https://litianbo243.github.io/tags/CSS/"},{"name":"学习CSS","slug":"学习CSS","permalink":"https://litianbo243.github.io/tags/学习CSS/"}]},{"title":"HTML 入门","text":"HTML 入门HTML5 元素表： http://www.html5star.com/manual/html5label-meaning/ HTML head 元素是所有头部元素的容器。 内的元素可包含脚本，指示浏览器在何处可以找到样式表，提供元信息，等等。 以下标签都可以添加到 head 部分：、、、、 以及 。 title: 定义文档标题 base: 都没货页面上所有链接的默认地址或默认目标 link: 定义文档与外部资源（外链样式表等）之间的关系 style: 定义文档的样式信息（内联样式表） meta: 定义关于html文档的元数据 script: 定义客户端脚本（插入js） &lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;!-- 防止乱码 --&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;定义文档标题&lt;/title&gt; &lt;base href=\"https://www.baidu.com/img/\" /&gt; &lt;base target=\"_blank\" /&gt; &lt;!-- 外联样式表 --&gt; &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\" /&gt; &lt;!-- ⽂档tag图标 --&gt; &lt;link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"http://www.baidu.com/favicon.ico\" /&gt; &lt;!-- 比shortcut icon 多支持gif，推荐一般时候使用shortcut icon --&gt; &lt;link rel=\"icon\" type=\"image/x-icon\" href=\"http://www.baidu.com/favicon.ico\" /&gt; &lt;style type=\"text/css\"&gt; P&#123; color: blue; &#125; h1&#123; color: red; &#125; &lt;/style&gt; &lt;!-- 编写发送浏览器的头部信息 --&gt; &lt;!-- content-type: text/html;charset=utf-8 --&gt; &lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\" /&gt; &lt;!-- charset:iso-8859-1 --&gt; &lt;meta http-equiv=\"charset\" content=\"iso-8859-1\"&gt; &lt;!-- expires:31 Dec 2008 --&gt; &lt;meta http-equiv=\"expires\" content=\"31 Dec 2008\"&gt; &lt;!-- SEO --&gt; &lt;!-- keyords 常用于搜索引擎可进行模糊搜索到该网站的关键字 --&gt; &lt;meta name=\"keywords\" content=\"8-12个以英⽂逗号隔开的单词或词语\"&gt; &lt;!-- description 是网站的相关介绍 --&gt; &lt;meta name=\"description\" content=\"80字以内的⼀段话，与⽹站内容相关\"&gt; &lt;!-- viewport 进行移动界面自适配，并且禁止用户进行修改适配大小 --&gt; &lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1,minimumscale=1,maximum-scale=1,user-scalable=no\" /&gt; &lt;!-- 可以放在head内也可以放在body内和外，通常放在body下，html内 --&gt; &lt;script type=\"text/javascript\"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;!-- 通过相对路径查找文件，默认从base指定路径文件夹内寻找 即：https://www.baidu.com/img/xinshouye_353af22a7f305e1fb6cfa259394dea9b.png --&gt; &lt;img src=\"xinshouye_353af22a7f305e1fb6cfa259394dea9b.png\" /&gt;&lt;br /&gt; &lt;h1&gt;hello style&lt;/h1&gt; &lt;p&gt;hello h5&lt;/p&gt; &lt;script type=\"text/javascript\"&gt; &lt;!-- js代码块 --&gt; document.write(\"&lt;h1&gt;Hello World!&lt;/h1&gt;\") &lt;/script&gt; &lt;/body&gt; &lt;script type=\"text/javascript\"&gt;&lt;/script&gt; &lt;/html&gt; HTML 基本元素 a: 超链接 b: 粗体 em: 斜体 u: 下划线 s: 删除线 &lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;基本元素&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;a href=\"http://www.w3school.com.cn\"&gt;W3School&lt;/a&gt; &lt;br /&gt; hello world! &lt;br /&gt; &lt;b&gt;hello world!&lt;/b&gt; &lt;br /&gt; &lt;em&gt;hello world!&lt;/em&gt; &lt;br /&gt; &lt;u&gt;hello world!&lt;/u&gt; &lt;br /&gt; &lt;s&gt;hello world!&lt;/s&gt; &lt;br /&gt;&lt;/body&gt;&lt;/html&gt; html 表格元素 table: 表格 tr: 行 td: 数据 caption: 标题 一个简单的表格： &lt;table border=\"1\"&gt; &lt;caption&gt;简单表格&lt;/caption&gt; &lt;tr&gt; &lt;td&gt;100&lt;/td&gt; &lt;td&gt;200&lt;/td&gt; &lt;td&gt;300&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; th:表头 &lt;table border=\"1\"&gt; &lt;caption&gt;简单表格&lt;/caption&gt; &lt;tr&gt; &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;职业&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;张三&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;td&gt;学生&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;李四&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;程序员&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; 合并单元格： &lt;table border=\"1\" width=\"300px\"&gt; &lt;caption&gt;简单表格&lt;/caption&gt; &lt;tr&gt; &lt;th rowspan=\"4\"&gt;基本情况&lt;/th&gt; //rowspan=\"4\"表示占一列的4个单元格 &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;职业&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;张三&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;td&gt;学生&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;李四&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;程序员&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td colspan=\"3\"&gt;共统计：2人&lt;/td&gt; //colspan=\"3\" 表示水平共占3个单元格。 &lt;/tr&gt;&lt;/table&gt; 表头和表脚： 使用thead可以指定一行为表头，可以在代码的任何地方。表头的内容是粗体显示，居中。tfoot 表示表脚。显示的指出哪一行为表脚tbody表示表的主体部分，可以不写 &lt;table border=\"1\" width=\"300px\"&gt; &lt;caption&gt;简单表格&lt;/caption&gt; &lt;thead&gt; &lt;tr&gt; &lt;th rowspan=\"4\"&gt;基本情况&lt;/th&gt; //rowspan=\"4\"表示占一列的4个单元格 &lt;th&gt;姓名&lt;/th&gt; &lt;th&gt;年龄&lt;/th&gt; &lt;th&gt;职业&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tr&gt; &lt;td&gt;张三&lt;/td&gt; &lt;td&gt;20&lt;/td&gt; &lt;td&gt;学生&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;李四&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;程序员&lt;/td&gt; &lt;/tr&gt; &lt;tfoot&gt; &lt;tr&gt; &lt;td colspan=\"3\"&gt;共统计：2人&lt;/td&gt; //colspan=\"3\" 表示水平共占3个单元格。 &lt;/tr&gt; &lt;/tfoot&gt;&lt;/table&gt; html 列表元素 ol: 有序列表 ul: 无序列表 li: 列表中的项 &lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;ol type=\"i\"&gt; &lt;li&gt;a&lt;/li&gt; &lt;li&gt;a&lt;/li&gt; &lt;li&gt;a&lt;/li&gt; &lt;li&gt;a&lt;/li&gt; &lt;li&gt;a&lt;/li&gt; &lt;li&gt;a&lt;/li&gt; &lt;li&gt;a&lt;/li&gt;&lt;/ol&gt;&lt;ol reversed&gt; &lt;li&gt;a&lt;/li&gt; &lt;li&gt;a&lt;/li&gt; &lt;li&gt;a&lt;/li&gt;&lt;/ol&gt;&lt;ol&gt; &lt;li&gt;处理图像&lt;/li&gt; &lt;ol type=\"a\"&gt; &lt;li&gt;在页面中插入图片&lt;/li&gt; &lt;li&gt;在图片和文件打印样式中提取文本&lt;/li&gt; &lt;li&gt;在笔记本页中插入屏幕剪辑&lt;/li&gt; &lt;/ol&gt;&lt;/ol&gt;&lt;ul&gt; &lt;li&gt;a&lt;/li&gt; &lt;li&gt;a&lt;/li&gt; &lt;li&gt;a&lt;/li&gt;&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt; HTML 表单 form: HTML表单，用于收集用户输入。 input: 表单元素，根据不同的type属性有不同的形态 &lt;!--文本输入&lt;input type=\"text\"&gt;--&gt;&lt;form&gt; First name: &lt;br /&gt; &lt;input type=\"text\" name=\"firstname\" /&gt; &lt;br /&gt; Last name: &lt;br /&gt; &lt;input type=\"text\" name=\"lastname\" /&gt; &lt;br /&gt;&lt;/form&gt; &lt;!--单选按钮输入&lt;input type=\"radio\"&gt;--&gt;&lt;form&gt; &lt;input type=\"radio\" name=\"sex\" value=\"male\" checked /&gt; Male &lt;br /&gt; &lt;input type=\"radio\" name=\"sex\" value=\"female\" /&gt; Female &lt;br /&gt;&lt;/form&gt; &lt;!--提交按钮&lt;input type=\"submit\"&gt;--&gt;&lt;form action=\"action_page.php\"&gt; First name:&lt;br /&gt; &lt;input type=\"text\" name=\"firstname\" value=\"Mickey\"&gt; &lt;br /&gt; Last name:&lt;br /&gt; &lt;input type=\"text\" name=\"lastname\" value=\"Mouse\"&gt; &lt;br /&gt;&lt;br /&gt; &lt;input type=\"submit\" value=\"Submit\"&gt;&lt;/form&gt; select: 下拉列表 &lt;!--下拉列表--&gt;&lt;form&gt; &lt;select name=\"cars\"&gt; &lt;option value=\"volvo\"&gt;Volvo&lt;/option&gt; &lt;option value=\"saab\"&gt;Saab&lt;/option&gt; &lt;option value=\"fiat\"&gt;Fiat&lt;/option&gt; &lt;option value=\"audi\" selected&gt;Audi&lt;/option&gt; &lt;/select&gt;&lt;/form&gt; textarea: 多行输入字段 &lt;!--多行输入字段--&gt;&lt;form&gt; &lt;textarea name=\"message\" row=\"100\" cols=\"100\"&gt; The cat was playing in the garden. &lt;/textarea&gt;&lt;/form&gt; button: 定义可点击的按钮 &lt;!--按钮--&gt;&lt;form&gt; &lt;button type=\"button\" onclick=\"alert('Hello World!')\"&gt;Click Me!&lt;/button&gt;&lt;/form&gt; datalist: 预定义选项列表 &lt;!--预定义选项列表--&gt;&lt;form action=\"action_page.php\"&gt; &lt;input type=\"text\" list=\"browsers\" /&gt; &lt;datalist id=\"browsers\"&gt; &lt;option value=\"Internet Explorer\" /&gt; &lt;option value=\"Firefox\" /&gt; &lt;option value=\"Chrome\" /&gt; &lt;option value=\"Opera\" /&gt; &lt;option value=\"Safari\" /&gt; &lt;/datalist&gt;&lt;/form&gt; input type=”password”: 定义密码字段 &lt;form&gt; User name:&lt;br/&gt; &lt;input type=\"text\" name=\"username\"/&gt; &lt;br/&gt; User password:&lt;br/&gt; &lt;input type=\"password\" name=\"userpassword\"/&gt;&lt;/form&gt; input type=”submit”: 定义提交表单数据至表单处理程序的按钮 &lt;form action=\"action._page.php\"&gt; First name:&lt;br/&gt; &lt;input type=\"text\" name=\"firstname\" value=\"Mickey\"/&gt; &lt;br/&gt; Last name:&lt;br/&gt; &lt;input type=\"text\" name=\"lastname\" value=\"Mouse\"/&gt; &lt;br/&gt;&lt;br/&gt; &lt;input type=\"submit\" value=\"Submit\"/&gt;&lt;/form&gt; input type=”checkbox”: 定义复选框 &lt;form&gt; &lt;input type=\"checkbox\" name=\"vehicle\" value=\"Bike\"/&gt; I have a bike &lt;br/&gt; &lt;input type=\"checkbox\" name=\"vehicle\" value=\"Car\"/&gt; I have a car &lt;br/&gt;&lt;/form&gt; input type=”button”: 定义按钮 &lt;form&gt; &lt;input type=\"button\" onclick=\"alert('Hello World!')\" value=\"Click Me!\"/&gt;&lt;/form&gt; input type=”number”: 包含数字值的输入字段 &lt;form&gt; Quantity(beween 1 and 5) &lt;br/&gt; &lt;input type=\"number\" name=\"quantity\" min=\"1\" max=\"5\"/&gt; &lt;br/&gt; Point &lt;input type=\"number\" name=\"points\" min=\"0\" max=\"100\" step=\"10\" value=\"30\"&gt; &lt;br/&gt;&lt;/form&gt; input type=”date”: 包含日期的输入字段 &lt;form&gt; Birthday:&lt;br/&gt; &lt;input type=\"date\" name=\"bday\"/&gt; &lt;br/&gt; Enter a date before 1980-01-01: &lt;input type=\"date\" name=\"bday\" max=\"1979-12-31\"/&gt;&lt;br/&gt; Enter a date after 2000-01-01: &lt;input type=\"date\" name=\"bday\" min=\"2000-01-02\"/&gt;&lt;br/&gt;&lt;/form&gt; input type=”color”: 包含颜色的输入字段 &lt;form&gt; Select your favorite color:&lt;br/&gt; &lt;input type=\"color\" name=\"favorite_color\"/&gt;&lt;/form&gt; input type=”range”: 包含一定范围的输入字段 &lt;form&gt; &lt;input type=\"range\" name=\"points\" min=\"0\" max=\"10\"/&gt;&lt;/form&gt; input type=”time”:允许用户选择时间 &lt;form&gt; Select a time: &lt;input type=\"time\" name=\"usr_time\"/&gt;&lt;/form&gt; HTML 图像&lt;img src=\"http://www.w3school.com.cn/i/w3school_logo_white.gif\" width=\"50\" height=\"50\" /&gt; 制作图像链接： &lt;a href=\"/example/html/lastpage.html\"&gt;&lt;img border=\"0\" src=\"/i/eg_buttonnext.gif\" /&gt;&lt;/a&gt; 分区响应图： &lt;img src=\"https://www.w3school.com.cn/i/eg_planets.jpg\" border=\"0\" usemap=\"#planetmap\" alt=\"Planets\"/&gt;&lt;map name=\"planetmap\" id=\"planetmap\"&gt;&lt;area shape=\"circle\" coords=\"180,139,14\" href=\"https://www.w3school.com.cn/i/eg_venus.gif\" target=\"_blank\" alt=\"Venus\"/&gt;&lt;area shape=\"circle\" coords=\"129,169,10\" href=\"https://www.w3school.com.cn/i/eg_merglobe.gif\" target=\"_blank\" alt=\"Mercury\"/&gt;&lt;area shape=\"rect\" coords=\"0,0,110,260\" href=\"https://www.w3school.com.cn/i/eg_sun.gif\" target=\"_blank\" alt=\"SUn\"/&gt;&lt;/map&gt; HTML5 视频当前，video 元素支持三种视频格式： 格式 IE Firefox Opera Chrome Safari Ogg No 3.5+ 10.5+ 5.0+ No MPEG 4 9.0+ No No 5.0+ 3.0+ WebM No 4.0+ 10.6+ 6.0+ No Ogg = 带有 Theora 视频编码和 Vorbis 音频编码的 Ogg 文件 MPEG4 = 带有 H.264 视频编码和 AAC 音频编码的 MPEG 4 文件 WebM = 带有 VP8 视频编码和 Vorbis 音频编码的 WebM 文件 &lt;video src=\"https://www.w3school.com.cn/i/movie.ogg\" width=\"320\" height=\"240\" controls=\"controls\"&gt; Your browser does not support the vedio tag.&lt;/video&gt; 做视频格式适配： &lt;video wid=\"320\" height=\"240\" controls&gt; &lt;source src=\"https://www.w3school.com.cn/i/movie.ogg\" type=\"video/ogg\"/&gt; &lt;source src=\"https://www.w3school.com.cn/i/movie.mp4\" type=\"video/mp4\"/&gt; Your browser does not support the video tag.&lt;/video&gt; 参考https://blog.csdn.net/qq_33961117/article/details/82774331 http://www.html5star.com/manual/html5label-meaning/ https://www.w3school.com.cn/html/ https://blog.csdn.net/qq_41490873/article/details/94365120","path":"2019/10/20/HTML-入门/","date":"10-20","excerpt":"","tags":[{"name":"HTML","slug":"HTML","permalink":"https://litianbo243.github.io/tags/HTML/"},{"name":"学习HTML","slug":"学习HTML","permalink":"https://litianbo243.github.io/tags/学习HTML/"}]},{"title":"Java 数据库","text":"Java 数据库Ubuntu安装mysqlsudo apt-get updatesudo apt-get install mysql-server mysql-clientsudo mysql_secure_installation mysql_secure_installation脚本设置的东西：更改root密码、移除MySQL的匿名用户、禁止root远程登录、删除test数据库。使用上面的这些选项可以提高MySQL的安全。 使用root用户登陆 mysql -u root -p 创建MySQL数据库 create database test; 上面命令创建了一个名为test的数据库。 创建用户 grant all on test.* to &quot;man_user&quot; identified by &quot;test1234&quot;; 新用户登录 mysql -u ltb -p test 创建表 CREATE TABLE user (id INT, name VARCHAR(20), email VARCHAR(20)); 插入记录 insert into user(id, name, email) values(1, &quot;bar&quot;, &quot;bar@gmail.com&quot;);insert into user(id, name, email) values(2, &quot;foo&quot;, &quot;foo@163.com&quot;);insert into user(id, name, email) values(3, &quot;cat&quot;, &quot;cat@gmail.com&quot;); 简单查询 SELECT * FROM user; 退出mysql命令行 quit 停止mysql数据库服务 sudo systemctl stop mysql.service 启动mysql数据库服务 sudo systemctl start mysql.service java连接mysqlpublic class MySQLDemo&#123; static final String JDBC_DRIVER = \"com.mysql.jdbc.Driver\"; static final String DB_URL = \"jdbc:mysql://localhost:3306/test?useSSL=fasle\"; static final String USER = \"ltb\"; static final String PASS = \"mima\"; public static void main(String[] args)&#123; Connection conn = null; Statement stmt = null; try&#123; // 注册 JDBC 驱动 Class.forName(JDBC_DRIVER); System.out.println(\"连接数据库...\"); // 打开链接 conn = DriverManager.getConnection(DB_URL, USER, PASS); // 执行查询 System.out.println(\"实例化Statement对象...\"); stmt = conn.createStatement(); String sql = \"SELECT id, name, email FROM user\"; ResultSet rs = stmt.executeQuery(sql); // 展开结果集数据库 while(rs.next())&#123; int id = rs.getInt(\"id\"); String name = rs.getString(\"name\"); String email = rs.getString(\"email\"); System.out.println(\"id: \" + id); System.out.println(\"name: \" + name); System.out.println(\"email: \" + email); &#125; // 完成后关闭 rs.close(); stmt.close(); conn.close(); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; try&#123; if(stmt!=null)&#123; stmt.close(); &#125; &#125;catch(Exception e)&#123;&#125; try&#123; if(conn!=null)&#123; conn.close(); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125; System.out.println(\"Goodbye!\"); &#125;&#125; 增添数据public class MySQLInsert &#123; static final String JDBC_DRIVER = \"com.mysql.jdbc.Driver\"; static final String DB_URL = \"jdbc:mysql://localhost:3306/test?useSSL=false\"; static final String USER = \"ltb\"; static final String PASS = \"mima\"; public static void main(String[] args) &#123; Connection conn = null; PreparedStatement psql = null; ResultSet rs = null; try &#123; Class.forName(JDBC_DRIVER); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; System.out.println(\"连接数据库...\"); try &#123; conn = DriverManager.getConnection(DB_URL, USER, PASS); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; String sql = \"INSERT INTO user(id, name, email)\" + \"VALUE(?, ?, ?)\"; try &#123; psql = conn.prepareStatement(sql); psql.setInt(1, 4); psql.setString(2, \"tiger\"); psql.setString(3, \"tiger@qq.com\"); psql.executeUpdate(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 删除数据package lesson20;import java.sql.*;public class MySQLDelete &#123; static final String JDBC_DRIVER = \"com.mysql.jdbc.Driver\"; static final String DB_URL = \"jdbc:mysql://localhost:3306/test?useSSL=false\"; static final String USER = \"ltb\"; static final String PASS = \"mima\"; public static void main(String[] args) &#123; Connection conn = null; PreparedStatement psql = null; ResultSet rs = null; try &#123; Class.forName(JDBC_DRIVER); System.out.println(\"连接数据库...\"); conn = DriverManager.getConnection(DB_URL, USER, PASS); psql = conn.prepareStatement(\"DELETE FROM user WHERE id = ?\"); psql.setInt(1, 2); psql.executeUpdate(); psql.close(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 更改数据public class MySQLUpdate &#123; static final String JDBC_DRIVER = \"com.mysql.jdbc.Driver\"; static final String DB_URL = \"jdbc:mysql://localhost:3306/test?useSSL=false\"; static final String USER = \"ltb\"; static final String PASS = \"mima\"; public static void main(String[] args) &#123; Connection conn = null; PreparedStatement psql = null; ResultSet rs = null; try &#123; Class.forName(JDBC_DRIVER); System.out.println(\"连接数据库...\"); conn = DriverManager.getConnection(DB_URL, USER, PASS); psql = conn.prepareStatement(\"UPDATE user SET id = ? WHERE id = ?\"); psql.setInt(1, 5); psql.setInt(2, 1); psql.executeUpdate(); psql.close(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 参考https://blog.csdn.net/chenqianfang/article/details/79958518 https://www.runoob.com/java/java-mysql-connect.html https://www.cnblogs.com/centor/p/6142775.html","path":"2019/10/13/Java-数据库/","date":"10-13","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java 多线程","text":"Java 多线程进程和线程什么是进程? 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。如下图所示，在 windows 中通过查看任务管理器的方式，我们就可以清楚看到 window 当前运行的进程（.exe文件的运行）。 什么是线程? 线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 什么是多线程? 多线程就是多个线程同时运行或交替运行。单核CPU的话是顺序执行，也就是交替运行。多核CPU的话，因为每个CPU有自己的运算器，所以在多个CPU中可以同时运行。 为什么多线程是必要的? 开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 为什么提倡多线程而不是多进程? 线程就是轻量级进程，是程序执行的最小单位。使用多线程而不是用多进程去进行并发程序的设计，是因为线程间的切换和调度的成本远远小于进程。 创建多线程1. 继承Thread类 public class ThreadCreateDemo&#123; public static void main(String[] args)&#123; Mythread thread = new MyThread(); thread.start(); // 该方法调用多次,出现IllegalThreadStateException &#125;&#125;class MyThread extends Thread&#123; @Override public void run()&#123; super.run(); System.out.println(\"通过继承Thread创建的线程!\"); &#125;&#125; 2.实现Runnable接口(推荐) public class ThreadCreateDemo &#123; public static void main(String[] args) &#123; Runnable runnable = new MyRunnable(); Thread thread = new Thread(runnable); thread.start(); &#125;&#125;class MyRunnable implements Runnable&#123; public void run()&#123; System.out.println(\"通过Runnable创建的线程!\") &#125;&#125; 上述两种创建方式,工作时性质一样。但是建议使用实现Runable接口方式。解决单继承的局限性。 实例变量和线程安全不共享数据的情况下 public class MyThread extends Thread &#123; private int count = 5; public MyThread(String name) &#123; super(); this.setName(name); &#125; @Override public void run() &#123; super.run(); while (count &gt; 0) &#123; count--; System.out.println(\"由 \" + MyThread.currentThread().getName() + \" 计算，count=\" + count); &#125; &#125; public static void main(String[] args) &#123; MyThread a = new MyThread(\"A\"); MyThread b = new MyThread(\"B\"); MyThread c = new MyThread(\"C\"); a.start(); b.start(); c.start(); &#125;&#125; 结果: 可以看出每个线程都有一个属于自己的实例变量count，它们之间互不影响。我们再来看看另一种情况。 共享数据的情况 public class SharedVariableThread extends Thread &#123; private int count = 5; @Override public void run() &#123; super.run(); count--; System.out.println(\"由 \" + SharedVariableThread.currentThread().getName() + \" 计算，count=\" + count); &#125; public static void main(String[] args) &#123; SharedVariableThread mythread = new SharedVariableThread(); // 下列线程都是通过mythread对象创建的 Thread a = new Thread(mythread, \"A\"); Thread b = new Thread(mythread, \"B\"); Thread c = new Thread(mythread, \"C\"); Thread d = new Thread(mythread, \"D\"); Thread e = new Thread(mythread, \"E\"); a.start(); b.start(); c.start(); d.start(); e.start(); &#125;&#125; 结果: 可以看出这里已经出现了错误。 如何解决 利用 synchronized 关键字（保证任意时刻只能有一个线程执行该方法） 同步锁(synchronized)synchronized修饰实例方法 这样的含义是锁住当前实例对象，多个线程运行该方法，只有一个线程能够获得该实例的锁，执行方法。 public synchronized void function()&#123; System.out.println(\"function同步锁中\");&#125; synchronized修饰静态方法 这样的含义是锁住当前类对象，此时与创建的对象无关，只有一个线程能够该类对象的锁。 public static synchronized void staticFunction()&#123; System.out.println(\"staticFunction同步锁中\");&#125; synchronized锁住代码块 对指定的对象加锁，那么多个线程中，只有一个线程能够获得该对象的锁。 Object obj = new Object();synchronized(obj)&#123; System.out.println(\"我在obj对象锁中\");&#125; 停止线程终止正在运行的线程方法有三种： 使用退出表示,是线程正常的执行完run方法终止. public class ThreadVariableStopDemo &#123; public static void main(String[] args) throws InterruptedException &#123; VariableStopThread thread = new VariableStopThread(\"thread_1\"); thread.start(); Thread.sleep(10); thread.interrupt(); &#125;&#125;class VariableStopThread extends Thread &#123; public VariableStopThread(String name) &#123; super(name); &#125; public void run() &#123; System.out.println(Thread.currentThread().getName() + \":线程开始运行!\"); while(!isInterrupted()) &#123; System.out.println(\"\" + (i++)); &#125; System.out.println(\"我停止了! timer:\" + System.currentTimeMillis()); &#125;&#125; 结果: 使用interrupt方法,使线程异常,线程进行捕获或抛异常,正常执行完run方法终止. public class ThreadInterruptDemo &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new InterruptThread(\"thread_1\"); thread.start(); Thread.sleep(1); System.out.println(thread.getName() + \"线程设置:interrupt\"); thread.interrupt(); &#125;&#125;class InterruptThread extends Thread &#123; public InterruptThread(String name) &#123; super(name); &#125; @Override public void run() &#123; System.out.println(Thread.currentThread().getName() + \"线程开始!\"); for(int i =0; i &lt; 1000; i++) &#123; try &#123; Thread.sleep(0); System.out.println(\"\" + (i + 1)); &#125; catch (InterruptedException e) &#123; System.out.println(Thread.currentThread().getName() + \"线程捕获异常,退出循环!\"); break; &#125; &#125; System.out.println(Thread.currentThread().getName() + \"线程结束!\"); &#125;&#125; 结果: 线程优先级线程优先级范围为1-10，API提供等级分为： 低（MIN_PRIORITY = 1)， 中（NORM_PRIORITY=5）， 高（MAX_PRIORITY=10）。 public class ThreadPriorityDemo &#123; public static void main(String[] args) &#123; Thread thread = new ThreadPriority(\"thread_1&lt;&lt;&lt;&lt;\"); Thread thread_1 = new ThreadPriority(\"&gt;&gt;&gt;thread_2\"); thread_1.setPriority(Thread.MIN_PRIORITY); //&lt;设置线程优先级 thread.setPriority(Thread.MAX_PRIORITY); thread_1.start(); thread.start(); &#125;&#125;class ThreadPriority extends Thread &#123; public ThreadPriority(String name) &#123; super(name); &#125; @Override public void run() &#123; for (int i = 0; i &lt; 10; i++) &#123; System.out.println(\"\" + Thread.currentThread().getName() + \",number:\" + i + \",Priority:\" + Thread.currentThread().getPriority()); &#125; &#125;&#125; 结果: 运行的很给力，以下体现了两个问题:①线程运行顺序与代码执行顺序无关。②线程优先级具有随机性，不是优先级高的就先完成。 守护线程用户线程：运行在前台，执行具体的任务，如程序的主线程、连接网络的子线程等都是用户线程 守护线程：运行在后台，为其他前台线程服务.也可以说守护线程是JVM中非守护线程的 “佣人”。 特点：一旦所有用户线程都结束运行，守护线程会随JVM一起结束工作 应用：数据库连接池中的检测线程，JVM虚拟机启动后的检测线程 最常见的守护线程：垃圾回收线程 守护线程顾名思义是一个线程守护另一个线程【此线程为非守护线程】，故守护的线程称为守护线程，被守护的线程称为非守护线程。作用是为其他线程运行提供便利服务。 public class DaemonThreadDemo &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread = new DaemonThread(); thread.setDaemon(true); thread.start(); System.out.println(\"\" + Thread.currentThread().getName() + \"停止运行!\" ); &#125;&#125;class DaemonThread extends Thread &#123; @Override public void run() &#123; while (true) &#123; System.out.println(\"DaemonThread 正在运行!\"); &#125; &#125;&#125; 结果: 从上图可以看出，主线程停止DaemonThread线程也相应的停止了，但不是立即停止。 线程让步线程让步【yield方法】让当前线程释放CPU资源，让其他线程抢占。 public class ThreadYieldDemo &#123; public static void main(String[] args) &#123; Thread thread = new ThreadYield(); thread.start(); &#125;&#125;class ThreadYield extends Thread &#123; @Override public void run() &#123; long time_start = System.currentTimeMillis(); for(int i = 0; i &lt; 500000; i++) &#123; Math.random();// Thread.yield(); &#125; long time_end = System.currentTimeMillis(); System.out.println(\"用时：\" + (time_end - time_start)); &#125;&#125; 不让步: 让步: 从以上两图可以看出，线程的让步操作比不让步耗时长。 join()方法Java对Thread的Join方法解释：等待当前线程终止。 public class TestJoin &#123; public static void main(String[] args) throws InterruptedException &#123; // TODO Auto-generated method stub ThreadTest t1=new ThreadTest(\"A\"); ThreadTest t2=new ThreadTest(\"B\"); t1.start(); t1.join(); t2.start(); &#125;&#125;class ThreadTest extends Thread &#123; private String name; public ThreadTest(String name)&#123; this.name=name; &#125; public void run()&#123; for(int i=1;i&lt;=5;i++)&#123; System.out.println(name+\"-\"+i); &#125; &#125;&#125; 结果: A-1A-2A-3A-4A-5B-1B-2B-3B-4B-5 显然，使用t1.join()之后，B线程需要等A线程执行完毕之后才能执行。 sleep()方法public class Multi extends Thread&#123; public void run() &#123; for(int i=1; i&lt;1000; i++) &#123; try &#123; Thread.sleep(500); &#125;catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(i); &#125; &#125; public static void main(String[] args) &#123; Multi t1 = new Multi(); Multi t2 = new Multi(); t1.start(); t2.start(); &#125;&#125; 结果: t1: 1t2: 1t1: 2t2: 2t1: 3…… 这是因为Sleep()使得当前线程进入阻塞状态，系统便调用了另一线程，循环往复，便出现了上面的输出结果。 wait()和notify()java的wait/notify的通知机制可以用来实现线程间通信。 wait表示线程的等待，调用该方法会导致线程阻塞，直至另一线程调用notify或notifyAll方法才可另其继续执行。 public class ThreadTest &#123; static final Object obj = new Object(); private static boolean flag = false; public static void main(String[] args) throws Exception &#123; Thread consume = new Thread(new Consume(), \"Consume\"); Thread produce = new Thread(new Produce(), \"Produce\"); consume.start(); Thread.sleep(1000); produce.start(); try &#123; produce.join(); consume.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; // 生产者线程 static class Produce implements Runnable &#123; @Override public void run() &#123; synchronized (obj) &#123; System.out.println(\"进入生产者线程\"); System.out.println(\"生产\"); try &#123; TimeUnit.MILLISECONDS.sleep(2000); //模拟生产过程 flag = true; obj.notify(); //通知消费者 TimeUnit.MILLISECONDS.sleep(1000); //模拟其他耗时操作 System.out.println(\"退出生产者线程\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //消费者线程 static class Consume implements Runnable &#123; @Override public void run() &#123; synchronized (obj) &#123; System.out.println(\"进入消费者线程\"); System.out.println(\"wait flag 1:\" + flag); while (!flag) &#123; //判断条件是否满足，若不满足则等待 try &#123; System.out.println(\"还没生产，进入等待\"); obj.wait(); System.out.println(\"结束等待\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(\"wait flag 2:\" + flag); System.out.println(\"消费\"); System.out.println(\"退出消费者线程\"); &#125; &#125; &#125;&#125; 结果: 进入消费者线程wait flag 1:false还没生产，进入等待进入生产者线程生产退出生产者线程结束等待wait flag 2:true消费退出消费者线程 在示例中没有体现但很重要的是，wait/notify方法的调用必须处在该对象的锁（Monitor）中，也即，在调用这些方法时首先需要获得该对象的锁。否则会抛出IllegalMonitorStateException异常。 从输出结果来看，在生产者调用notify()后，消费者并没有立即被唤醒，而是等到生产者退出同步块后才唤醒执行。（这点其实也好理解，synchronized同步方法（块）同一时刻只允许一个线程在里面，生产者不退出，消费者也进不去） 注意，消费者被唤醒后是从wait()方法（被阻塞的地方）后面执行，而不是重新从同步块开头。 线程的生命周期 新建状态: 使用 new 关键字和 Thread 类或其子类建立一个线程对象后，该线程对象就处于新建状态。它保持这个状态直到程序 start() 这个线程。 就绪状态: 当线程对象调用了start()方法之后，该线程就进入就绪状态。就绪状态的线程处于就绪队列中，要等待JVM里线程调度器的调度。 运行状态: 如果就绪状态的线程获取 CPU 资源，就可以执行 run()，此时线程便处于运行状态。处于运行状态的线程最为复杂，它可以变为阻塞状态、就绪状态和死亡状态。 阻塞状态: 如果一个线程执行了sleep（睡眠）、suspend（挂起）等方法，失去所占用资源之后，该线程就从运行状态进入阻塞状态。在睡眠时间已到或获得设备资源后可以重新进入就绪状态。可以分为三种： 等待阻塞：运行状态中的线程执行 wait() 方法，使线程进入到等待阻塞状态。 同步阻塞：线程在获取 synchronized 同步锁失败(因为同步锁被其他线程占用)。 其他阻塞：通过调用线程的 sleep() 或 join() 发出了 I/O 请求时，线程就会进入到阻塞状态。当sleep() 状态超时，join() 等待线程终止或超时，或者 I/O 处理完毕，线程重新转入就绪状态。 死亡状态: 一个运行状态的线程完成任务或者其他终止条件发生时，该线程就切换到终止状态。 参考https://www.jianshu.com/p/d901b25e0d4a https://blog.csdn.net/qq_34337272/article/details/79640870#__10 https://www.runoob.com/java/java-multithreading.html https://blog.csdn.net/u013425438/article/details/80205693 https://www.cnblogs.com/hqinglau/p/10053564.html https://blog.csdn.net/jianiuqi/article/details/53448849 https://blog.csdn.net/wthfeng/article/details/78762343 https://baijiahao.baidu.com/s?id=1630613830514012483&amp;wfr=spider&amp;for=pc","path":"2019/10/12/Java-多线程/","date":"10-12","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java Socket","text":"Java SocketSocket，即套接字，是一种介于应用和传输层之间的抽象层，可实现同网络内不同应用之间相互发送和接收数据。 对于 Socket，有以下三个关键问题： 1. 如何定位到目标应用 在发送和接收数据时，关键在于如何定位到应用，而定位到应用首先需要定位到主机。 在同一子网内，每台主机的 IP 地址是唯一的，故可以借助 IP 地址定位主机，而在每一台主机上，不同的程序往往监听在不同的端口号上，因此，可以利用端口号来定位应用。 因此，一个 socket 由一个 url 地址和一个 port 端口号唯一确定。 2. 与TCP/IP的关系 socket 与 TCP/IP 协议簇无关，其本质是编程接口，用于向应用提供数据传输服务的接口。Java 中的 socket 主要是基于 TCP/IP 的封装，在使用过程中，应用可借助socket接口，建立基于TCP或UDP的数据传输机制，从而实现同网络跨应用之间的数据传输功能，从而将应用与传输层的具体协议分离开来，使得上层应用无需关注过多细节，只专注数据传输即可。 3. Socket的工作过程 根据传输类型选择 socket 接口进行调用，以 TCP/IP 协议簇为例，主要包括两种socket，即针对 TCP 的流 socket 和针对 UDP 的数据报 socket。 根据目的应用主机、所在端口号等信息实现寻址过程。 链接建立与数据传输。 关闭链接。 TCP Echo Request 实例Server端 import java.io.IOException;import java.io.InputStream;import java.io.OutputStream;import java.net.ServerSocket;import java.net.Socket;import java.net.SocketAddress;public class TCPEchoServer &#123; private static final int BUFSIZE = 32; public static void main(String[] args) throws IOException &#123; // 参数校验，传入参数 端口号 if (args.length != 1) &#123; throw new IllegalArgumentException(\"Parameter(s): &lt;Port&gt;\"); &#125; int servPort = Integer.parseInt(args[0]); // 创建 ServerSocket 实例，并监听给定端口号 servPort ServerSocket servSock = new ServerSocket(servPort); int recvMsgSize; byte[] receiveBuf = new byte[BUFSIZE]; while (true) &#123; // 用于获取下一个客户端连接，根据连接创建 Socket 实例 Socket clntSock = servSock.accept(); // 获取客户端地址和端口号 SocketAddress clientAddress = clntSock.getRemoteSocketAddress(); System.out.println(\"Handling client at \" + clientAddress); // 获取 socket 的输入输出流 InputStream in = clntSock.getInputStream(); OutputStream out = clntSock.getOutputStream(); // 每次从输入流中读取数据并写到输出流中，直至输入流为空 while ((recvMsgSize = in.read(receiveBuf)) != -1) &#123; out.write(receiveBuf, 0, recvMsgSize); &#125; // 关闭 Socket clntSock.close(); &#125; &#125;&#125; 说明： Socket 中的输入输出流是流抽象，可看做一个字符序列，输入流支持读取字节，输出流支持取出字节。每个 Socket 实例都维护了 一个 InputStream 和一个 OutputStream 实例，数据传输也主要依靠从流中获取数据并解析的过程。 ServerSocket 与 Socket 区别，ServerSocket 主要用于服务端，用于为新的 TCP 连接请求提供一个新的已连接的 Socket 实例。Socket 则用于服务端和客户端，用于表示 TCP 连接的一端。因此，服务端需要同时处理 ServerSocket 和 Socket 实例，而客户端只需要处理 Socket 实例即可。 Client端 import java.io.IOException;import java.io.InputStream;import java.io.OutputStream;import java.net.Socket;import java.net.SocketException;public class TCPEchoClient &#123; public static void main(String[] args) throws IOException &#123; // 参数校验，传入格式 url \"info\" 或 url \"info\" 10240 if ((args.length &lt; 2) || (args.length &gt; 3)) &#123; throw new IllegalArgumentException(\"Parameter(s): &lt;server&gt;&lt;word&gt; [&lt;Port&gt;]\"); &#125; // 获取目标应用 url String server = args[0]; // 将传送数据转化为字节数组 byte[] data = args[1].getBytes(); // 解析端口号，若无则设为 10240 int servPort = (args.length == 3) ? Integer.parseInt(args[2]) : 10240; // 根据参数创建 Socket 实例 Socket socket = new Socket(server, servPort); System.out.println(\"Connected to server... sending echo string\"); // 获取 socket 的输入输出流 InputStream in = socket.getInputStream(); OutputStream out = socket.getOutputStream(); // 将数据写入到 Socket 的输出流中，并发送数据 out.write(data); int totalBytesRcvd = 0; int bytesRcvd; // 接收返回信息 while (totalBytesRcvd &lt; data.length) &#123; if ((bytesRcvd = in.read(data, totalBytesRcvd, data.length - totalBytesRcvd)) == -1) &#123; throw new SocketException(\"Connection closed permaturely\"); &#125; totalBytesRcvd += bytesRcvd; &#125; System.out.println(\"Received: \" + new String(data)); // 关闭 Socket socket.close(); &#125;&#125; 说明： 发送数据时只通过 write() 方法，接收时为何需要多个 read() 方法?TCP 协议无法确定在 read() 和 write() 方法中所发送信息的界限，而且发送过程中可能存在乱序现象，即分割成多个部分，所以无法通过一次 read() 获取到全部数据信息。 UDP Echo Request 实例Server端 import java.io.IOException;import java.net.DatagramPacket;import java.net.DatagramSocket;import java.net.SocketException;public class UDPEchoServer &#123; private static final int ECHOMAX = 255; public static void main(String[] args) throws IOException &#123; // 参数校验，格式 port if (args.length != 1) &#123; throw new IllegalArgumentException(\"Parameter(s): &lt;Port&gt;\"); &#125; // 获取端口号 int servPort = Integer.parseInt(args[0]); // 创建数据报文 Socket DatagramSocket socket = new DatagramSocket(servPort); // 创建数据报文 DatagramPacket packet = new DatagramPacket(new byte[ECHOMAX], ECHOMAX); while (true) &#123; // 接收请求报文 socket.receive(packet); System.out.println(\"Handling client at \" + packet.getAddress().getHostAddress() + \" on port \" + packet.getPort()); // 发送数据报文 socket.send(packet); // 重置缓存区大小 packet.setLength(ECHOMAX); &#125; &#125;&#125; 说明： UDP服务端 与 TCP 服务端不同，TCP 对于每一个客户端请求都需要先建立连接，而 UDP 则不需要。因此，UDP 只需创建一个 Socket 等待客户端连接即可。 在该 UDP 服务器的实现中，只接收和发送数据报文中的前 ECHOMAX 个字符，超出部分直接丢弃。 在处理过接收到的消息后，数据包的内部长度会设置为刚处理过的消息长度，通常比初始长度要短，因此需重置缓冲区为初始长度。否则后续可能会使得缓冲区长度不断减小，使得数据包被截断。 Client端 import java.io.IOException;import java.io.InterruptedIOException;import java.net.*;public class UDPEchoClient &#123; private static final int TIMEOUT = 3000; private static final int MAXTRIES = 5; public static void main(String[] args) throws IOException &#123; // 参数解析，格式 url \"info\" 或 url \"info\" 10240 if ((args.length &lt; 2) || (args.length &gt; 3)) &#123; throw new IllegalArgumentException(\"Parameter(s): &lt;Server&gt; &lt;Word&gt; [&lt;Port&gt;]\"); &#125; // 创建目标 Server IP 地址对象 InetAddress serverAddress = InetAddress.getByName(args[0]); // 将需传输字符转换为字节数组 byte[] byteToSend = args[1].getBytes(); // 获取服务端端口号，默认 10241 int servPort = (args.length == 3) ? Integer.parseInt(args[2]) : 10241; // 创建 UDP 套接字，选择本地可用的地址和可用端口号 DatagramSocket socket = new DatagramSocket(); // 设置超时时间，用于控制 receive() 方法调用的实际最短阻塞时间 socket.setSoTimeout(TIMEOUT); // 创建发送数据报文 DatagramPacket sendPacket = new DatagramPacket(byteToSend, byteToSend.length, serverAddress, servPort); // 创建接收数据报文 DatagramPacket receivePacket = new DatagramPacket(new byte[byteToSend.length], byteToSend.length); // 设置最大重试次数，以减少数据丢失产生的影响 int tries = 0; // 是否收到响应 boolean receivedResponse = false; do &#123; // 将数据报文传输到指定服务器和端口 socket.send(sendPacket); try &#123; // 阻塞等待，直到收到一个数据报文或等待超时，超时会抛出异常 socket.receive(receivePacket); // 校验服务端返回报文的地址和端口号 if (!receivePacket.getAddress().equals(serverAddress)) &#123; throw new IOException(\"Received packet from an unknown source\"); &#125; receivedResponse = true; &#125; catch (InterruptedIOException e) &#123; tries += 1; System.out.println(\"Timed out, \" + (MAXTRIES - tries) + \" more tries...\"); &#125; &#125; while (!receivedResponse &amp;&amp; (tries &lt; MAXTRIES)); if (receivedResponse) &#123; System.out.println(\"Received: \" + new String(receivePacket.getData())); &#125; else &#123; System.out.println(\"No response -- giving up.\"); &#125; // 关闭 Socket socket.close(); &#125;&#125; 说明： 由于 UDP 提供的是尽最大可能的交付，所以在发送 Echo Request 请求时，无法保证一定可以送达目标地址和端口，因此考虑设置重传次数，若在超过最大等待时间后仍未收到回复，则重发当前请求，若重发次数超过最大重试次数，则可直接返回未发送成功。 UDP Socket 与 TCP Socket 区别 UDP 保存了消息的边界信息，而 TCP 则没有。在 TCP 中需通过多次 read() 来接收一次 write() 的信息，而 UDP 中对于单次 send() 的数据，最多只需一次 receive() 调用。 TCP 存在传输缓冲区，UDP 则无需对数据进行缓存。由于 TCP 存在错误重传机制，因此需保留数据的缓存，以便于重传操作，当调用 write() 方法并返回后，数据被复制到传输缓冲区中，数据有可能处于发送过程中或还没有发生传送。而 UDP 则不存在该机制，因此无需缓存数据，当调用 send() 方法返回后，消息处于发送过程中。 UDP 会丢掉超过最大长度限制的数据，而 TCP 不会。在 TCP 中，一旦建立连接后，对于所有数据都可以看做一个连续的字节序列。而在 UDP 中接收到的消息则可能来自于不同的源地址和端口，因此会将接收到的数据放在消息队列中，按照顺序来响应，超过最大长度的消息直接截断。Datagrampacket 所能传输的最大数据量为 65507 字节，也就是一个 UDP 报文能承载的最大数据。 参考https://blog.csdn.net/wyzidu/article/details/83826656 https://www.jianshu.com/p/cde27461c226","path":"2019/10/12/Java-Socket/","date":"10-12","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java 反射","text":"Java 反射JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能称为java语言的反射机制。 反射机制的相关类 类名 用途 Class类 代表类的实体，在运行的Java应用程序中表示类和接口 Field类 代表类的成员变量（成员变量也称为类的属性） Method类 代表类的方法 Constructor类 代表类的构造方法 Class类Class代表类的实体，在运行的Java应用程序中表示类和接口。在这个类中提供了很多有用的方法，这里对他们简单的分类介绍。 获取Class的三种方式 String name = \"ZhangSan\";Class c1 = name.getClass();System.out.println(c1.getName()); String name = \"java.lang.String\";Class c1 = null;try &#123; c1 = Class.forName(name); System.out.println(c1.getName());&#125; catch (ClassNotFoundException e) &#123;&#125; Class c1 = String.class; Type属性基本类型都有type属性，可以得到这个基本类型的类型，比如： Class c1 = Boolean.TYPE;Class c2 = Byte.TYPE;Class c3 = Float.TYPE;Class c4 = Double.TYPE; 获取类的构造函数// Test类public class Test &#123; private int age; private String name; private int testint; public Test(int age) &#123; this.age = age; &#125; public Test(int age, String name) &#123; this.age = age; this.name = name; System.out.println(\"hello \" + name + \"i am \" + age); &#125; private Test(String name) &#123; this.name = name; System.out.println(\"My Name is \" + name); &#125; public Test() &#123; &#125; private void welcome(String tips)&#123; System.out.println(tips); &#125; 获取所有构造方法 Test test = new Test();Class c4 = test.getClass();Constructor[] constructors = c4.getDeclaredConstructors(); getDeclaredConstructors可以返回类的所有构造方法 getModifiers可以得到构造方法的类型 getParameterTypes可以得到构造方法的所有参数 for (int i = 0; i &lt; constructors.length; i++) &#123; System.out.print(Modifier.toString(constructors[i].getModifiers()) + \"参数：\"); Class[] parametertypes = constructors[i].getParameterTypes(); for (int j = 0; j &lt; parametertypes.length; j++) &#123; System.out.print(parametertypes[j].getName() + \" \"); &#125; System.out.println(\"\");&#125; 结果： public参数：public参数： java.lang.Stringpublic参数： int java.lang.Stringpublic参数： int 获取类中特定的构造方法 getDeclaredConstructor传参获取特定参数类型的构造方法 try&#123; Constructor constructor = c4.getDeclaredConstructor(); System.out.print(Modifier.toString(constructors.getModifiers()));&#125;catch(NoSuchMethodException e)&#123; e.printStackTrace();&#125; 结果： public Class[] p = &#123;int.class,String.class&#125;;try &#123; constructors = c4.getDeclaredConstructor(p); System.out.print(Modifier.toString(constructors.getModifiers()) + \"参数:\"); Class[] parametertypes = constructors.getParameterTypes(); for (int j = 0; j &lt; parametertypes.length; j++) &#123; System.out.print(parametertypes[j].getName() + \" \"); &#125;&#125; catch (NoSuchMethodException e) &#123; e.printStackTrace();&#125; 结果： public参数： int java.lang.String 调用构造方法 Class[] p = &#123;int.class,String.class&#125;;constructors = c4.getDeclaredConstructor(p);constructors.newInstance(23,\"LeeSkyWave\"); 结果： hello LeeskyWave i am 23 调用私有构造方法 Class[] p = &#123;String.class&#125;;constructors = c4.getDeclaredConstructor(p);constructors.setAccessible(true);constructors.newInstance(\"LeeSkyWave\"); 结果： My Name is LeeSkyWave 获取类的成员方法调用类的私有方法 Class[] p4 = &#123;String.class&#125;;Method method = c4.getDeclaredMethod(\"welcome\",p4);method.setAccessible(true);Object arg1s[] = &#123;\"welcone to my room\"&#125;;// invoke需要两个参数一个是类的实例，一个是方法参数。method.invoke(test,arg1s); 结果： welcone to my room 获取类的成员变量获取私有成员变量并修改值 Field field = c4.getDeclaredField(\"name\");field.setAccessible(true);// o是通过反射构造方法获取的实例field.set(o,\"LeeSkyWave\"); 参考https://blog.csdn.net/sinat_38259539/article/details/71799078 https://blog.csdn.net/huangliniqng/article/details/88554510","path":"2019/10/10/Java-反射/","date":"10-10","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java 注解","text":"Java 注解Java的注解本身对代码逻辑没有任何影响。 根据@Retention的配置： SOURCE类型的注解在编译期就被丢掉了； CLASS类型的注解仅保存在class文件中，它们不会被加载进JVM； RUNTIME类型的注解会被加载进JVM，并且在运行期可以被程序读取。 如何使用注解完全由工具决定。SOURCE类型的注解主要由编译器使用，因此我们一般只使用，不编写。CLASS类型的注解主要由底层工具库使用，涉及到class的加载，一般我们很少用到。只有RUNTIME类型的注解不但要使用，还经常需要编写。 因为注解定义后也是一种class，所有的注解都继承自java.lang.annotation.Annotation，因此，读取注解，需要使用反射API。 判断注解是否存在判断某个注解是否存在于Class、Field、Method或Constructor： Class.isAnnotationPresent(Class) Field.isAnnotationPresent(Class) Method.isAnnotationPresent(Class) Constructor.isAnnotationPresent(Class) // 判断@Report是否存在于Person类:Person.class.isAnnotationPresent(Report.class); 使用反射读取Annotation使用反射API读取Annotation： Class.getAnnotation(Class) Field.getAnnotation(Class) Method.getAnnotation(Class) Constructor.getAnnotation(Class) // 获取Person定义的@Report注解:Report report = Person.class.getAnnotation(Report.class);int type = report.type();String level = report.level(); 使用反射API读取Annotation有两种方法。方法一是先判断Annotation是否存在，如果存在，就直接读取： Class cls = Person.class;if (cls.isAnnotationPresent(Report.class)) &#123; Report report = cls.getAnnotation(Report.class); ...&#125; 第二种方法是直接读取Annotation，如果Annotation不存在，将返回null： Class cls = Person.class;Report report = cls.getAnnotation(Report.class);if (report != null) &#123; ...&#125; 读取方法、字段和构造方法的Annotation和Class类似。但要读取方法参数的Annotation就比较麻烦一点，因为方法参数本身可以看成一个数组，而每个参数又可以定义多个注解，所以，一次获取方法参数的所有注解就必须用一个二维数组来表示。例如，对于以下方法定义的注解： public void hello(@NotNull @Range(max=5) String name, @NotNull String prefix) &#123;&#125; 要读取方法参数的注解，我们先用反射获取Method实例，然后读取方法参数的所有注解： // 获取Method实例:Method m = ...// 获取所有参数的Annotation:Annotation[][] annos = m.getParameterAnnotations();// 第一个参数（索引为0）的所有Annotation:Annotation[] annosOfName = annos[0];for (Annotation anno : annosOfName) &#123; if (anno instanceof Range) &#123; // @Range注解 Range r = (Range) anno; &#125; if (anno instanceof NotNull) &#123; // @NotNull注解 NotNull n = (NotNull) anno; &#125;&#125; 使用注解注解如何使用，完全由程序自己决定。例如，JUnit是一个测试框架，它会自动运行所有标记为@Test的方法。 我们来看一个@Range注解，我们希望用它来定义一个String字段的规则：字段长度满足@Range的参数定义： @Retention(RetentionPolicy.RUNTIME)@Target(ElementType.FIELD)public @interface Range &#123; int min() default 0; int max() default 255;&#125; 在某个JavaBean中，我们可以使用该注解： public class Person &#123; @Range(min=1, max=20) public String name; @Range(max=10) public String city;&#125; 但是，定义了注解，本身对程序逻辑没有任何影响。我们必须自己编写代码来使用注解。这里，我们编写一个Person实例的检查方法，它可以检查Person实例的String字段长度是否满足@Range的定义： void check(Person person) throws IllegalArgumentException, ReflectiveOperationException &#123; // 遍历所有Field: for (Field field : person.getClass().getFields()) &#123; // 获取Field定义的@Range: Range range = field.getAnnotation(Range.class); // 如果@Range存在: if (range != null) &#123; // 获取Field的值: Object value = field.get(person); // 如果值是String: if (value instanceof String) &#123; String s = (String) value; // 判断值是否满足@Range的min/max: if (s.length() &lt; range.min() || s.length() &gt; range.max()) &#123; throw new IllegalArgumentException(\"Invalid field: \" + field.getName()); &#125; &#125; &#125; &#125;&#125; 这样一来，我们通过@Range注解，配合check()方法，就可以完成Person实例的检查。注意检查逻辑完全是我们自己编写的，JVM不会自动给注解添加任何额外的逻辑。 参考https://www.liaoxuefeng.com/wiki/1252599548343744/1265102026065728","path":"2019/10/10/Java-注解/","date":"10-10","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java 集合类","text":"Java 集合类集合的长度是可变的，用来存放对象的引用。常见的集合类有List集合、Set集合、Map集合。 List接口import java.util.Iterator;import java.util.LinkedList;import java.util.List;public class CollectionDemoList &#123; public static void main(String[] args) &#123; String aString = \"A\", bString = \"B\", cString = \"C\", dString = \"D\", eString = \"E\"; List&lt;String&gt; list = new LinkedList&lt;&gt;(); // 创建list集合对象 list.add(aString); // 向集合中添加元素 list.add(bString); list.add(eString); // 输出语句，用迭代器 Iterator&lt;String&gt; iter = list.iterator(); // 创建集合迭代器 while(iter.hasNext()) &#123; // 遍历集合中的元素 System.out.print(iter.next() + \" \"); &#125; System.out.println(); // 换行 list.set(1, cString); // 将索引位置1的对象修改为对象bString Iterator&lt;String&gt; it = list.iterator(); while(it.hasNext()) &#123; System.out.print(it.next() + \" \"); &#125; &#125;&#125; 结果： A B EA C E Set接口import java.util.ArrayList;import java.util.HashSet;import java.util.Iterator;import java.util.List;import java.util.Set;public class CollectionDemoSet &#123; public static void main(String[] args) &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); // 创建List集合对象 list.add(\"dog\"); list.add(\"cat\"); list.add(\"fish\"); list.add(\"cat\"); //重复值 Set&lt;String&gt; set = new HashSet&lt;&gt;(); // 创建List对象集合 set.addAll(list); // 将List集合对象添加到Set集合中 Iterator&lt;String&gt; it = set.iterator(); while(it.hasNext()) &#123; System.out.print(it.next() + \" \"); &#125; &#125;&#125; 结果： cat fish dog 于Set集合中的对象是无序的，遍历Set集合的结果与插入Set集合的顺序并不相同。 Map接口Map接口提供了将键映射到值的对象，一个映射不能包含重复的键，每个键最多只能映射一个值。 import java.util.HashMap;import java.util.Map;public class MapDemo &#123; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); // 创建Map集合 map.put(\"1\", \"dog\"); map.put(\"2\", \"cat\"); map.put(\"3\", \"fish\"); for(int i=1; i&lt;=3; i++) &#123; System.out.println(\"第\" + i + \"个元素是：\" + map.get(\"\" + i + \"\")); &#125; &#125;&#125; 结果： 第1个元素是：dog第2个元素是：cat第3个元素是：fish 创建Map集合时，Map接口的“&lt;&gt;”中含有两个类型，分别对应其key与value。 List接口的实现类List接口的实现类常用的有ArrayList和LinkedList。 ArrayList类实现了可变的数组，可以根据索引位置对集合进行快速的随机访问。LinkedList类采用链表结构保存对象，便于向集合中插入和删除对象。 List list1 = new ArrayList（）;List list2 = new LinkedList(); //ArrayList实现类ArrayList&lt;String&gt; list1=new ArrayList&lt;String&gt;();list1.add(\"第一条数据\"); list1.add(\"第二条数据\");list1.add(\"第三条数据\");list1.add(\"第四条数据\");list1.add(\"第五条数据\");list1.add(\"第三条数据\");list1.add(2,\"第六条数据\"); //在第三条数据后添加“第六条数据”System.out.println(list1.size()); //返回列表长度System.out.println(list1.get(2)); //返回“第六条数据”//list1.clear(); //清除所有数据System.out.println(list1.isEmpty()); //返回falseSystem.out.println(list1.contains(\"第一条数据\")); //返回trueSystem.out.println(list1.indexOf(\"第三条数据\")); //返回3System.out.println(list1.lastIndexOf(\"第三条数据\")); //返回6System.out.println(list1.remove(1)); //删除“第二条数据”System.out.println(list1.set(0, \"替换第一条数据\")); //替换第一条数据List list=list1.subList(2, 5); //截取第三到第五条数据，返回ListSystem.out.println(list1.toString()); //转成数组 //LinkedList实现类LinkedList&lt;News&gt; list2=new LinkedList&lt;News&gt;();list2.add(new News(1,\"xxxxxxx\",\"赵\"));list2.add(new News(2,\"sssssss\",\"钱\"));list2.add(new News(3,\"yyyyyyy\",\"孙\"));list2.add(new News(4,\"nnnnnnn\",\"李\"));list2.add(new News(5,\"rrrrrrr\",\"周\"));System.out.println(list2.contains(new News(3,\"yyyyyyy\",\"孙\"))); //返回为true；重写equals()方法；如果不重写equals()方法，返回为false；System.out.println(list2.remove(new News(1,\"xxxxxxx\",\"赵\"))); //返回为true /* * 使用for循环遍历 */ for (int i = 0; i &lt; list1.size(); i++) &#123; if(list1.get(i) instanceof String)&#123; //判断传入的数据属不属于String类型 String str=list1.get(i); System.out.println(str); &#125; &#125; /* * foreach遍历 */ for (News news : list2) &#123; System.out.println(news.getId()+\" \"+news.getTitle()+\"\\t\"+news.getAuthor()); &#125; /* * 迭代器遍历 */ Iterator&lt;String&gt; iter=list.iterator(); while(iter.hasNext())&#123; //判断list有没有下一条 String s=iter.next(); //字符串取到下一条 System.out.println(s); &#125; Set接口的实现类Set接口的实现类常用的有HashSet和TreeSet。 Set&lt;String&gt; set1 = new HashSet&lt;String&gt;();Set&lt;String&gt; set2 = new TreeSet&lt;String&gt;(); //HashSet无序的Set&lt;String&gt; set1 = new HashSet&lt;String&gt;();set1.add(\"set1\");set1.add(\"set2\");set1.add(\"set3\");set1.add(\"set4\");set1.add(\"set5\");//foreach遍历for (String set : set1) &#123; System.out.println(set);&#125;// TreeSet从小到大输出。存入元素，默认升序排序；存入实体对象，需要传入比较器Set&lt;Person&gt; set3 = new TreeSet&lt;Person&gt;();set3.add(new Person(11,\"a\"));set3.add(new Person(22,\"b\"));set3.add(new Person(33,\"c\"));set3.add(new Person(44,\"d\"));// 迭代器遍历Iterator&lt;Person&gt; iter = set3.iterator();while(iter.hasNext())&#123; Person p=iter.next(); System.out.println(p.getId()+\"\\t\"+p.getName());&#125; Map接口的实现类Map接口的实现类常用的有HashMap和TreeMap，建议使用HashMap（效率相对较高）。 Map map1 = new HashMap();Map map2 = new TreeMap(); 常用方法 put（key，value）：向Map的最后追加一个键值对； get（key）：通过键，取到一个值； clear（）：清除Map中的所有数据； containsKey（key）：检测是否包含指定的键； containsValue（obj）：检测是否包含指定的值； replace（）：替换指定键的值； 遍历Map keySet（）：返回Set，先取键，再取值； values（）：返回Collection，直接取值； 取代一个entry键值对，返回Set","path":"2019/10/08/Java-集合类/","date":"10-08","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java 枚举类型与泛型","text":"Java 枚举类型与泛型枚举类型可以取代以往的常量的定义方式，即将常量封装在类或者接口中，此外，它还提供了安全检查功能。枚举类型本质上还是以类的形式存在。 泛型的出现不仅仅可以让程序员少写某些代码，其主要作用还是解决泛型安全的问题，它提供编译时候的安全检查，不会因为将某个对象置于某个容器中而失去其类型。 枚举类型以往的方式： public class DayDemo &#123; public static final int MONDAY =1; public static final int TUESDAY=2; public static final int WEDNESDAY=3; public static final int THURSDAY=4; public static final int FRIDAY=5; public static final int SATURDAY=6; public static final int SUNDAY=7;&#125; 枚举出现后： //枚举类型，使用关键字enumenum Day &#123; MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY&#125; 枚举类型Day中分别定义了从周一到周日的值，这里要注意，值一般是大写的字母，多个值之间以逗号分隔。 使用： public class EnumDemo &#123; public static void main(String[] args)&#123; //直接引用 Day day =Day.MONDAY; &#125;&#125;//定义枚举类型enum Day &#123; MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY&#125; 就像上述代码那样，直接引用枚举的值即可，这便是枚举类型的最简单模型。 枚举类型的用法用法1：常量 public enum Color &#123; RED, GREEN, BLANK, YELLOW &#125; 用法2：switch enum Signal &#123; GREEN, YELLOW, RED &#125; public class TrafficLight &#123; Signal color = Signal.RED; public void change() &#123; switch (color) &#123; case RED: color = Signal.GREEN; break; case YELLOW: color = Signal.RED; break; case GREEN: color = Signal.YELLOW; break; &#125; &#125; &#125; 用法3：向枚举中添加新方法 public enum Color &#123; RED(\"红色\", 1), GREEN(\"绿色\", 2), BLANK(\"白色\", 3), YELLO(\"黄色\", 4); // 成员变量 private String name; private int index; // 构造方法 private Color(String name, int index) &#123; this.name = name; this.index = index; &#125; // 普通方法 public static String getName(int index) &#123; for (Color c : Color.values()) &#123; if (c.getIndex() == index) &#123; return c.name; &#125; &#125; return null; &#125; // get set 方法 public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getIndex() &#123; return index; &#125; public void setIndex(int index) &#123; this.index = index; &#125; &#125; 用法4：覆盖枚举的方法 public enum Color &#123; RED(\"红色\", 1), GREEN(\"绿色\", 2), BLANK(\"白色\", 3), YELLO(\"黄色\", 4); // 成员变量 private String name; private int index; // 构造方法 private Color(String name, int index) &#123; this.name = name; this.index = index; &#125; //覆盖方法 @Override public String toString() &#123; return this.index+\"_\"+this.name; &#125; &#125; 用法5：实现接口 所有的枚举都继承自java.lang.Enum类。由于Java 不支持多继承，所以枚举对象不能再继承其他类。 用法6：使用接口组织枚举 用法7：关于集合的使用 泛型泛型，即“参数化类型”。泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。 泛型类public class Generic&lt;T&gt; &#123; private T t; public void set(T t) &#123; this.t = t; &#125; public T get() &#123; return t; &#125;&#125; 泛型接口public interface Generator&lt;T&gt; &#123; public T next();&#125; 当实现泛型接口的类，未给泛型传入实参时： class FruitGenerator&lt;T&gt; implements Generator&lt;T&gt;&#123; @Override public T next() &#123; return null; &#125;&#125; 未传入泛型实参时，与泛型类的定义相同，在声明类的时候，需将泛型的声明也一起加到类中。 如果不声明泛型，如：class FruitGenerator implements Generator，编译器会报错：”Unknown class”。 当实现泛型接口的类，给泛型传入了实参时 public class FruitGenerator implements Generator&lt;String&gt; &#123; private String[] fruits = new String[]&#123;\"Apple\", \"Banana\", \"Pear\"&#125;; @Override public String next() &#123; Random rand = new Random(); return fruits[rand.nextInt(3)]; &#125;&#125; 如果类已经将泛型类型传入实参类型，则所有使用泛型的地方都要替换成传入的实参类型。即：Generator，public T next();中的的T都要替换成传入的String类型。 泛型方法public class Util &#123; public static &lt;K, V&gt; boolean compare(Generic&lt;K, V&gt; g1, Generic&lt;K, V&gt; g2) &#123; return g1.getKey().equals(g2.getKey()) &amp;&amp; g1.getValue().equals(g2.getValue()); &#125;&#125;public class Generic&lt;K, V&gt; &#123; private K key; private V value; public Generic(K key, V value) &#123; this.key = key; this.value = value; &#125; public void setKey(K key) &#123; this.key = key; &#125; public void setValue(V value) &#123; this.value = value; &#125; public K getKey() &#123; return key; &#125; public V getValue() &#123; return value; &#125;&#125; Util.compare()就是一个泛型方法，于是我们可以像下面这样调用泛型： Generic&lt;Integer, String&gt; g1 = new Generic&lt;&gt;(1, \"apple\");Generic&lt;Integer, String&gt; g2 = new Generic&lt;&gt;(2, \"pear\");boolean same = Util.&lt;Integer, String&gt;compare(g1, g2); 可缩写成： Generic&lt;Integer, String&gt; p1 = new Generic&lt;&gt;(1, \"apple\");Generic&lt;Integer, String&gt; p2 = new Generic&lt;&gt;(2, \"pear\");boolean same = Util.compare(p1, p2); 通配符Generic不能被看作为Generic的子类。由此可以看出:同一种泛型可以对应多个版本（因为参数类型是不确定的），而不同版本的泛型类实例之间是不兼容的。 此时可以： public void showKeyValue(Generic&lt;?&gt; obj)&#123; System.out.println(\"泛型测试,value is \" + obj.get());&#125; 此时，showKeyValue方法可以传入任意类型的Generic参数，这是一个无界的通配符。 泛型上下边界在Java泛型定义时:用&lt;T&gt;等大写字母标识泛型类型，用于表示未知类型。用&lt;T extends ClassA &amp; InterfaceB …&gt;等标识有界泛型，用于表示有边界的类型。在Java泛型实例化时:用&lt;?&gt;标识通配符，用于表示实例化时的类型。用&lt;? extends 父类型&gt;标识上边界通配符，用于表示实例化时可以确定父类型的类型。用&lt;? super 子类型&gt;标识下边界通配符，用于表示实例化时可以确定子类型的类型。对上面的Generic类增加一个新方法： public void showKeyValue1(Generic&lt;? extends Number&gt; obj)&#123; System.out.println(\"泛型测试,value is \" + obj.get());&#125; Generic&lt;String&gt; generic1 = new Generic&lt;String&gt;(\"11111\");Generic&lt;Integer&gt; generic2 = new Generic&lt;Integer&gt;(2222);Generic&lt;Float&gt; generic3 = new Generic&lt;Float&gt;(2.4f);Generic&lt;Double&gt; generic4 = new Generic&lt;Double&gt;(2.56);//这一行代码编译器会提示错误，因为String类型并不是Number类型的子类showKeyValue1(generic1);showKeyValue1(generic2);showKeyValue1(generic3);showKeyValue1(generic4); 泛型的限制1. Java泛型不能使用基本类型 2.Java泛型不允许进行直接实例化 3.Java泛型不允许进行静态化 4.Java泛型不允许直接进行类型转换（通配符可以） 5.Java泛型不允许直接使用instanceof运算符进行运行时类型检查（通配符可以） 6.Java泛型不允许创建确切类型的泛型数组（通配符可以） 7.Java泛型不允许作为参数进行重载 参考https://www.jianshu.com/p/31b44188b973 https://www.cnblogs.com/alter888/p/9163612.html https://www.cnblogs.com/jin-zhe/p/8259422.html","path":"2019/10/08/Java-枚举类型与泛型/","date":"10-08","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java I/O流(输入/输出流)","text":"Java I/O流(输入/输出流)IO：Java对数据的操作是通过流的方式，IO流用来处理设备之间的数据传输，上传文件和下载文件，Java用于操作流的对象都在IO包中。 InputStream 和 OutputStream 两个类都是抽象类，抽象类不能进行实例化。在实际应用中我们要用到一系列基本数据流类，都是他们的子类，在实现其父类方法的同时又都定义了其特有的功能。 字节流基类InputStream： // 从输入流中读取数据的下一个字节abstract int read() // 从输入流中读取一定数量的字节，并将其存储在缓冲区数组 b中int read(byte[] b) // 将输入流中最多 len 个数据字节读入 byte 数组int read(byte[] b, int off, int len) // 跳过和丢弃此输入流中数据的 n个字节long skip(long n) // 关闭此输入流并释放与该流关联的所有系统资源void close() OutputStream： // 将指定的字节写入此输出流abstract void write(int b) // 将 b.length 个字节从指定的 byte 数组写入此输出流void write(byte[] b) // 将指定 byte 数组中从偏移量 off 开始的 len 个字节写入此输出流void write(byte[] b, int off, int len) // 关闭此输出流并释放与此流有关的所有系统资源void close()// 刷新此输出流并强制写出所有缓冲的输出字节void flush() 文件字节流FileInputStream： // 通过打开一个到实际文件的连接来创建一个FileInputStream，该文件通过文件系统中的File对象file指定FileInputStream(File file)// 通过打开一个到实际文件的连接来创建一个FileInputStream，该文件通过文件系统中的路径name指定FileInputStream(String name) // 读取f盘下该文件f://hell/test.txt//构造方法1InputStream inputStream = new FileInputStream(new File(\"f://hello//test.txt\"));int i = 0;//一次读取一个字节while ((i = inputStream.read()) != -1) &#123; // System.out.print(i + \" \");// 65 66 67 68 //为什么会输出65 66 67 68？因为字符在底层存储的时候就是存储的数值。即字符对应的ASCII码。 System.out.print((char) i + \" \");// A B C D&#125;//关闭IO流inputStream.close(); // 读取f盘下该文件f://hell/test.txt//构造方法2InputStream inputStream2 = new FileInputStream(\"f://hello/test.txt\");// 字节数组byte[] b = new byte[2];int i2 = 0;// 一次读取一个字节数组while ((i2 = inputStream2.read(b)) != -1) &#123; System.out.print(new String(b, 0, i2) + \" \");// AB CD&#125;//关闭IO流inputStream2.close(); 注： 一次读取一个字节数组，提高了操作效率,IO流使用完毕一定要关闭。 FileOutputStream： // 创建一个向指定File对象表示的文件中写入数据的文件输出流FileOutputStream(File file) // 创建一个向指定File对象表示的文件中写入数据的文件输出流FileOutputStream(File file, boolean append) // 创建一个向具有指定名称的文件中写入数据的输出文件流FileOutputStream(String name) // 创建一个向具有指定name的文件中写入数据的输出文件流FileOutputStream(String name, boolean append) OutputStream outputStream = new FileOutputStream(new File(\"test.txt\"));// 写出数据outputStream.write(\"ABCD\".getBytes());// 关闭IO流outputStream.close();// 内容追加写入OutputStream outputStream2 = new FileOutputStream(\"test.txt\", true);// 输出换行符outputStream2.write(\"\\r\\n\".getBytes());// 输出追加内容outputStream2.write(\"hello\".getBytes());// 关闭IO流outputStream2.close(); 注1：输出的目的地文件不存在，则会自动创建，不指定盘符的话，默认创建在项目目录下; 注2：输出换行符时一定要写\\r\\n不能只写\\n,因为不同文本编辑器对换行符的识别存在差异性。 字节缓冲流BufferedInputStream // 创建一个 BufferedInputStream并保存其参数，即输入流in，以便将来使用。BufferedInputStream(InputStream in) // 创建具有指定缓冲区大小的 BufferedInputStream并保存其参数，即输入流in以便将来使用BufferedInputStream(InputStream in, int size) InputStream in = new FileInputStream(\"test.txt\");// 字节缓存流BufferedInputStream bis = new BufferedInputStream(in);byte[] bs = new byte[20];int len = 0;while ((len = bis.read(bs)) != -1) &#123; System.out.print(new String(bs, 0, len)); // ABCD // hello&#125;// 关闭流bis.close(); BufferedOutputStream // 创建一个新的缓冲输出流，以将数据写入指定的底层输出流BufferedOutputStream(OutputStream out) // 创建一个新的缓冲输出流，以将具有指定缓冲区大小的数据写入指定的底层输出流BufferedOutputStream(OutputStream out, int size) BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\"test.txt\", true));// 输出换行符bos.write(\"\\r\\n\".getBytes());// 输出内容bos.write(\"Hello Android\".getBytes());// 刷新此缓冲的输出流bos.flush();// 关闭流bos.close(); 字符流基类Reader： // 读取单个字符int read() // 将字符读入数组int read(char[] cbuf) // 将字符读入数组的某一部分abstract int read(char[] cbuf, int off, int len) // 跳过字符long skip(long n)// 关闭该流并释放与之关联的所有资源abstract void close() Writer： // 写入字符数组void write(char[] cbuf) // 写入字符数组的某一部分abstract void write(char[] cbuf, int off, int len) // 写入单个字符void write(int c) // 写入字符串void write(String str) // 写入字符串的某一部分void write(String str, int off, int len) // 将指定字符添加到此 writerWriter append(char c) // 将指定字符序列添加到此 writerWriter append(CharSequence csq) // 将指定字符序列的子序列添加到此 writer.AppendableWriter append(CharSequence csq, int start, int end) // 关闭此流，但要先刷新它abstract void close() // 刷新该流的缓冲abstract void flush() 字符转换流InputStreamReader // 创建一个使用默认字符集的 InputStreamReaderInputStreamReader(InputStream in) // 创建使用给定字符集的 InputStreamReaderInputStreamReader(InputStream in, Charset cs) // 创建使用给定字符集解码器的 InputStreamReaderInputStreamReader(InputStream in, CharsetDecoder dec) // 创建使用指定字符集的 InputStreamReaderInputStreamReader(InputStream in, String charsetName) 特有方法： String getEncoding() //使用默认编码 InputStreamReader reader = new InputStreamReader(new FileInputStream(\"test.txt\"));int len;while ((len = reader.read()) != -1) &#123; System.out.print((char) len);//爱生活，爱Android&#125;reader.close();//指定编码 InputStreamReader reader = new InputStreamReader(new FileInputStream(\"test.txt\"),\"utf-8\");int len;while ((len = reader.read()) != -1) &#123; System.out.print((char) len);//????????Android&#125;reader.close(); OutputStreamWriter // 创建使用默认字符编码的 OutputStreamWriterOutputStreamWriter(OutputStream out) // 创建使用给定字符集的 OutputStreamWriterOutputStreamWriter(OutputStream out, Charset cs) // 创建使用给定字符集编码器的 OutputStreamWriterOutputStreamWriter(OutputStream out, CharsetEncoder enc) // 创建使用指定字符集的 OutputStreamWriterOutputStreamWriter(OutputStream out, String charsetName) 特有方法： String getEncoding() 字符缓冲流（高效流）BufferedReader // 创建一个使用默认大小输入缓冲区的缓冲字符输入流BufferedReader(Reader in) // 创建一个使用指定大小输入缓冲区的缓冲字符输入流BufferedReader(Reader in, int sz) 特有方法： String readLine() //生成字符缓冲流对象BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(\"test.txt\")));String str;//一次性读取一行while ((str = reader.readLine()) != null) &#123; System.out.println(str);// 爱生活，爱Android&#125;//关闭流reader.close(); BufferedWriter // 创建一个使用默认大小输出缓冲区的缓冲字符输出流BufferedWriter(Writer out) // 创建一个使用给定大小输出缓冲区的新缓冲字符输出流BufferedWriter(Writer out, int sz) 特有方法： void newLine() 文件字符流FileReader FileReader：InputStreamReader类的直接子类，用来读取字符文件的便捷类，使用默认字符编码。 FileWriter FileWriter：OutputStreamWriter类的直接子类，用来写入字符文件的便捷类，使用默认字符编码。 高效流效率对比读取方式一 FileInputStream inputStream = new FileInputStream(\"f://滑板//HEEL_FLIP.mp4\");FileOutputStream outputStream = new FileOutputStream(\"HEEL_FLIP.mp4\");int len;// 开始时间long begin = System.currentTimeMillis();// 一次读取一个字节while ((len = inputStream.read()) != -1) &#123; outputStream.write(len);&#125;// 用时毫秒System.out.println(System.currentTimeMillis() - begin);// 213195//关闭流释放资源inputStream.close();outputStream.close(); 读取方式二 FileInputStream inputStream = new FileInputStream(\"f://滑板//HEEL_FLIP.mp4\");FileOutputStream outputStream = new FileOutputStream(\"HEEL_FLIP.mp4\");int len;byte[] bs = new byte[1024];// 开始时间long begin = System.currentTimeMillis();// 一次读取一个字节数组while ((len = inputStream.read(bs)) != -1) &#123; outputStream.write(bs, 0, len);&#125;// 用时毫秒System.out.println(System.currentTimeMillis() - begin);// 281inputStream.close();outputStream.close(); 读取方式三 FileInputStream inputStream = new FileInputStream(\"f://滑板//HEEL_FLIP.mp4\");BufferedInputStream bis = new BufferedInputStream(inputStream);FileOutputStream outputStream = new FileOutputStream(\"HEEL_FLIP.mp4\");BufferedOutputStream bos = new BufferedOutputStream(outputStream);int len;byte[] bs = new byte[1024];// 开始时间long begin = System.currentTimeMillis();while ((len = bis.read(bs)) != -1) &#123; bos.write(bs, 0, len);&#125;// 用时毫秒System.out.println(System.currentTimeMillis() - begin);// 78bis.close();bos.close(); 注：由此可以看出高效缓冲流读写速度是非常快的，建议使用","path":"2019/10/08/Java-IO(输入输出)/","date":"10-08","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java 异常处理","text":"Java 异常处理所有的异常类是从 java.lang.Exception 类继承的子类。 Exception 类是 Throwable 类的子类。除了Exception类外，Throwable还有一个子类Error 。 Java 程序通常不捕获错误。错误一般发生在严重故障时，它们在Java程序处理的范畴之外。 Error 用来指示运行时环境发生的错误。 例如，JVM 内存溢出。一般地，程序不会从错误中恢复。 异常类有两个主要的子类：IOException 类和 RuntimeException 类。 捕获异常使用 try 和 catch 关键字可以捕获异常。try/catch 代码块放在异常可能发生的地方。 try/catch代码块中的代码称为保护代码，使用 try/catch 的语法如下： import java.io.*;public class ExcepTest&#123; public static void main(String args[])&#123; try&#123; int a[] = new int[2]; System.out.println(\"Access element three :\" + a[3]); &#125;catch(ArrayIndexOutOfBoundsException e)&#123; System.out.println(\"Exception thrown :\" + e); &#125; System.out.println(\"Out of the block\"); &#125;&#125; 结果： Exception thrown :java.lang.ArrayIndexOutOfBoundsException: 3Out of the block throws/throw 关键字如果一个方法没有捕获到一个检查性异常，那么该方法必须使用 throws 关键字来声明。throws 关键字放在方法签名的尾部。 也可以使用 throw 关键字抛出一个异常，无论它是新实例化的还是刚捕获到的。 下面方法的声明抛出一个 RemoteException 异常： import java.io.*;public class className&#123; public void deposit(double amount) throws RemoteException &#123; // Method implementation throw new RemoteException(); &#125; //Remainder of class definition&#125; finally 关键字finally 关键字用来创建在 try 代码块后面执行的代码块。 无论是否发生异常，finally 代码块中的代码总会被执行。 在 finally 代码块中，可以运行清理类型等收尾善后性质的语句。 finally 代码块出现在 catch 代码块最后，语法如下： public class ExcepTest&#123; public static void main(String args[])&#123; int a[] = new int[2]; try&#123; System.out.println(\"Access element three :\" + a[3]); &#125;catch(ArrayIndexOutOfBoundsException e)&#123; System.out.println(\"Exception thrown :\" + e); &#125; finally&#123; a[0] = 6; System.out.println(\"First element value: \" +a[0]); System.out.println(\"The finally statement is executed\"); &#125; &#125;&#125; 结果： Exception thrown :java.lang.ArrayIndexOutOfBoundsException: 3First element value: 6The finally statement is executed 异常对象方法getMessage()函数：输出错误性质 toString()函数：给出异常的类型与性质 printStackTrace()函数：指出异常的类型、性质、栈层次及出现在程序中的位置 参考https://www.runoob.com/java/java-exceptions.html?tdsourcetag=s_pcqq_aiomsg","path":"2019/10/07/Java-异常处理/","date":"10-07","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java 面向对象总结","text":"Java 面向对象总结图解 特点面向对象具有封装、继承、多态、抽象等特性。 类和对象类是对一类事物的描述，是抽象的。 对象是一类事物的实例，是具体的。 类是对象的模板，对象是类的实体。 类是封装对象的属性和行为的载体。 成员变量和局部变量成员变量的默认值 数据类型 默认值 基本类型 整数(byte,short,int,long) 0 浮点数(float,double) 0.0 字符(char) ‘\\u0000’ 引用类型 布尔（boolean） false 数组，类，接口 null 权限修饰符Java 中主要有 private、protected、public 和 默认访问权限四种。 public 修饰符，具有最大的访问权限，可以访问任何一个在CLASSPATH 下的类、接口、异常等。 protected 修饰符，主要作用就是用来保护子类，子类可以访问这些成员变量和方法，其余类不可以。 default 修饰符，主要是本包的类可以访问。 private 修饰符，访问权限仅限于本类内部，在实际开发过程中，大多数的成员变量和方法都是使用 private 修饰的。 修饰符 本类 本包 外包子类 外包 public √ √ √ √ protected √ √ √ default √ √ private √ static 关键字随着类的加载而加载，优先于对象存在。（在静态方法中是没有this关键字的，this是随着对象的创建而存在）。 被类的所有对象共享，可以通过类名调用，也可以通过对象名调用，推荐使用类名调用。 静态变量存储于方法区的静态区，随着类的加载而加载，随着类的消失而消失。 this和superthis是对当前对象的引用，是运行期间的对象本身。 super是直接父类对象的引用，可以访问弗雷中被子类覆盖的属性和方法。 this和super调用构造函数this this关键字在构造函数中的应用：构造函数间调用，只能使用this进行互相调用，this函数不能用在一般函数间。 this语句(不是this关键字)只能定义在构造函数的第一行。 class Person&#123; private String name; private int age; Person(String name)&#123; this.name = name; &#125; Person(String name, int age)&#123; this(name); this.age = age; &#125; &#125; super 在对子类对象进行初始化时，父类构造函数也会运行，是因为子类的构造函数默认第一行有一条隐式的语句super()，super()会访问父类中空参数的构造函数，而且子类中所有的构造函数默认第一行都是super()。 当父类中没有空参数的构造时，系统会报错，所以必须手动指定子类引用父类的哪个非空的构造函数。 如果要访问父类中指定的构造函数，可以通过手动定义super语句的方式进行指定。 class Fu&#123; int num; Fu()&#123; this.num = 60; &#125; Fu(int x)&#123; this.num = x; &#125; &#125;class Zi extends Fu&#123; Zi()&#123; // super(); // super(30); &#125; Zi(int x)&#123; this(); &#125;&#125; 结论（子类的实例化过程）：子类中所有的构造函数，默认都会访问父类中的空参数的构造函数，因为子类每一个构造函数内的第一行都有一句隐式的super()，当父类中没有空参数的构造函数时，子类必须手动通过super语句或者this语句形式来指定要访问的父类中的构造函数。子类的构造函数第一行也可以手动指定this语句来访问本类中的构造函数，子类中至少要有一个构造函数会访问父类中的构造函数(没写也会有默认的)。 封装在面向对象程式设计方法中，封装（英语：Encapsulation）是指一种将抽象性函式接口的实现细节部份包装、隐藏起来的方法。 封装的步骤 使用 private 关键字来修饰成员变量。 对需要访问的成员变量，提供对应的一对 getXxx 方法 、 setXxx 方法。 标准代码——JavaBean JavaBean 是 Java语言编写类的一种标准规范。符合 JavaBean 的类，要求类必须是具体的和公共的，并且具有无参数的构造方法，提供用来操作成员变量的 set 和 get 方法。 public class Student &#123; //成员变量 private String name; private int age; //构造方法 public Student() &#123;&#125; public Student(String name,int age) &#123; this.name = name; this.age = age; &#125; //成员方法 publicvoid setName(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setAge(int age) &#123; this.age = age; &#125; public int getAge() &#123; return age; &#125;&#125; 继承继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例和方法，或子类从父类继承方法，使得子类具有父类相同的行为。 需要注意的是 Java 不支持多继承，但支持多重继承 继承的特性 1、子类拥有父类非 private 的属性、方法2、子类可以拥有自己的属性和方法，即子类可以对父类进行扩展3、子类可以用自己的方式实现父类的方法4、提高了类之间的耦合性 public class Animal&#123; private String name; private int id; public Animal(String name, int id)&#123; this.name = name; this.id = id; &#125; public void eat()&#123; System.out.println(name + \"正在吃\"); &#125; &#125;public class Pig extends Animal&#123; public Pig(String name, int id)&#123; super(name, id); &#125;&#125; 多态概念：同一操作作用于不同的对象，可以有不同的解释，产生不同的执行结果，这就是多态性。 简单的说：就是用基类的引用指向子类的对象。 java引用类型有两个： 编译时类型 编译时类型由声明该变量时使用的类型决定 运行时类型 运行时类型由实际赋给该变量的对象决定 静态多态和动态多态的区别： 多态分为编译时多态和运行时多态。其中编辑时多态是静态的，主要是指方法的重载，它是根据参数列表的不同来区分不同的函数，通过编辑之后会变成两个不同的函数，在运行时谈不上多态。而运行时多态是动态的，它是通过动态绑定来实现的，也就是我们所说的多态性。 多态存在的三个必要条件 要有继承 要有重写 父类引用指向子类对象 class Animal&#123; public int month = 2; public void eat()&#123; System.out.println(\"动物吃东西\"); &#125; &#125;class Dog extends Animal&#123; public int month = 3; public void eat() &#123; System.out.println(\"小狗吃肉\"); &#125; public void sleep() &#123; System.out.println(\"小狗睡午觉\"); &#125;&#125;class Cat extends Animal&#123; public int month = 4; public void eat() &#123; System.out.println(\"小猫吃鱼\"); &#125;&#125;public class Test &#123; public static void main(String[] args)&#123; Animal a = new Dog(); Animal b = new Cat(); a.eat(); System.out.println(a.month); //下面代码编译时会出错 //a.sleep(); b.eat(); System.out.println(b.month); &#125;&#125; 运行结果： 小狗吃肉2小猫吃鱼2 上例中main方法中注释了a.sleep()，由于a的编译时类型为Animal，而Animal类中没有sleep方法，因此无法在编译时调用sleep方法。 总的来说： 引用变量在编译阶段只能调用编译时类型所具有的方法，但运行时则执行他运行时类型所具有的方法。 对象类型的转换父子对象之间的转换分为了向上转型和向下转型,它们区别如下: 向上转型 : 通过子类对象(小范围)实例化父类对象(大范围),这种属于自动转换 向下转型 : 通过父类对象(大范围)实例化子类对象(小范围),这种属于强制转换 向上转型 class A &#123; public void print() &#123; System.out.println(\"A:print\"); &#125;&#125;class B extends A &#123; public void print() &#123; System.out.println(\"B:print\"); &#125;&#125;public class Test&#123; public static void main(String args[]) &#123; A a = new B(); //通过子类去实例化父类 a.print(); &#125;&#125; // 结果// B:print 可以看到打印的是class B的print,这是因为我们通过子类B去实例化的,所以父类A的print方法已经被子类B的print方法覆盖了.从而打印classB的print. 这样做的意义在于: 当我们需要多个同父的对象调用某个方法时,通过向上转换后,则可以确定参数的统一.方便程序设计(参考下面示例) class A &#123; public void print() &#123; System.out.println(\"A:print\"); &#125;&#125;class B extends A &#123; public void print() &#123; System.out.println(\"B:print\"); &#125;&#125;class C extends B &#123; public void print() &#123; System.out.println(\"C:print\"); &#125;&#125;public class Test&#123; public static void func(A a) &#123; a.print(); &#125; public static void main(String args[]) &#123; func(new B()); //等价于 A a =new B(); func(new C()); //等价于 C c =new C(); &#125;&#125; // 结果// B:print// C:print PS:向上转型时,父类只能调用父类方法或者子类覆写后的方法,而子类中的单独方法则是无法调用的. 向下转型 在java中,向下转型则是为了,通过父类强制转换为子类,从而来调用子类独有的方法(向下转型,在工程中很少用到). 为了保证向下转型的顺利完成,在java中提供了一个关键字:instanceof,通过instanceof可以判断某对象是否是某类的实例,如果是则返回true,否则为false,instanceof使用如下: A a = new B(); //向上转型 (B类是A的子类)a instanceof A; //返回true.a instanceof B; //返回truea instanceof C; //返回false 我们便来分析向下转型的意义. class A &#123; public void print() &#123; System.out.println(\"A:print\"); &#125;&#125;class B extends A &#123; public void print() &#123; System.out.println(\"B:print\"); &#125; public void funcB()&#123; System.out.println(\"funcB\"); &#125;&#125;class C extends A &#123; public void print() &#123; System.out.println(\"C:print\"); &#125; public void funcC()&#123; System.out.println(\"funcC\"); &#125;&#125;public class Test&#123; public static void func(A a) &#123; a.print(); if(a instanceof B) &#123; B b = (B)a; //向下转型,通过父类实例化子类 b.funcB(); //调用B类独有的方法 &#125; else if(a instanceof C) &#123; C c = (C)a; //向下转型,通过父类实例化子类 c.funcC(); //调用C类独有的方法 &#125; &#125; public static void main(String args[]) &#123; func(new A()); func(new B()); func(new C()); &#125;&#125; // 结果// A:print// B:peint// funcB// C:print// funcC 从上面打印可以看到,我们成功通过向下转型来调用B类和C类独有的方法. 方法重写和方法重载方法重写Override子类中出现了和父类中方法声明一模一样的方法。与返回值类型有关,返回值是一致(或者是子父类)的。 方法重载Overload本类中出现的方法名一样，参数列表不同的方法。与返回值类型无关。 Object类Java中的所有类最终都继承于Object。 equals()方法该方法用于比较两个对象，如果这两个对象引用指向的是同一个对象，那么返回true，否则返回false。 clone()方法默认的clone方法是浅拷贝模式。所谓浅拷贝，指的是对象内属性引用的对象只会拷贝引用地址，而不会将引用的对象重新分配内存。深拷贝则是会连引用的对象也重新创建。 toString()方法这个方法的使用频率非常高，用于返回一个可代表对象的字符串。 默认返回格式如下：对象的class名称 + @ + hashCode的十六进制字符串 finalize()方法这个方法用于在GC的时候再次被调用，如果我们实现了这个方法，对象可能在这个方法中再次复活，从而避免被GC回收。 通常情况下，我们不需要自己实现这个方法。 抽象类和接口抽象类没有方法主体的方法称为抽象方法。Java语法规定，包含抽象方法的类就是抽象类。 抽象类：abstract class 类名 {} 抽象方法：public abstract void eat(); 注意： ​ 1、抽象类不能实例化，由具体的子类实例化。其实这也是多态的一种，抽象类多态。 2、抽象类中，可以有构造方法，是供子类创建对象时，初始化父类成员使用的。 3、抽象类中，不一定包含抽象方法，但是有抽象方法的类必定是抽象类。 4、抽象类的子类，必须重写抽象父类中所有的抽象方法，否则，编译无法通过而报错。除非该子类也是抽象类。 public abstract class Animal &#123; public abstract void eat(); public abstract void sleep();&#125; public class Cat extends Animal&#123; @Override public void eat() &#123; System.out.println(\"我是猫，我吃的是猫粮呀\"); &#125; @Override public void sleep() &#123; System.out.println(\"我是猫，我比你们人类睡的时间短！\"); &#125; &#125; public class Tiger extends Animal &#123; @Override public void eat() &#123; // TODO Auto-generated method stub System.out.println(\"我是老虎，我要吃大鱼大肉！！！\"); &#125; @Override public void sleep() &#123; // TODO Auto-generated method stub System.out.println(\"我是老虎，每天必须睡够24个小时！！！！\"); &#125; &#125; 抽象类的用处： 虽然也要在Cat和Tiger中定义sleep和eat这两个方法，看似代码上没有太多简化。但是这背后却隐藏着一个规范问题：也就是“是不是”的问题。cat和person都“是”animal，所以就必须继承animal里面的方法。相当于提供了一个大的体制。 接口接口就是一个规范和抽象类比较相似。它只管做什么，不管怎么做。通俗的讲，借口就是某个事物对外提供的一些功能的声明，其定义和类比较相似，只不过是通过interface关键字来完成。 其中重要的几个知识点： 1.接口中的所有属性默认为：public static final **； 2.接口中的所有方法默认为：public abstract **； 3.接口不再像类一样用关键字 extends去“继承”，而是用 implements 去“实现”，也就是说类和接口的关系叫做实现，（例如：A类实现了B接口，那么成为A为B接口的实现类。而类与类之间的继承的话，叫做A类继承了B类，其中B类即为A类的父类）。实现接口与类的继承比较相似。 public interface Sleep &#123; public static int a = 1; public static int b = 2; public void ioSleep(int i);&#125; public interface Eat &#123; public abstract void ioEat();&#125; public interface Study &#123; public void ioStudy();&#125; public class Cat implements Sleep,Eat&#123; @Override public void ioSleep(int i) &#123; System.out.println(\"我是猫，我每天都不用睡觉！！！\"); &#125; @Override public void ioEat() &#123; System.out.println(\"我是猫，我吃猫粮！！！\"); &#125;&#125; public class Person implements Sleep,Eat,Study&#123; @Override public void ioStudy() &#123; System.out.println(\"我是人，我每天都要学习\"); &#125; @Override public void ioEat() &#123; System.out.println(\"我是人，我要吃大鱼大肉还要喝酒\"); &#125; @Override public void ioSleep(int i) &#123; System.out.println(\"我是人，我每天要睡24小时\"); &#125;&#125; 所以也就可以列出抽象类和接口的几点区别： ​ 1.抽象类描述的是“是不是”的问题，而接口描述的是“有没有”的问题； ​ 2.在Java中类的继承是“单继承”，可以“多对一”，但是不允许“一对多”。而一个类却可以同时实现多个接口； final关键字final修饰特点 修饰类，类不能被继承 修饰变量，变量就变成了常量，只能被赋值一次 修饰方法，方法不能被重写 内部类jdk 推出了「内部类」的概念，当然，内部类不仅仅弥补了 Java 不能多继承的一个不足，通过将一个类定义在另一个类的内部，也可以有效的隐藏该类的可见性，等等。 接口 + 内部类 = 多继承 在这之前，Java 的继承机制主要由接口和单根继承实现，通过实现多个接口里的方法，看似能够实现多继承，但是并不总是高效的，因为一旦我们继承了一个接口就必然要实现它内部定义的所有方法。 现在我们可以通过内部类多次继承某个具体类或者接口，省去一些不必要的实现动作。只能说 Java 的内部类完善了它的多继承机制，而不是主要实现，因为内部类终究是一种破坏封装性的设计，除非有很强的把控能力，否则还是越少用越好。 public class Father &#123; public String powerFul = \"市长\";&#125;public class Mother &#123; public String wealthy = \"一百万\";&#125; public class Son &#123; class Extends_Father extends Father&#123; &#125; class Extends_Mother extends Mother&#123; &#125; public void sayHello()&#123; String father = new Extends_Father().powerFul; String mother = new Extends_Mother().wealthy; System.out.println(\"my father is:\" + father + \"my mother has:\" + mother); &#125;&#125; 显然，我们的 Son 类是不可能同时继承 Father 和 Mother 的，但是我们却可以通过在其内部定义内部类继承了 Father 和 Mother，必要的情况下，我们还能够重写继承而来的各个类的属性或者方法。 这就是典型的一种通过内部类实现多继承的实现方式，但是同时你也会发现，单单从 Son 来外表看，你根本不知道它内部多继承了 Father 和 Mother，从而往往会给我们带来一些错觉。所以你看，内部类并不绝对是一个好东西，它破坏了封装性，用的不好反而会适得其反，让你的程序一团糟，所以谨慎！ 当然，并不是贬低它的价值，有些情况下它也能给你一种「四两拨千斤」的感觉，省去很多麻烦。下面我们看看几种不同的内部类类型。 从种类上说，内部类可以分为四类：普通内部类、静态内部类、匿名内部类、局部内部类。 成员内部类普通内部类对象依赖外部类对象而存在，即在创建一个普通内部类对象时首先需要创建其外部类对象 public class InnerClassTest &#123; public int outField1 = 1; protected int outField2 = 2; int outField3 = 3; private int outField4 = 4; public InnerClassTest() &#123; // 在外部类对象内部，直接通过 new InnerClass(); 创建内部类对象 InnerClassA innerObj = new InnerClassA(); System.out.println(\"创建 \" + this.getClass().getSimpleName() + \" 对象\"); System.out.println(\"其内部类的 field1 字段的值为: \" + innerObj.field1); System.out.println(\"其内部类的 field2 字段的值为: \" + innerObj.field2); System.out.println(\"其内部类的 field3 字段的值为: \" + innerObj.field3); System.out.println(\"其内部类的 field4 字段的值为: \" + innerObj.field4); &#125; public class InnerClassA &#123; public int field1 = 5; protected int field2 = 6; int field3 = 7; private int field4 = 8;// static int field5 = 5; // 编译错误！普通内部类中不能定义 static 属性 public InnerClassA() &#123; System.out.println(\"创建 \" + this.getClass().getSimpleName() + \" 对象\"); System.out.println(\"其外部类的 outField1 字段的值为: \" + outField1); System.out.println(\"其外部类的 outField2 字段的值为: \" + outField2); System.out.println(\"其外部类的 outField3 字段的值为: \" + outField3); System.out.println(\"其外部类的 outField4 字段的值为: \" + outField4); &#125; &#125; public static void main(String[] args) &#123; InnerClassTest outerObj = new InnerClassTest(); // 不在外部类内部，使用：外部类对象. new 内部类构造器(); 的方式创建内部类对象// InnerClassA innerObj = outerObj.new InnerClassA(); &#125;&#125; 这里的内部类就像外部类声明的一个属性字段一样，因此其的对象时依附于外部类对象而存在的，我们来看一下结果： 我们注意到，内部类对象可以访问外部类对象中所有访问权限的字段，同时，外部类对象也可以通过内部类的对象引用来访问内部类中定义的所有访问权限的字段。 Java 不允许成员内部类中定义静态的成员。 局部内部类局部内部类就是在代码块中定义一个类，最典型的应用是在方法中定义一个类。例如： public class Method &#123; private static String name; private int age; public void hello()&#123; class MyInnerClass&#123; public void sayHello()&#123; System.out.println(name); System.out.println(age); &#125; &#125; &#125;&#125; 匿名内部类匿名内部类，顾名思义，是没有名字的类，那么既然它没有名字，自然也就无法显式的创建出其实例对象了，所以匿名内部类适合那种只使用一次的情境，例如： public class Test&#123; public static void main(String[] args)&#123; Object obj = new Object()&#123; @Override public String toString()&#123; return \"hello world\"; &#125; &#125;; &#125;&#125; 这就是一个典型的匿名内部类的使用，它等效于下面的代码： public class MyObj extends Object&#123; @Override public String toString()&#123; return \"hello world\"; &#125;&#125; public static void main(String[] args)&#123; Object obj = new MyObj();&#125; 为了一个只使用一次的类而单独创建一个 .java 文件，是否有些浪费和繁琐？ 在我看来，匿名内部类最大的好处就在于能够简化代码块。 显然，我们匿名内部类的构造器会调用相对应的父类构造器进行父类成员的初始化动作。而匿名内部类的本质也就这样，只是你看不到名字而已，其实编译器还是会为它生成单独的一份 Class 文件并拥有唯一的类名的。 其实你从编译器的层面上看，匿名内部类和一个实际的类型相差无几，它也能继承某个类并重写其中方法，实现某个接口的所有方法等。最吸引人的可能就是它无需单独创建类文件的简便性。 静态内部类静态内部类通过对定义在外部类内部的类加上关键字「static」进行修饰，以标示一个静态内部类，例如： public class OuterClass &#123; private static String name = \"hello world\"; private int age = 23; public static class MyInnerClass&#123; private static String myName = \"single\"; private int myAge = 23; public void sayHello()&#123; System.out.println(name); //编译器报错提示：不可访问的字段 age System.out.println(age); &#125; &#125;&#125; 首先，MyInnnerClass 作为一个内部类，它可以定义自己的静态属性，静态方法，实例属性，实例方法，和普通类一样。 此外，由于 MyInnerClass 作为一个内部类，它对于外部类 OuterClass 中部分成员也是可见的，但并全部可见，不同类型的内部类可见的外部类成员不尽相同，例如我们的静态内部类对于外部类的以下成员时可见的： 静态属性 静态方法 所以，我们上述的例子中，外部类 OuterClass 的实例属性 age 对于静态内部类 MyInnerClass 是不可见的。 那么 Java 是如何做到在一个类的内部定义另一个类的呢？ 实际上编译器在编译我们的外部类的时候，会扫描其内部是否还存在其他类型的定义，如果有那么会「搜集」这些类的代码，并按照某种特殊名称规则单独编译这些类。正如我们上述的 MyInnerClass 内部类会被单独编译成 OuterClass$MyInnerClass.class 文件。 当然，这里的特殊命名规则其实就是：外部类名 + $ + 内部类名。 如果你想要在外部直接创建一个静态内部类的实例，也是被允许的。例如： public static void main(String[] args)&#123; //创建静态内部类实例 OuterClass.MyInnerClass innerClass = new OuterClass.MyInnerClass(); innerClass.sayHello();&#125; 当然，这样的操作一般也不被推荐，因为一个内部类既然被定义在某个外围类的内部，那它一定是为这个外围类服务的，而你从外部越过外围类而单独创建内部类的实现显然是不符合面向对象设计思想的。 静态内部类的应用场景其实还是很多的，但有一个基本的设计准则是，静态内部类不需要依赖外围类的实例，独立于外围类，为外围类提供服务。 单例设计模式单例设计模式：保证类在内存中只有一个对象。 如何保证类在内存中只有一个对象呢？ (1)控制类的创建，不让其他类来创建本类的对象。private (2)在本类中定义一个本类的对象 。Singleton s; (3)提供公共的访问方式。 public static Singleton getInstance(){return s} 饿汉式的方法 public class Singleton &#123; //1.将构造方法私有化，不允许外部直接创建对象 private Singleton()&#123; &#125; //2.创建类的唯一实例，使用private static修饰 private static Singleton instance=new Singleton(); //3.提供一个用于获取实例的方法，使用public static修饰 public static Singleton getInstance()&#123; return instance; &#125;&#125; 懒汉式的方法 public class Singleton2 &#123; //1.将构造方式私有化，不允许外边直接创建对象 private Singleton2()&#123; &#125; //2.声明类的唯一实例，使用private static修饰 private static Singleton2 instance; //3.提供一个用于获取实例的方法，使用public static修饰 public static Singleton2 getInstance()&#123; if(instance==null)&#123; instance=new Singleton2(); &#125; return instance; &#125;&#125; 工厂方法模式工厂方法是在设计模式中常用的一种模式，它属于设计模式的创造类型模式，主要用来创建对象。 使用场景 创建对象需要大量重复的代码 客户端(应用层)不依赖与产品类实例如何被创建、实现细节 一个类通过其子类来指定创建那个对象 定义产品接口和工厂接口 /** * 冰箱接口 */public interface IRefrigerator &#123; //获取品牌名 String getBrandName(); //获取价格 double getPrice();&#125;/** * 冰箱工厂接口 */public interface IRefrigeratorFactory &#123; IRefrigerator getIRefrigerator();&#125; 产品实现类 /** * 海尔冰箱 */public class HaiErRefrigerator implements IRefrigerator &#123; @Override public String getBrandName() &#123; return \"海尔冰箱\"; &#125; @Override public double getPrice() &#123; return 5999; &#125;&#125;/** * 美的冰箱 */public class MeiDiRefrigerator implements IRefrigerator &#123; @Override public String getBrandName() &#123; return \"美的冰箱\"; &#125; @Override public double getPrice() &#123; return 2999; &#125;&#125;/** * 格力冰箱 */public class GeLiRefrigerator implements IRefrigerator &#123; @Override public String getBrandName() &#123; return \"格力冰箱\"; &#125; @Override public double getPrice() &#123; return 3999; &#125;&#125; 工厂实现类 /** * 海尔冰箱工厂 */public class HaiErRefrigeratorFactory implements IRefrigeratorFactory &#123; @Override public IRefrigerator getIRefrigerator() &#123; return new HaiErRefrigerator(); &#125;&#125;/** * 美的冰箱工厂 */public class MeiDiRefrigeratorFactory implements IRefrigeratorFactory &#123; @Override public IRefrigerator getIRefrigerator() &#123; return new MeiDiRefrigerator(); &#125;&#125;/** * 格力冰箱工厂 */public class GeLiRefrigeratorFactory implements IRefrigeratorFactory &#123; @Override public IRefrigerator getIRefrigerator() &#123; return new GeLiRefrigerator(); &#125;&#125; 测试类 /** * 测试类 */public class Test &#123; public static void main(String[] args) &#123; IRefrigeratorFactory refrigeratorFactory = new HaiErRefrigeratorFactory(); IRefrigeratorFactory refrigeratorFactory2 = new GeLiRefrigeratorFactory(); IRefrigeratorFactory refrigeratorFactory3 = new MeiDiRefrigeratorFactory(); IRefrigerator refrigerator = refrigeratorFactory.getIRefrigerator(); System.out.println(\"您购买了：\" + refrigerator.getBrandName() + \"，您需要支付：\" + refrigerator.getPrice()); &#125;&#125; 结果： 您购买了：海尔冰箱，您需要支付：5999.0 参考https://www.cnblogs.com/liulyuan/p/10318732.html https://images2015.cnblogs.com/blog/1081488/201704/1081488-20170425113401365-1070846569.jpg https://www.processon.com/view/link/5c920ad7e4b0ed6b43088a87#map https://www.cnblogs.com/lifexy/p/10812841.html https://blog.csdn.net/luojun13class/article/details/83043100 https://blog.csdn.net/qq_40180411/article/details/81385518 https://www.cnblogs.com/haiyuexiaozu/p/10986510.html https://blog.csdn.net/My_name_is_ZwZ/article/details/80001121 https://www.cnblogs.com/yangming1996/p/8869081.html https://blog.csdn.net/qq_33427267/article/details/84767419 https://www.jianshu.com/p/4c7d0ee96094","path":"2019/10/02/Java-面向对象总结/","date":"10-02","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java 包装类","text":"Java 包装类为什么需要包装类将基本数据类型封装成对象的好处在于可以在对象中定义更多的功能方法操作该数据。 常用的操作之一：用于基本数据类型与字符串之间的转换。 基本数据类型与包装类的对应关系byte → Byte short → Short int → Integer long → Long float → Float double → Double char → Character boolean→ Boolean 包装类的集成关系 包装类的基本操作 Integer类Integer 类在对象中包装了一个基本类型 int 的值,该类提供了多个方法，能在 int 类型和 String 类型之间互相转换，还提供了处理 int 类型时非常有用的其他一些常量和方法。 // 构造IntegerInteger i1 = new Integer(123);Integer i2 = new Integer(\"123\"); String和int类型的相互转换 // int转Stringint i = 100;String s1 = i + \" \";String s2 = String.valueOf(i);// String转intString s3 = \"123\";Integer i3 = new Integer(s3);int i4 = i3.intValue();int i5 = Integer.parseInt(s3); JDK5的新特性自动装箱和拆箱 // 自动装箱Integer i1 = 100;// 自动拆箱int i2 = i1; 思考1Integer i1 = 97;Integer i2 = 97;System.out.println(i1 == i2);System.out.println(i1.equals(i2));System.out.println(\"-----------\");Integer i3 = 197;Integer i4 = 197;System.out.println(i3 == i4);System.out.println(i3.equals(i4)); /*trueture-----------falsetrue*/ 分析：因为-128~127是byte的取值范围，如果在这个取值范围内，自动装箱就不会创建新的对象，而是从常量池中获取，超过了byte取值范围就会再创建新对象~这个就是 i1==i2 的结果为 true 的原因了。 思考2public class Test_Integer &#123; public static void main(String[] args) &#123; Integer i1 = new Integer(97); Integer i2 = new Integer(97); System.out.println(i1 == i2); System.out.println(i1.equals(i2)); System.out.println(\"----------------\"); Integer i3 = new Integer(197); Integer i4 = new Integer(197); System.out.println(i3 == i4); System.out.println(i3.equals(i4)); System.out.println(\"----------------\"); Integer i5 = 97; Integer i6 = 97; System.out.println(i5 == i6); System.out.println(i5.equals(i6)); System.out.println(\"----------------\"); Integer i7 = 197; Integer i8 = 197; System.out.println(i7 == i8); System.out.println(i7.equals(i8)); &#125;&#125; /*falseture----------------falsetrue----------------truetrue----------------falsetrue*/ 分析： 在String类前面文章，我们画过内存图。已经了解到== 这个符号既可以比较基本数据类型和引用数据类型，如果是基本数据类型，比较的是值相同，如果是引用数据类型，比较的是对象的内存地址是否相同。利用equals方法比较的是值是否相同，因为Integer类里面重写了Object类的equals方法。 所以，上面的equals方法打印输出都是true，这个很简单，没有疑问。然后来看，前面两组里面的==判断，由于==是比较内存地址，我们看到s1 s2 s3 s4都使用了new关键字，所以这两组里面的==比较也是不相等，内存地址不同。 最后来看后两组，同样是自动包装，为什么97就输出true，而197就不相等。我们这里先抛出个结论：在Java中，byte的取值范围是-128到127之间。在自动包装和拆箱中，如果变量取值范围在byte的范围内，在自动包装和拆装过程中就不新创建对象（堆内存）；如果范围不在byte取值范围内，就会新创建Integer对象。 参考https://www.cnblogs.com/wq-9/p/10305002.html https://blog.csdn.net/y0q2t57s/article/details/81039200 https://blog.csdn.net/u011541946/article/details/79978325","path":"2019/10/01/Java-包装类/","date":"10-01","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java 数组","text":"Java 数组首先，数组是Java中的对象。它们不是像int、short或long这样的基本类，也不是具有很多方法的全功能对象，但由于它们是对象，所以它们隐式地扩展了Object，这就是为什么可以使用数组引用（例如toString（））调用java.lang.object的任何方法。 Java中数组的另一个重要之处是，一旦创建，就不能更改数组的大小。 数组拷贝目前在Java中数据拷贝提供了如下方式： clone System.arraycopy Arrays.copyOf Arrays.copyOfRange clone 方法 clone方法是从Object类继承过来的，基本数据类型（int ，boolean，char，byte，short，float ，double，long）都可以直接使用clone方法进行克隆，注意String类型是因为其值不可变所以才可以使用。 int[] a1 = &#123;1, 3&#125;;int[] a2 = a1.clone();a1[0] = 666;System.out.println(Arrays.toString(a1)); //[666, 3]System.out.println(Arrays.toString(a2)); //[1, 3] String[] a1 = &#123;\"a1\", \"a2\"&#125;;String[] a2 = a1.clone();a1[0] = \"b1\"; //更改a1数组中元素的值System.out.println(Arrays.toString(a1)); //[b1, a2]System.out.println(Arrays.toString(a2)); //[a1, a2] System.arraycopy int[] a1 = &#123;1, 2, 3, 4, 5&#125;;int[] a2 = new int[10];//（原数组， 原数组的开始位置， 目标数组， 目标数组的开始位置， 拷贝个数）System.arraycopy(a1, 1, a2, 3, 3);System.out.println(Arrays.toString(a1)); // [1, 2, 3, 4, 5]System.out.println(Arrays.toString(a2)); // [0, 0, 0, 2, 3, 4, 0, 0, 0, 0] Arrays.copyOf int[] a1 = &#123;1, 2, 3, 4, 5&#125;;//（原数组，拷贝的个数）int[] a2 = Arrays.copyOf(a1, 3);System.out.println(Arrays.toString(a1)) // [1, 2, 3, 4, 5]System.out.println(Arrays.toString(a2)) // [1, 2, 3] Arrays.copyOfRange int[] a1 = &#123;1, 2, 3, 4, 5&#125;;//（原数组，开始位置，拷贝的个数）int[] a2 = Arrays.copyOfRange(a1, 0, 1);System.out.println(Arrays.toString(a1)) // [1, 2, 3, 4, 5]System.out.println(Arrays.toString(a2)) // [1] 深拷贝和浅拷贝浅拷贝是按位拷贝对象，它会创建一个新对象，这个对象有着原始对象属性值的一份精确拷贝。如果属性是基本类型，拷贝的就是基本类型的值；如果属性是内存地址（引用类型），拷贝的就是内存地址 ，因此如果其中一个对象改变了这个地址，就会影响到另一个对象。即默认拷贝构造函数只是对对象进行浅拷贝复制(逐个成员依次拷贝)，即只复制对象空间而不复制资源。 深拷贝，在拷贝引用类型成员变量时，为引用类型的数据成员另辟了一个独立的内存空间，实现真正内容上的拷贝。","path":"2019/09/29/Java-数组/","date":"09-29","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"Java 字符串","text":"Java 字符串字符串常量池public static void main(String[] args) &#123; String s1 = \"hello\"; String s2 = new String(\"hello\"); System.out.println(s1 == s2); //false&#125; 对于这种直接通过双引号””声明字符串的方式, 虚拟机首先会到字符串常量池中查找该字符串是否已经存在. 如果存在会直接返回该引用, 如果不存在则会在堆内存中创建该字符串对象, 然后到字符串常量池中注册该字符串. 当我们使用new关键字创建字符串对象的时候, JVM将不会查询字符串常量池, 它将会直接在堆内存中创建一个字符串对象, 并返回给所属变量. 格式化字符串String类的format()方法用于创建格式化的字符串以及连接多个字符串对象。 转 换 符 说 明 示 例 %s 字符串类型 “mingrisoft” %c 字符类型 ‘m’ %b 布尔类型 true %d 整数类型（十进制） 99 %x 整数类型（十六进制） FF %o 整数类型（八进制） 77 %f 浮点类型 99.99 %a 十六进制浮点类型 FF.35AE %e 指数类型 9.38e+5 %g 通用浮点类型（f和e类型中较短的） %h 散列码 %% 百分比类型 ％ %n 换行符 %tx 日期与时间类型（x代表不同的日期与时间转换符 public static void main(String[] args) &#123; String str=null; str=String.format(\"Hi,%s\", \"王力\"); System.out.println(str); str=String.format(\"Hi,%s:%s.%s\", \"王南\",\"王力\",\"王张\"); System.out.println(str); System.out.printf(\"字母a的大写是：%c %n\", 'A'); System.out.printf(\"3&gt;7的结果是：%b %n\", 3&gt;7); System.out.printf(\"100的一半是：%d %n\", 100/2); System.out.printf(\"100的16进制数是：%x %n\", 100); System.out.printf(\"100的8进制数是：%o %n\", 100); System.out.printf(\"50元的书打8.5折扣是：%f 元%n\", 50*0.85); System.out.printf(\"上面价格的16进制数是：%a %n\", 50*0.85); System.out.printf(\"上面价格的指数表示：%e %n\", 50*0.85); System.out.printf(\"上面价格的指数和浮点数结果的长度较短的是：%g %n\", 50*0.85); System.out.printf(\"上面的折扣是%d%% %n\", 85); System.out.printf(\"字母A的散列码是：%h %n\", 'A'); &#125;//输出结果/*Hi,王力 Hi,王南:王力.王张 字母a的大写是：A 3&gt;7的结果是：false 100的一半是：50 100的16进制数是：64 100的8进制数是：144 50元的书打8.5折扣是：42.500000 元 上面价格的16进制数是：0x1.54p5 上面价格的指数表示：4.250000e+01 上面价格的指数和浮点数结果的长度较短的是：42.5000 上面的折扣是85% 字母A的散列码是：41*/ 标 志 说 明 示 例 结 果 + 为正数或者负数添加符号 (“%+d”,15) +15 − 左对齐 (“%-5d”,15) \\ 15 \\ 0 数字前面补0 (“%04d”, 99) 0099 空格 在整数之前添加指定数量的空格 (“% 4d”, 99) \\ 99\\ , 以“,”对数字分组 (“%,f”, 9999.99) 9,999.990000 ( 使用括号包含负数 (“%(f”, -99.99) (99.990000) # 如果是浮点数则包含小数点，如果是16进制或8进制则添加0x或0 (“%#x”, 99)(“%#o”, 99) 0x630143 &lt; 格式化前一个转换符所描述的参数 (“%f和%&lt;3.2f”, 99.45) 99.450000和99.45 $ 被格式化的参数索引 (“%1$d,%2$s”, 99,”abc”) 99,abc public static void main(String[] args) &#123; String str=null; //$使用 str=String.format(\"格式参数$的使用：%1$d,%2$s\", 99,\"abc\"); System.out.println(str); //+使用 System.out.printf(\"显示正负数的符号：%+d与%d%n\", 99,-99); //补O使用 System.out.printf(\"最牛的编号是：%03d%n\", 7); //空格使用 System.out.printf(\"Tab键的效果是：% 8d%n\", 7); //.使用 System.out.printf(\"整数分组的效果是：%,d%n\", 9989997); //空格和小数点后面个数 System.out.printf(\"一本书的价格是：% 50.5f元%n\", 49.8); &#125;//输出结果/*格式参数$的使用：99,abc 显示正负数的符号：+99与-99 最牛的编号是：007 Tab键的效果是： 7 整数分组的效果是：9,989,997 一本书的价格是： 49.80000元*/ 日期和时间字符串格式化字符串格式中还有%tx转换符没有详细介绍，它是专门用来格式化日期和时 间的。%tx转换符中的x代表另外的处理日期和时间格式的转换符，它们的组合能够将日期和时间格式化成多种格式。 转 换 符 说 明 示 例 c 包括全部日期和时间信息 星期六 十月 27 14:21:20 CST 2007 F “年-月-日”格式 2007-10-27 D “月/日/年”格式 10/27/07 r “HH:MM:SS PM”格式（12时制） 02:25:51 下午 T “HH:MM:SS”格式（24时制） 14:28:16 R “HH:MM”格式（24时制） 14:28 public static void main(String[] args) &#123; Date date=new Date(); //c的使用 System.out.printf(\"全部日期和时间信息：%tc%n\",date); //f的使用 System.out.printf(\"年-月-日格式：%tF%n\",date); //d的使用 System.out.printf(\"月/日/年格式：%tD%n\",date); //r的使用 System.out.printf(\"HH:MM:SS PM格式（12时制）：%tr%n\",date); //t的使用 System.out.printf(\"HH:MM:SS格式（24时制）：%tT%n\",date); //R的使用 System.out.printf(\"HH:MM格式（24时制）：%tR\",date); &#125;//输出结果/*全部日期和时间信息：星期一 九月 10 10:43:36 CST 2012 年-月-日格式：2012-09-10 月/日/年格式：09/10/12 HH:MM:SS PM格式（12时制）：10:43:36 上午 HH:MM:SS格式（24时制）：10:43:36 HH:MM格式（24时制）：10:43*/ 正则表达式元字符 代码 说明 . 匹配除换行符以外的任意字符 \\w 匹配字母或数字或下划线或汉字 \\s 匹配任意的空白符 \\d 匹配数字 ^ 匹配字符串的开始 $ 匹配字符串的结束 \\b 匹配字符串的结束 重复 代码/语法 说明 * 重复零次或更多次 + 重复一次或更多次 ? 重复零次或一次 {n} 重复n次 {n,} 重复n次或更多次 {n,m} 重复n到m次 字符类 像[aeiou]就匹配任何一个英文元音字母，[.?!]匹配标点符号(.或?或!)。我们也可以轻松地指定一个字符范围，像[0-9]代表的含意与\\d就是完全一致的：一位数字；同理[a-z0-9A-Z_]也完全等同于\\w（如果只考虑英文的话）。 import java.util.regex.*; class RegexExample1&#123; public static void main(String args[])&#123; String content = \"I am noob \" + \"from runoob.com.\"; String pattern = \".*runoob.*\"; boolean isMatch = Pattern.matches(pattern, content); System.out.println(\"字符串中是否包含了 'runoob' 子字符串? \" + isMatch); &#125;&#125;//输出结果//字符串中是否包含了 'runoob' 子字符串? true 字符串生成器字符串生成器即StringBuilder类，是字符串一个重要的常用类。新创建的StringBuilder对象初始容量是16个字符，可以自行指定初始长度，也可以动态地执行添加、删除和插入等字符串的编辑操作，大大提高了频繁增加字符串的效率。 如果在程序中频繁地附加字符串，建议使用StringBuilder。 public class Jerque &#123; // 新建类 public static void main(String[] args) &#123; // 主方法 String str = \"\"; // 创建空字符串 // 定义对字符串执行操作的起始时间 long starTime = System.currentTimeMillis(); for (int i = 0; i &lt; 10000; i++) &#123; // 利用for循环执行10000次操作 str = str + i; // 循环追加字符串 &#125; long endTime = System.currentTimeMillis(); // 定义对字符串操作后的时间 long time = endTime - starTime; // 计算对字符串执行操作的时间 System.out.println(\"Sting消耗时间：\" + time); // 将执行的时间输出 StringBuilder builder = new StringBuilder(\"\"); // 创建字符串生成器 starTime = System.currentTimeMillis(); // 定义操作执行前的时间 for (int j = 0; j &lt; 10000; j++) &#123; // 利用for循环进行操作 builder.append(j); // 循环追加字符 &#125; endTime = System.currentTimeMillis(); // 定义操作后的时间 time = endTime - starTime; // 追加操作执行的时间 System.out.println(\"StringBuilder消耗时间：\" + time); // 将操作时间输出 &#125;&#125;//输出结果/*运行结果：Sting消耗时间：1355StringBuilder消耗时间：2*/","path":"2019/09/29/Java-字符串/","date":"09-29","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"JDK下载配置环境","text":"JDK下载配置环境Ubuntu打开jdk下载页面 https://www.oracle.com/technetwork/java/javase/downloads/index.html 选择一个jdk版本，如jdk13 下载Linux版本，这里选择压缩包版本jdk-13_linux-x64_bin.tar.gz 这里把把压缩包解压到/usr/local文件夹下方便管理 sudo tar -zxvf jdk-13_linux-x64_bin.tar.gz -C /usr/local/ 然后添加环境变量 sudo vim ~/bashrc# 打开配置# 添加以下内容\"\"\"#set oracle jdk environmentexport JAVA_HOME=/usr/local/jdk-13/ ## 这里要注意目录要换成自己解压的jdk 目录export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH \"\"\"# 然后使环境变量生效source ~/.bashrc 然后测试jdk是否安装完毕 java -version 若是要转换jdk版本的话就将所有jdk放在/usr/local/文件夹下，然后修改~/.bashrc中的JAVA_HOME路径即可 Windows打开jdk下载页面 https://www.oracle.com/technetwork/java/javase/downloads/index.html 选择一个jdk版本，如jdk13 这里下载jdk-13_windows-x64_bin.zip 这里把它解压到F盘并命名为jdk-13 最后配置环境变量 JAVA_HOME F:\\java\\jdk-13 Path %JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin CLASSPATH .;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\tools.jar 最后管理员打开cmd，测试是否安装完成 java -version 若是要转换jdk版本的话就将所有jdk放在F:\\java\\jdk-13文件夹下，然后修改环境变量中的JAVA_HOME路径即可","path":"2019/09/27/JDK下载配置环境/","date":"09-27","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"IntelliJ IDEA 使用","text":"IntelliJ IDEA 使用配置vmoptions 参数 意义 -Xms128m Java Heap初始值，Server端JVM最好将-Xms和-Xmx设为相同值，开发测试机JVM可以保留默认值 -Xmx750m Java Heap最大值，默认值为物理内存的1/4，最佳设值应该视物理内存大小及计算机内其他内存开销而定 -Xms128m //初始内存值，影响idea启动速度-Xmx750m //最大内存数，降低垃圾回收频率，提升程序性能-XX:ReservedCodeCacheSize=240m //保留代码的缓存大小，减少垃圾代码回收频率 实用快捷键Ctrl+/ 或 Ctrl+Shift+/ 注释（// 或者/…/ ） Ctrl+D 复制行 Ctrl+X 删除行 快速修复 alt+enter(modify/cast) 代码提示 alt+/ Ctrl+G 定位某一行 Shift+F6 重构-重命名 IDEA 批量修改变量名 点击变量名后按shift+F6 Ctrl+R 替换文本 Ctrl+F 查找文本 代码处F2 快速定位编译出错位置 Ctrl+E 最近打开的文件 Ctrl+J 自动代码 Ctrl+ home/end 抵达文件头部,底部 组织导入 ctr+alt+O 大小写转化 ctr+shift+U IntelliJ Idea 常用快捷键Alt+回车导入包,自动修正 Ctrl+N 查找类 Ctrl+Shift+Space 自动补全代码 CTRL+D 复制行 CTRL+X 剪切,删除行 CIRL+U 大小写切换 CTRL+Z 倒退 CTRL+SHIFT+Z 向前","path":"2019/09/24/IntelliJ-IDEA-使用/","date":"09-24","excerpt":"","tags":[{"name":"java","slug":"java","permalink":"https://litianbo243.github.io/tags/java/"},{"name":"学习java","slug":"学习java","permalink":"https://litianbo243.github.io/tags/学习java/"}]},{"title":"深度学习之多GPU并行训练","text":"深度学习之多GPU并行训练通常情况下，多GPU运算分为单机多卡和多机多卡，两者在pytorch上面的实现并不相同，因为多机时，需要多个机器之间的通信协议等设置。 pytorch实现单机多卡十分容易，其基本原理就是：加入我们一次性读入一个batch的数据, 其大小为[16, 10, 5]，我们有4张卡可以使用。那么计算过程遵循以下步骤： 假设我们有4个GPU可以用，pytorch先把模型同步放到4个GPU中。 那么首先将数据分为4份，按照次序放置到四个GPU的模型中，每一份大小为[4, 10, 5]； 每个GPU分别进行前项计算过程； 前向过程计算完后，pytorch再从四个GPU中收集计算后的结果假设[4, 10, 5]，然后再按照次序将其拼接起来[16, 10, 5]，计算loss。整个过程其实就是 同步模型参数→分别前向计算→计算损失→梯度反传 pytorch 实现在我们设备中确实存在多卡的条件下，最简单的方法是直接使用torch.nn.DataParallel将你的模型wrap一下即可： net = torch.nn.DataParallel(model) 这时，默认所有存在的显卡都会被使用。 如果我们机子中有很多显卡(例如我们有八张显卡)，但我们只想使用0、1、2号显卡，那么我们可以： net = torch.nn.DataParallel(model, device_ids=[0, 1, 2]) DistributedParallel另一种方法DistributedParallel，虽然主要的目标为分布式训练，但也是可以实现单主机多GPU方式训练的，只不过比上一种方法稍微麻烦一点，但是训练速度和效果比上一种更好。 上述的命令和我们平常的命令稍有区别，这里我们用到了torch.distributed.launch这个module，我们选择运行的方式变换为python -m，上面相当于使用torch.distributed.launch.py去运行我们的YOUR_TRAINING_SCRIPT.py，其中torch.distributed.launch会向我们的运行程序传递一些变量。 为此，我们的YOUR_TRAINING_SCRIPT.py也就是我们的训练代码中这样写(省略多余代码，只保留核心代码)： import torch.distributed as dist# 这个参数是torch.distributed.launch传递过来的，我们设置位置参数来接受，local_rank代表当前程序进程使用的GPU标号parser.add_argument(\"--local_rank\", type=int, default=0) def synchronize(): \"\"\" Helper function to synchronize (barrier) among all processes when using distributed training \"\"\" if not dist.is_available(): return if not dist.is_initialized(): return world_size = dist.get_world_size() if world_size == 1: return dist.barrier() ## WORLD_SIZE 由torch.distributed.launch.py产生 具体数值为 nproc_per_node*node(主机数，这里为1)num_gpus = int(os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1 is_distributed = num_gpus &gt; 1 if is_distributed: torch.cuda.set_device(args.local_rank) # 这里设定每一个进程使用的GPU是一定的 torch.distributed.init_process_group( backend=\"nccl\", init_method=\"env://\" ) synchronize() # 将模型移至到DistributedDataParallel中，此时就可以进行训练了if is_distributed: model = torch.nn.parallel.DistributedDataParallel( model, device_ids=[args.local_rank], output_device=args.local_rank, # this should be removed if we update BatchNorm stats broadcast_buffers=False, ) # 注意，在测试的时候需要执行 model = model.module 参考https://www.jianshu.com/p/b366cad90a6c https://blog.csdn.net/andrew80/article/details/89189544","path":"2019/09/19/深度学习之多GPU并行训练/","date":"09-19","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"pytorch在验证和测试时的注意事项","text":"pytorch在验证和测试时的注意事项pytorch有时在训练过程中，没有出现CUDA error: out of memory 但是在验证和测试的时候出现CUDA error: out of memory 这是因为在验证和测试时没有设置torch.no_grad(): 这样在验证和测试的时候，tensor就会进行对梯度进行保留，并且这个梯度信息会累加，随着时间，模型小号的显存和越来越大，最后out of memory 解决方法： # 验证和测试时使用with torch.no_grad(): forward()","path":"2019/09/19/pytorch在验证和测试时的注意事项/","date":"09-19","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"深度学习之GPU和显存分析","text":"深度学习之GPU和显存分析预备知识nvidia-smi是Nvidia显卡命令行管理套件，基于NVML库，旨在管理和监控Nvidia GPU设备. 这是nvidia-smi命令的输出，其中最重要的两个指标： 显存占用 GPU利用率 显存占用和GPU利用率是两个不一样的东西，显卡是由GPU计算单元和显存等组成的，显存和GPU的关系有点类似于内存和CPU的关系。 推荐工具：gpustat pip install gpustatwatch --color -n1 gpustat -cpu 显存可以看成是空间，类似于内存。 显存用于存放模型，数据 显存越大，所能运行的网络也就越大 GPU计算单元类似于CPU中的核，用来进行数值计算。衡量计算量的单位是flop： the number of floating-point multiplication-adds，浮点数先乘后加算一个flop。计算能力越强大，速度越快。衡量计算能力的单位是flops： 每秒能执行的flop数量 1*2+3 1 flop1*2 + 3*4 + 4*5 3 flop 显存分析1Byte = 8 bit1K = 1024 Byte1M = 1024 K1G = 1024 M1T = 1024 G 10 K = 10*1024 Byte 除了K、M，G，T等之外，我们常用的还有KB 、MB，GB，TB 。二者有细微的差别。 1Byte = 8 bit1KB = 1000 Byte1MB = 1000 KB1GB = 1000 MB1TB = 1000 GB 10 KB = 10000 Byte Float32 是在深度学习中最常用的数值类型，称为单精度浮点数，每一个单精度浮点数占用4Byte的显存。 举例来说：有一个1000x1000的 矩阵，float32，那么占用的显存差不多就是 1000x1000x4 Byte = 4MB 32x3x256x256的四维数组（BxCxHxW）占用显存为：24M 神经网络显存占用 神经网络模型占用的显存包括： 模型自身的参数 模型的输入输出 举例来说，对于如下图所示的一个全连接网络(不考虑偏置项b) 模型的输入输出和参数 模型的显存占用包括： 参数：二维数组 W 模型的输出： 二维数组 Y 输入X可以看成是上一层的输出，因此把它的显存占用归于上一层。 这么看来显存占用就是W和Y两个数组？ 并非如此！！！ 只有有参数的层，才会有显存占用。这部份的显存占用和输入无关，模型加载完成之后就会占用。 有参数的层主要包括： 卷积 全连接 BatchNorm Embedding层 … … 无参数的层： 多数的激活层(Sigmoid/ReLU) 池化层 Dropout … … Linear(M-&gt;N): 参数数目：M×N Conv2d(Cin, Cout, K): 参数数目：Cin × Cout × K × K BatchNorm(N): 参数数目： 2N Embedding(N,W): 参数数目： N × W 参数占用显存 = 参数数目×n n = 4 ：float32 n = 2 : float16 n = 8 : double64 在PyTorch中，当你执行完model=MyGreatModel().cuda()之后就会占用相应的显存，占用的显存大小基本与上述分析的显存差不多（会稍大一些，因为其它开销）。 梯度与动量的显存占用 举例来说， 优化器如果是SGD： 可以看出来，除了保存W之外还要保存对应的梯度∇F(W) ，因此显存占用等于参数占用的显存x2, 如果是带Momentum-SGD 这时候还需要保存动量， 因此显存x3 如果是Adam优化器，动量占用的显存更多，显存x4 输入输出的显存占用 这部份的显存主要看输出的feature map 的形状。 据此可以计算出每一层输出的Tensor的形状，然后就能计算出相应的显存占用。 深度学习中神经网络的显存占用，我们可以得到如下公式： 显存占用 = 模型显存占用 + batch_size × 每个样本的显存占用 AlexNet 分析AlexNet的分析如下图，左边是每一层的参数数目（不是显存占用），右边是消耗的计算资源 可以看出： 全连接层占据了绝大多数的参数 卷积层的计算量最大 参考https://blog.csdn.net/wsxzhbzl/article/details/88815488","path":"2019/09/18/深度学习之GPU和显存分析/","date":"09-18","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"loss function 总结","text":"loss function损失函数的基本用法： criterion = LossCriterion() # 构造函数有自己的参数loss = criterion(x, y) # 调用标准时也有参数 得到的loss结果已经对mini-batch数量取了平均值 注： reduction ( string , optional ) – Specifies the reduction to apply to the output: &#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;. &#39;none&#39;: no reduction will be applied, &#39;mean&#39;: the sum of the output will be divided by the number of elements in the output, &#39;sum&#39;: the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: &#39;mean&#39; weight (Tensor, optional) – a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. L1Loss创建一个criterion计算input x和target y的每个元素的平均绝对误差(mean absolute error (MAE)) loss(x_n,y_n) = \\sum_{n=1}^N | x_n - y_n |from torch import nnloss = nn.L1Loss()# Input: (N, *)where ∗ means, any number of additional dimensionsinput = torch.randn(3, 5, requires_grad=True)# Target: (N, *), same shape as the inputtarget = torch.randn(3, 5)# Output: scalar. If reduction is 'none', then (N,∗), same shape as the inputoutput = loss(input, target)output.backward() MSELoss创建一个criterion计算input x和target y的每个元素的均方误差(mean absolute error (MAE)) torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean') loss(x_n,y_n) = \\sum_{n=1}^N (x_n - y_n)^2from torch import nnloss = nn.MSELoss()# Input: (N, *)where ∗ means, any number of additional dimensionsinput = torch.randn(3, 5, requires_grad=True)# Target: (N, *), same shape as the inputtarget = torch.randn(3, 5)# Output: scalar. If reduction is 'none', then (N,∗), same shape as the inputoutput = loss(input, target)output.backward() CrossEntropyLoss该criterion将nn.LogSoftmax()和nn.NLLLoss()方法结合到一个类中 torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') loss(x,class) = -log(\\frac {e^{x[class]}} {\\sum_j e^{x[j]}})from torch import nnloss = nn.CrossEntropyLoss()# Input: (N, C) where C = number of classesinput = torch.randn(3, 5, requires_grad=True)# Target: (N) where each value is 0 &lt;= targets[i] &lt;= C-1target = torch.empty(3, dtype=torch.long).random_(5)# Output: scalar. If reduction is 'none', then the same size as the target: (N)output = loss(input, target)output.backward() NLLLoss用于多分类的负对数似然损失函数(negative log likelihood loss) torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean') from torch import nnm = nn.LogSoftmax(dim=1)loss = nn.NLLLoss()# input is of size N x C = 3 x 5input = torch.randn(3, 5, requires_grad=True)# each element in target has to have 0 &lt;= value &lt; Ctarget = torch.tensor([1, 0, 4])output = loss(m(input), target)output.backward()# 2D loss example (used, for example, with image inputs)N, C = 5, 4loss = nn.NLLLoss()# input is of size N x C x height x widthdata = torch.randn(N, 16, 10, 10)conv = nn.Conv2d(16, C, (3, 3))m = nn.LogSoftmax(dim=1)# each element in target has to have 0 &lt;= value &lt; Ctarget = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)output = loss(m(conv(data)), target)output.backward() BCELoss创建一个衡量目标和输出之间二进制交叉熵的criterion torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean') loss(x_n,y_n) = -w_n[y_n \\cdot logx_n + (1-y_n) \\cdot log(1-x_n)]from torch import nnm = nn.Sigmoid()loss = nn.BCELoss()# Input: (N, *) where ∗ means, any number of additional dimensionsinput = torch.randn(3, requires_grad=True)# Target: (N, *) , same shape as the inputtarget = torch.empty(3).random_(2)# Output: scalar. If reduction is 'none', then (N, *) , same shape as input.output = loss(m(input), target)output.backward() BCEWithLogitsLoss将sigmoid函数和BCELoss方法结合到一个类中 这个版本在数值上比使用一个带着BCELoss损失函数的简单的Sigmoid函数更稳定，通过将操作合并到一层中，我们利用log-sum-exp技巧来实现数值稳定性。 torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None) 多出参数： pos_weight (Tensor,**可选) –正值例子的权重，必须是有着与分类数目相同的长度的向量 from torch import nnloss = nn.BCEWithLogitsLoss()# Input: (N, *) where ∗ means, any number of additional dimensionsinput = torch.randn(3,requires_grad=True)# Target: (N, *) , same shape as the inputtarget = torch.empty(3).random_(2)# Output: scalar. If reduction is 'none', then (N, *) , same shape as input.output = loss(input, target)output.backward() 参考https://pytorch.org/docs/stable/index.html","path":"2019/09/16/loss-function-总结/","date":"09-16","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"优化器 介绍","text":"优化器介绍要使用torch.optim，您必须构造一个optimizer对象。这个对象能保存当前的参数状态并且基于计算梯度更新参数 optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)optimizer = optim.Adam([var1, var2], lr = 0.0001)for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() SGDBatch Gradient Descent在每一轮的训练过程中，Batch Gradient Descent算法用整个训练集的数据计算cost fuction的梯度，并用该梯度对模型参数进行更新： \\Theta = \\Theta - \\alpha \\cdot \\bigtriangledown_{\\Theta} J(\\Theta)优点: cost fuction若为凸函数，能够保证收敛到全局最优值；若为非凸函数，能够收敛到局部最优值 缺点: 由于每轮迭代都需要在整个数据集上计算一次，所以批量梯度下降可能非常慢 训练数较多时，需要较大内存 批量梯度下降不允许在线更新模型，例如新增实例。 Stochastic Gradient Descent和批梯度下降算法相反，Stochastic gradient descent 算法每读入一个数据，便立刻计算cost fuction的梯度来更新参数： \\Theta = \\Theta - \\alpha \\cdot \\bigtriangledown_{\\Theta} J(\\Theta; x^{(i)}, y^{(i)})优点: 算法收敛速度快(在Batch Gradient Descent算法中, 每轮会计算很多相似样本的梯度, 这部分是冗余的) 可以在线更新 有几率跳出一个比较差的局部最优而收敛到一个更好的局部最优甚至是全局最优 缺点: 容易收敛到局部最优，并且容易被困在鞍点 Mini-batch Gradient Descentmini-batch Gradient Descent的方法是在上述两个方法中取折衷, 每次从所有训练数据中取一个子集（mini-batch） 用于计算梯度： \\Theta = \\Theta - \\alpha \\cdot \\bigtriangledown_{\\Theta} J(\\Theta; x^{(i:i+n)}, y^{(i:i+n)})Mini-batch Gradient Descent在每轮迭代中仅仅计算一个mini-batch的梯度，不仅计算效率高，而且收敛较为稳定。该方法是目前深度学训练中的主流方法 Mini-batch Gradient Descent在每轮迭代中仅仅计算一个mini-batch的梯度，不仅计算效率高，而且收敛较为稳定。该方法是目前深度学训练中的主流方法 上述三个方法面临的主要挑战如下： 选择适当的学习率α 较为困难。太小的学习率会导致收敛缓慢，而学习速度太块会造成较大波动，妨碍收敛。 目前可采用的方法是在训练过程中调整学习率大小，例如模拟退火算法：预先定义一个迭代次数m，每执行完m次训练便减小学习率，或者当cost function的值低于一个阈值时减小学习率。然而迭代次数和阈值必须事先定义，因此无法适应数据集的特点。 上述方法中, 每个参数的 learning rate 都是相同的，这种做法是不合理的：如果训练数据是稀疏的，并且不同特征的出现频率差异较大，那么比较合理的做法是对于出现频率低的特征设置较大的学习速率，对于出现频率较大的特征数据设置较小的学习速率。 近期的的研究表明，深层神经网络之所以比较难训练，并不是因为容易进入local minimum。相反，由于网络结构非常复杂，在绝大多数情况下即使是 local minimum 也可以得到非常好的结果。而之所以难训练是因为学习过程容易陷入到马鞍面中，即在坡面上，一部分点是上升的，一部分点是下降的。而这种情况比较容易出现在平坦区域，在这种区域中，所有方向的梯度值都几乎是 0。 optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False) from torch import optimoptimizer = optim.SGD(model.parameters(), lr=0.1)optimizer.zero_grad()loss_fn(model(input), target).backward()optimizer.step() MomentumSGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力： v_t = \\gamma \\cdot v_{t-1} + \\alpha \\cdot \\bigtriangledown_{\\Theta} J(\\Theta) \\\\ \\Theta = \\Theta - v_tMomentum算法会观察历史梯度vt−1，若当前梯度的方向与历史梯度一致（表明当前样本不太可能为异常点），则会增强这个方向的梯度，若当前梯度与历史梯方向不一致，则梯度会衰减。一种形象的解释是：我们把一个球推下山，球在下坡时积聚动量，在途中变得越来越快，γ可视为空气阻力，若球的方向发生变化，则动量会衰减。 optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False) from torch import optimoptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)optimizer.zero_grad()loss_fn(model(input), target).backward()optimizer.step() Nesterov Momentum在小球向下滚动的过程中，我们希望小球能够提前知道在哪些地方坡面会上升，这样在遇到上升坡面之前，小球就开始减速。这方法就是Nesterov Momentum，其在凸优化中有较强的理论保证收敛。并且，在实践中Nesterov Momentum也比单纯的 Momentum 的效果好： v_t = \\gamma \\cdot v_{t-1} + \\alpha \\cdot \\bigtriangledown_{\\Theta} J(\\Theta - \\gamma v_{t-1}) \\\\ \\Theta = \\Theta -v_t其核心思想是：注意到 momentum 方法，如果只看 γ v 项，那么当前的 θ经过 momentum 的作用会变成 θ-γ v。因此可以把 θ-γ v这个位置看做是当前优化的一个”展望”位置。所以，可以在 θ-γ v求导, 而不是原始的θ。 optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False) from torch import optimoptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)optimizer.zero_grad()loss_fn(model(input), target).backward()optimizer.step() Adagrad上述方法中，对于每一个参数θi 的训练都使用了相同的学习率α。Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新；相反，对于出现频率较高的参数采用较小的α更新。因此，Adagrad非常适合处理稀疏数据。 我们设gt,为第t轮第i个参数的梯度，即gt,i=▽ΘJ(Θi)。因此，SGD中参数更新的过程可写为： \\Theta_{t+1,i} = \\Theta_{t,i} - \\alpha \\cdot g_{t,i}Adagrad在每轮训练中对每个参数θi 的学习率进行更新，参数更新公式如下： \\Theta_{t+1,i} = \\Theta_{t,i} - \\frac {\\alpha} {\\sqrt {G_{t,ii} + \\epsilon}} \\cdot g_{t,i}其中，Gt∈ℝd×d 为对角矩阵，每个对角线位置i,i为对应参数θi 从第1轮到第t轮梯度的平方和。ϵ是平滑项，用于避免分母为0，一般取值1e−8。Adagrad的缺点是在训练的中后期，分母上梯度平方的累加将会越来越大，从而梯度趋近于0，使得训练提前结束。 optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0) from torch import optimoptimizer = optim.Adagrad(model.parameters(), lr=0.1)optimizer.zero_grad()loss_fn(model(input), target).backward()optimizer.step() RMSpropRMSprop是Geoff Hinton提出的一种自适应学习率方法。Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题。 E[g^2]_t = 0.9E[g^2]_{t-1} + 0.1g_t^2 \\\\ \\Theta_{t+1} = \\Theta_{t} - \\frac {\\alpha} {\\sqrt {E[g^2]_t + \\epsilon}} \\cdot g_{t}torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False) from torch import optimoptimizer = optim.RMSprop(model.parameters(), lr=0.1, alpha=0.9)optimizer.zero_grad()loss_fn(model(input), target).backward()optimizer.step() AdamAdam(Adaptive Moment Estimation)是另一种自适应学习率的方法。它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下： m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t \\\\ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2 \\\\ \\hat m_t = \\frac {m_t} {1 - \\beta_1^t} \\\\ \\hat v_t = \\frac {v_t} {1 - \\beta_2^t} \\\\ \\Theta_{t+1} = \\Theta_t - \\frac {\\alpha} {\\sqrt {\\hat v_t} + \\epsilon} \\hat m_t其中，mt ，vt 分别是对梯度的一阶矩估计和二阶矩估计，可以看作对期望E[gt] ，E[g2t] 的近似;mt^，vt^是对mt ，vt 的校正，这样可以近似为对期望的无偏估计。Adam算法的提出者建议β1的默认值为0.9，β2 的默认值为.999， \\epsilon 默认为10^−8 。 另外，在数据比较稀疏的时候，adaptive的方法能得到更好的效果，例如Adagrad，RMSprop, Adam 等。Adam 方法也会比 RMSprop方法收敛的结果要好一些, 所以在实际应用中 ，Adam为最常用的方法，可以比较快地得到一个预估结果。 optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) from torch import optimoptimizer = optim.Adam(model.parameters(), lr=0.1, beta=(0.9, 0.99))optimizer.zero_grad()loss_fn(model(input), target).backward()optimizer.step() 表现最后两张动图从直观上展现了算法的优化过程。第一张图为不同算法在损失平面等高线上随时间的变化情况，第二张图为不同算法在鞍点处的行为比较。 调整学习率torch.optim.lr_scheduler 提供了几种方法来根据epoches的数量调整学习率。torch.optim.lr_scheduler.ReduceLROnPlateau允许基于一些验证测量来降低动态学习速率。 optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)# Assuming optimizer uses lr = 0.5 for all groups# lr = 0.05 if epoch &lt; 30# lr = 0.005 if 30 &lt;= epoch &lt; 60# lr = 0.0005 if 60 &lt;= epoch &lt; 90# ...scheduler = StepLR(optimizer, step_size=30, gamma=0.1)for epoch in range(100): scheduler.step() train(...) validate(...) optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)# Assuming optimizer uses lr = 0.5 for all groups# lr = 0.05 if epoch &lt; 30# lr = 0.005 if 30 &lt;= epoch &lt; 80# lr = 0.0005 if epoch &gt;= 80scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)for epoch in range(100): scheduler.step() train(...) validate(...) optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)# mode (str) – min, max中的一个. 在最小模式下，当监测量停止下降时，lr将减少; 在最大模式下，当监控量停止增加时，会减少。默认值：'min'。# factor (float) – 使学习率降低的因素。 new_lr = lr * factor. 默认: 0.1.# patience (int) –epochs没有改善后，学习率将降低。 默认: 10.# verbose (bool) – 如果为True，则会向每个更新的stdout打印一条消息。 默认: False.# threshold (float) – 测量新的最优值的阈值，只关注显着变化。 默认: 1e-4.# threshold_mode (str) – rel, abs中的一个. 在rel模型, dynamic_threshold = best ( 1 + threshold ) in ‘max’ mode or best ( 1 - threshold ) 在最小模型. 在绝对值模型中, dynamic_threshold = best + threshold 在最大模式或最佳阈值最小模式. 默认: ‘rel’.# cooldown (int) – 在lr减少后恢复正常运行之前等待的时期数。默认的: 0.# min_lr (float or list) – 标量或标量的列表。对所有的组群或每组的学习速率的一个较低的限制。 默认: 0.# eps (float) – 适用于lr的最小衰减。如果新旧lr之间的差异小于eps，则更新将被忽略。默认: 1e-8.# 当指标停止改善时，降低学习率。当学习停滞不前时，模型往往会使学习速度降低2-10倍。这个调度程序读取一个指标量，如果没有提高epochs的数量，学习率就会降低。optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)scheduler = torch.optim.ReduceLROnPlateau(optimizer, 'min')for epoch in range(10): train(...) val_loss = validate(...) # Note that step should be called after validate() scheduler.step(val_loss) 参考https://blog.csdn.net/u010089444/article/details/76725843 https://ptorch.com/docs/1/optim#algorithms","path":"2019/09/16/优化器-介绍/","date":"09-16","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"RNN","text":"RNNRNN（Recurrent Neural Network）是一类用于处理序列数据的神经网络。 我们从基础的神经网络中知道，神经网络包含输入层、隐层、输出层，通过激活函数控制输出，层与层之间通过权值连接。激活函数是事先确定好的，那么神经网络模型通过训练“学“到的东西就蕴含在“权值“中。基础的神经网络只在层与层之间建立了权连接，RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。如图。 这是一个标准的RNN结构图，图中每个箭头代表做一次变换，也就是说箭头连接带有权值。左侧是折叠起来的样子，右侧是展开的样子，左侧中h旁边的箭头代表此结构中的“循环“体现在隐层。 在展开结构中我们可以观察到，在标准的RNN结构中，隐层的神经元之间也是带有权值的。也就是说，随着序列的不断推进，前面的隐层将会影响后面的隐层。图中O代表输出，y代表样本给出的确定值，L代表损失函数，我们可以看到，“损失“也是随着序列的推荐而不断积累的。 除上述特点之外，标准RNN的还有以下特点：1、权值共享，图中的W全是相同的，U和V也一样。2、每一个输入值都只与它本身的那条路线建立权连接，不会和别的神经元连接。 以上是RNN的标准结构，然而在实际中这一种结构并不能解决所有问题，例如我们输入为一串文字，输出为分类类别，那么输出就不需要一个序列，只需要单个输出。如图。 同样的，我们有时候还需要单输入但是输出为序列的情况。那么就可以使用如下结构： 还有一种结构是输入虽是序列，但不随着序列变化，就可以使用如下结构： 原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。下面我们来介绍RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。 从名字就能看出，这个结构的原理是先编码后解码。左侧的RNN用来编码得到c，拿到c后再用右侧的RNN进行解码。得到c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。 除了以上这些结构以外RNN还有很多种结构，用于应对不同的需求和解决不同的问题。 RNN的前向输出流程 来介绍一下各个符号的含义：x是输入，h是隐层单元，o为输出，L为损失函数，y为训练集的标签。这些元素右上角带的t代表t时刻的状态，其中需要注意的是，因策单元h在t时刻的表现不仅由此刻的输入决定，还受t时刻之前时刻的影响。V、W、U是权值，同一类型的权连接权值相同。 有了上面的理解，前向传播算法其实非常简单，对于t时刻： h^{t} = \\phi (Ux^{(t)} + Wh^{(t-1)} + b)其中ϕ()为激活函数，一般来说会选择tanh函数，b为偏置。 t时刻的输出就更为简单： o{(t)} = Vh^{(t)} + c最终模型的预测输出为： \\hat y^{(t)} = \\sigma(o^{(t)})其中σ 为激活函数，通常RNN用于分类，故这里一般用softmax函数。 RNN的训练方法BPTT（back-propagation through time）算法是常用的训练RNN的方法，其实本质还是BP算法，只不过RNN处理时间序列数据，所以要基于时间反向传播，故叫随时间反向传播。BPTT的中心思想和BP算法相同，沿着需要优化的参数的负梯度方向不断寻找更优的点直至收敛。综上所述，BPTT算法本质还是BP算法，BP算法本质还是梯度下降法，那么求各个参数的梯度便成了此算法的核心。 再次拿出这个结构图观察，需要寻优的参数有三个，分别是U、V、W。与BP算法不同的是，其中W和U两个参数的寻优过程需要追溯之前的历史数据，参数V相对简单只需关注目前，那么我们就来先求解参数V的偏导数。 \\frac {\\partial L^{(t)}} {\\partial V} = \\frac {\\partial L^{(t)}} {\\partial o^{(t)}} \\cdot \\frac {\\partial o^{(t)}} {\\partial V}这个式子看起来简单但是求解起来很容易出错，因为其中嵌套着激活函数函数，是复合函数的求道过程。 RNN的损失也是会随着时间累加的，所以不能只求t时刻的偏导。 L = \\sum_{t=1}^n L^{(t)} \\frac {\\partial L} {\\partial V} = \\sum_{t=1}^n \\frac {\\partial L^{(t)}} {\\partial o^{(t)}} \\cdot \\frac {\\partial o^{(t)}} {\\partial V}W和U的偏导的求解由于需要涉及到历史数据，其偏导求起来相对复杂，我们先假设只有三个时刻，那么在第三个时刻 L对W的偏导数为： \\frac {\\partial L^{(3)}} {\\partial W} = \\frac {\\partial L^{(3)}} {\\partial o^{(3)}} \\cdot \\frac {\\partial o^{(3)}} {\\partial h^{(3)}} \\cdot \\frac {\\partial h^{(3)}} {\\partial W} + \\frac {\\partial L^{(3)}} {\\partial o^{(3)}} \\cdot \\frac {\\partial o^{(3)}} {\\partial h^{(3)}} \\cdot \\frac {\\partial h^{(3)}} {\\partial h^{(2)}} \\cdot \\frac {\\partial h^{(2)}} {\\partial W} + \\frac {\\partial L^{(3)}} {\\partial o^{(3)}} \\cdot \\frac {\\partial o^{(3)}} {\\partial h^{(3)}} \\cdot \\frac {\\partial h^{(3)}} {\\partial h^{(2)}} \\cdot \\frac {\\partial h^{(2)}} {\\partial h^{(1)}} \\cdot \\frac {\\partial h^{(1)}} {\\partial W}相应的，L在第三个时刻对U的偏导数为： \\frac {\\partial L^{(3)}} {\\partial U} = \\frac {\\partial L^{(3)}} {\\partial o^{(3)}} \\cdot \\frac {\\partial o^{(3)}} {\\partial h^{(3)}} \\cdot \\frac {\\partial h^{(3)}} {\\partial U} + \\frac {\\partial L^{(3)}} {\\partial o^{(3)}} \\cdot \\frac {\\partial o^{(3)}} {\\partial h^{(3)}} \\cdot \\frac {\\partial h^{(3)}} {\\partial h^{(2)}} \\cdot \\frac {\\partial h^{(2)}} {\\partial U} + \\frac {\\partial L^{(3)}} {\\partial o^{(3)}} \\cdot \\frac {\\partial o^{(3)}} {\\partial h^{(3)}} \\cdot \\frac {\\partial h^{(3)}} {\\partial h^{(2)}} \\cdot \\frac {\\partial h^{(2)}} {\\partial h^{(1)}} \\cdot \\frac {\\partial h^{(1)}} {\\partial U}可以观察到，在某个时刻的对W或是U的偏导数，需要追溯这个时刻之前所有时刻的信息，这还仅仅是一个时刻的偏导数，上面说过损失也是会累加的，那么整个损失函数对W和U的偏导数将会非常繁琐。虽然如此但好在规律还是有迹可循，我们根据上面两个式子可以写出L在t时刻对W和U偏导数的通式： \\frac {\\partial L^{(t)}} {\\partial W} =\\sum_{k=0}^t \\frac {\\partial L^{(t)}} {\\partial o^{(t)}} \\cdot \\frac {\\partial o^{(t)}} {\\partial h^{(t)}} \\cdot (\\prod_{j=k+1}^t \\frac {\\partial h^{(j)}} {\\partial h^{(j-1)}}) \\cdot \\frac {\\partial h^{(3)}} {\\partial W} \\\\ \\frac {\\partial L^{(t)}} {\\partial U} =\\sum_{k=0}^t \\frac {\\partial L^{(t)}} {\\partial o^{(t)}} \\cdot \\frac {\\partial o^{(t)}} {\\partial h^{(t)}} \\cdot (\\prod_{j=k+1}^t \\frac {\\partial h^{(j)}} {\\partial h^{(j-1)}}) \\cdot \\frac {\\partial h^{(3)}} {\\partial U}整体的偏导公式就是将其按时刻再一一加起来。 前面说过激活函数是嵌套在里面的，如果我们把激活函数放进去，拿出中间累乘的那部分： \\prod_{j=k+1}^t \\frac {\\partial h^j} {\\partial h^{j-1}} = \\prod_{j=k+1}^t tanh^{'} \\cdot W_s \\\\ \\prod_{j=k+1}^t \\frac {\\partial h^j} {\\partial h^{j-1}} = \\prod_{j=k+1}^t sigmoid^{'} \\cdot W_s我们会发现累乘会导致激活函数导数的累乘，进而会导致“梯度消失“和“梯度爆炸“现象的发生。 至于为什么，我们先来看看这两个激活函数的图像。 它们二者是何其的相似，都把输出压缩在了一个范围之内。他们的导数图像也非常相近，我们可以从中观察到，sigmoid函数的导数范围是(0,0.25]，tach函数的导数范围是(0,1]，他们的导数最大都不大于1。 这就会导致一个问题，在上面式子累乘的过程中，如果取sigmoid函数作为激活函数的话，那么必然是一堆小数在做乘法，结果就是越乘越小。随着时间序列的不断深入，小数的累乘就会导致梯度越来越小直到接近于0，这就是“梯度消失“现象。其实RNN的时间序列与深层神经网络很像，在较为深层的神经网络中使用sigmoid函数做激活函数也会导致反向传播时梯度消失，梯度消失就意味消失那一层的参数再也不更新，那么那一层隐层就变成了单纯的映射层，毫无意义了，所以在深层神经网络中，有时候多加神经元数量可能会比多家深度好。 你可能会提出异议，RNN明明与深层神经网络不同，RNN的参数都是共享的，而且某时刻的梯度是此时刻和之前时刻的累加，即使传不到最深处那浅层也是有梯度的。这当然是对的，但如果我们根据有限层的梯度来更新更多层的共享的参数一定会出现问题的，因为将有限的信息来作为寻优根据必定不会找到所有信息的最优解。之前说过我们多用tanh函数作为激活函数，那tanh函数的导数最大也才1啊，而且又不可能所有值都取到1，那相当于还是一堆小数在累乘，还是会出现“梯度消失“，那为什么还要用它做激活函数呢？原因是tanh函数相对于sigmoid函数来说梯度较大，收敛速度更快且引起梯度消失更慢。 还有一个原因是sigmoid函数还有一个缺点，Sigmoid函数输出不是零中心对称。sigmoid的输出均大于0，这就使得输出不是0均值，称为偏移现象，这将导致后一层的神经元将上一层输出的非0均值的信号作为输入。关于原点对称的输入和中心对称的输出，网络会收敛地更好。 RNN的特点本来就是能“追根溯源“利用历史数据，现在告诉我可利用的历史数据竟然是有限的，这就令人非常难受，解决“梯度消失“是非常必要的。解决“梯度消失“的方法主要有：1、选取更好的激活函数2、改变传播结构 关于第一点，一般选用ReLU函数作为激活函数，ReLU函数的图像为： ReLU函数的左侧导数为0，右侧导数恒为1，这就避免了“梯度消失“的发生。但恒为1的导数容易导致“梯度爆炸“，但设定合适的阈值可以解决这个问题。还有一点就是如果左侧横为0的导数有可能导致把神经元学死，不过设置合适的步长（学习旅）也可以有效避免这个问题的发生。 关于第二点，LSTM结构可以解决这个问题。 总结一下，sigmoid函数的缺点：1、导数值范围为(0,0.25]，反向传播时会导致“梯度消失“。tanh函数导数值范围更大，相对好一点。2、sigmoid函数不是0中心对称，tanh函数是，可以使网络收敛的更好。 pytorch中的RNNfrom torch import nn# input_size, hidden_size, num_layers# num_layers为RNN的堆叠层数，由上一层每个时间节点的输出作为下一层每个时间节点的输入rnn = nn.RNN(10, 20, 2)# input of shape (seq_len, batch, input_size)input = torch.randn(5, 3, 10)# h_0 of shape (num_layers * num_directions, batch, hidden_size)h0 = torch.randn(2, 3, 20)output, hn = rnn(input, h0) LSTM下面来了解一下LSTM（long short-term memory）。长短期记忆网络是RNN的一种变体，RNN由于梯度消失的原因只能有短期记忆，LSTM网络通过精妙的门控制将短期记忆与长期记忆结合起来，并且一定程度上解决了梯度消失的问题。 长期依赖（Long-Term Dependencies）问题RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。 有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。. 但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。 不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。 在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。 然而，幸运的是，LSTM 并没有这个问题 LSTM 网络Long Short Term 网络—— 一般就叫做 LSTM ——是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 由Hochreiter &amp; Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！ 所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。 LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，整体上除了h在随时间流动，细胞状态c也在随时间流动，细胞状态c就代表着长期记忆。 不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。 黄色的矩形是学习得到的神经网络层 粉色的圆形表示一些运算操作，诸如加法乘法 黑色的单箭头表示向量的传输 两个箭头合成一个表示向量的连接 一个箭头分开表示向量的复制 LSTM 的核心思想LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。 细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。 LSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。 Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！ LSTM 拥有三个门，来保护和控制细胞状态。 逐步理解 LSTM在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为遗忘门完成。该门会读取ht-1和xt，输出一个在 0 到 1 之间的数值给每个在细胞状态Ct−1中的数字。1 表示“完全保留”，0 表示“完全舍弃”。 让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。 这里可以抛出两个问题：这个门怎么做到“遗忘“的呢？怎么理解？既然是遗忘旧的内容，为什么这个门还要接收新的xt? 对于第一个问题，“遗忘“可以理解为“之前的内容记住多少“，其精髓在于只能输出（0，1）小数的sigmoid函数和粉色圆圈的乘法，LSTM网络经过学习决定让网络记住以前百分之多少的内容。对于第二个问题就更好理解，决定记住什么遗忘什么，其中新的输入肯定要产生影响。 下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，Ct，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。 现在是更新旧细胞状态的时间了，Ct−1更新为 Ct。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。 我们把旧状态与f_t 相乘，丢弃掉我们确定需要丢弃的信息。接着加上相乘，丢弃掉我们确定需要丢弃的信息。接着加上 i_t Ct。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。有了上面的理解基础输入门，输入门理解起来就简单多了。*sigmoid函数选择更新内容，tanh函数创建更新候选。 最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。 这三个门虽然功能上不同，但在执行任务的操作上是相同的。他们都是使用sigmoid函数作为选择工具，tanh函数作为变换工具，这两个函数结合起来实现三个门的功能。 pytorch中的LSTMfrom torch import nn# input_size, hidden_size, num_layers# num_layers为RNN的堆叠层数，由上一层每个时间节点的输出作为下一层每个时间节点的输入rnn = nn.LSTM(10, 20, 2)# input of shape (seq_len, batch, input_size)input = torch.randn(5, 3, 10)# h_0 and c0 of shape (num_layers * num_directions, batch, hidden_size)h0 = torch.randn(2, 3, 20)c0 = torch.randn(2, 3, 20)output, (hn, cn) = rnn(input, (h0, c0)) Gated Recurrent Unit (GRU)它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。 pytorch中的GRUfrom torch import nn# input_size, hidden_size, num_layers# num_layers为RNN的堆叠层数，由上一层每个时间节点的输出作为下一层每个时间节点的输入rnn = nn.GRU(10, 20, 2)# input of shape (seq_len, batch, input_size)input = torch.randn(5, 3, 10)# h_0 of shape (num_layers * num_directions, batch, hidden_size)h0 = torch.randn(2, 3, 20)output, hn = rnn(input, h0) 参考https://blog.csdn.net/zhaojc1995/article/details/80572098","path":"2019/09/15/RNN/","date":"09-15","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"Dropout","text":"Dropout在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。 过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。 综上所述，训练深度神经网络的时候，总是会遇到两大缺点： （1）容易过拟合 （2）费时 Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。 Dropout可以作为训练深度神经网络的一种trick供选择。在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用，检测器相互作用是指某些检测器依赖其他检测器才能发挥作用。 Dropout说的简单一点就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。 Dropout为什么可以解决过拟合 取平均的作用： 先回到标准的模型即没有dropout，我们用相同的训练数据去训练5个不同的神经网络，一般会得到5个不同的结果，此时我们可以采用 “5个结果取均值”或者“多数取胜的投票策略”去决定最终结果。例如3个网络判断结果为数字9,那么很有可能真正的结果就是数字9，其它两个网络给出了错误结果。这种“综合起来取平均”的策略通常可以有效防止过拟合问题。因为不同的网络可能产生不同的过拟合，取平均则有可能让一些“相反的”拟合互相抵消。dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。 减少神经元之间复杂的共适应关系： 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，这些特征在其它的神经元的随机子集中也存在。换句话说假如我们的神经网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。从这个角度看dropout就有点像L1，L2正则，减少权重使得网络对丢失特定神经元连接的鲁棒性提高。 Dropout类似于性别在生物进化中的角色：物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。 pytorch中的Dropoutfrom torch import nnm = nn.Dropout(p=0.2)input = torch.randn(20, 16)output = m(input) from torch import nnm = nn.Dropout2d(p=0.2)input = torch.randn(20, 16, 32, 32)output = m(input)","path":"2019/09/14/Dropout/","date":"09-14","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"归一化层(Normalization layers)","text":"归一化层(Normalization layers)归一化层，目前主要有这几个方法，Batch Normalization（2015年）、Layer Normalization（2016年）、Instance Normalization（2017年）、Group Normalization（2018年）、Switchable Normalization（2018年）； 将输入的图像shape记为[N, C, H, W]，这几个方法主要的区别就是在： batchNorm是在batch上，对NHW做归一化，对小batchsize效果不好；layerNorm在通道方向上，对CHW归一化，主要对RNN作用明显；instanceNorm在图像像素上，对HW做归一化，用在风格化迁移；GroupNorm将channel分组，然后再做归一化；SwitchableNorm是将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。 Batch Noirmalization首先，在进行训练之前，一般要对数据做归一化，使其分布一致，但是在深度神经网络训练过程中，通常以送入网络的每一个batch训练，这样每个batch具有不同的分布；此外，为了解决internal covarivate shift问题，这个问题定义是随着batch normalizaiton这篇论文提出的，在训练过程中，数据分布会发生变化，对下一层网络的学习带来困难。 所以batch normalization就是强行将数据拉回到均值为0，方差为1的正太分布上，这样不仅数据分布一致，而且避免发生梯度消失。 此外，internal corvariate shift和covariate shift是两回事，前者是网络内部，后者是针对输入数据，比如我们在训练数据前做归一化等预处理操作。 \\mu_B = \\frac 1 m \\sum_{i=1}^m x_i \\\\ \\sigma_B^2 = \\frac 1 m \\sum_{i=1}^m (x_i - \\mu_B)^2 \\\\ \\hat {x_i} = \\frac {x_i - \\mu_B} {\\sqrt {\\sigma_B^2 + \\epsilon}} \\\\ y_i = \\gamma \\hat x_i + \\beta \\equiv BN_{\\gamma , \\beta}(x_i)加入缩放平移变量的原因是：保证每一次数据经过归一化后还保留原有学习来的特征，同时又能完成归一化操作，加速训练。 这两个参数是用来学习的参数。 注：BatchNorm2d的情况则是在每个通道C上计算H、W和N的均值和方差，即每个通道上的N张图像所有像素计算均值和方差。 from torch import nn# With Learnable Parametersm = nn.BatchNorm2d(100)# Without Learnable Parametersm = nn.BatchNorm2d(100, affine=False)input = torch.randn(20, 100, 35, 45)output = m(input) Layer Normalizationbatch normalization存在以下缺点： 对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布； BN实际使用时需要计算并且保存某一层神经网络batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；但对于RNN来说，sequence的长度是不一致的，换句话说RNN的深度不是固定的，不同的time-step需要保存不同的statics特征，可能存在一个特殊sequence比其他sequence长很多，这样training时，计算很麻烦。 与BN不同，LN是针对深度网络的某一层的所有神经元的输入按以下公式进行normalize操作。 BN与LN的区别在于： LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差； BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。 所以，LN不依赖于batch的大小和输入sequence的深度，因此可以用于batchsize为1和RNN中对边长的输入sequence的normalize操作。LN用于RNN效果比较明显，但是在CNN上，不如BN。 from torch import nninput = torch.randn(20, 5, 10, 10)# With Learnable Parametersm = nn.LayerNorm(input.size()[1:])# Without Learnable Parametersm = nn.LayerNorm(input.size()[1:], elementwise_affine=False)# Normalize over last two dimensionsm = nn.LayerNorm([10, 10])# Normalize over last dimension of size 10m = nn.LayerNorm(10)# Activating the moduleoutput = m(input) Instance NormalizationBN注重对每个batch进行归一化，保证数据分布一致，因为判别模型中结果取决于数据整体分布。 但是图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。 和BatchNorm的区别： 公式： 代码： from torch import nn# Without Learnable Parametersm = nn.InstanceNorm2d(100)# With Learnable Parametersm = nn.InstanceNorm2d(100, affine=True)input = torch.randn(20, 100, 35, 45)output = m(input) Group Normalization主要是针对Batch Normalization对小batchsize效果差，GN将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值，这样与batchsize无关，不受其约束。 from torch import nninput = torch.randn(20, 6, 10, 10)# Separate 6 channels into 3 groupsm = nn.GroupNorm(3, 6)# Separate 6 channels into 6 groups (equivalent with InstanceNorm)m = nn.GroupNorm(6, 6)# Put all 6 channels into a single group (equivalent with LayerNorm)m = nn.GroupNorm(1, 6)# Activating the moduleoutput = m(input) Normalization layer的作用 没有它之前，需要小心的调整学习率和权重初始化，但是有了BN可以放心的使用大学习率，但是使用了BN，就不用小心的调参了，较大的学习率极大的提高了学习速度； Batchnorm本身上也是一种正则的方式，可以代替其他正则方式如dropout等； 另外，个人认为，batchnorm降低了数据之间的绝对差异，有一个去相关的性质，更多的考虑相对差异性，因此在分类任务上具有更好的效果。 BatchNorm为什么NB呢，关键还是效果好。不仅仅极大提升了训练速度，收敛过程大大加快，还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果。另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。 参考https://blog.csdn.net/liuxiao214/article/details/81037416","path":"2019/09/14/归一化层(Normalization-layers)/","date":"09-14","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"随机初始化","text":"随机初始化在神经网络中，如果初始化所有的参数（也就是权重）相同，那么所有输入都相同，神经网络就失去了它的作用了。所以我们需要随机初始化。 a = Wx + b若我们随机初始化所有参数为0，则正向传播时，所有的a也会都为0。 在反向传播时，必定有一项为a，所以求出来的梯度都为0或者都一样。 就导致每一层中的权重都会做相同的更新。这样的话就没有意义了。 我们需要牢记参数初始化的目的是为了让神经网络在训练过程中学习到有用的信息，这意味着参数梯度不应该为0。而我们知道在全连接的神经网络中，参数梯度和反向传播得到的状态梯度以及入激活值有关——激活值饱和会导致该层状态梯度信息为0，然后导致下面所有层的参数梯度为0；入激活值为0会导致对应参数梯度为0。所以如果要保证参数梯度不等于0，那么参数初始化应该使得各层激活值不会出现饱和现象且激活值不为0。我们把这两个条件总结为参数初始化条件： 初始化必要条件一：各层激活值不会出现饱和现象。初始化必要条件二：各层激活值不为0。 torch.nn.init.calculate_gain(nonlinearity, param=None) nonlinearlity - 非线性函数名 param - 非线性函数的可选参数 import torch.nn as nngain = nn.init.calculate_gain('leaky_relu')&gt;&gt;&gt; 1.414... torch.nn.init 初始化函数import torchimport torch.nn as nnw = torch.empty(2, 3)# 1. 均匀分布 - u(a,b)# torch.nn.init.uniform_(tensor, a=0, b=1)nn.init.uniform_(w)# tensor([[ 0.0578, 0.3402, 0.5034],# [ 0.7865, 0.7280, 0.6269]])# 2. 正态分布 - N(mean, std)# torch.nn.init.normal_(tensor, mean=0, std=1)nn.init.normal_(w)# tensor([[ 0.3326, 0.0171, -0.6745],# [ 0.1669, 0.1747, 0.0472]])# 3. 常数 - 固定值 val# torch.nn.init.constant_(tensor, val)nn.init.constant_(w, 0.3)# tensor([[ 0.3000, 0.3000, 0.3000],# [ 0.3000, 0.3000, 0.3000]])# 4. 对角线为 1，其它为 0# torch.nn.init.eye_(tensor)nn.init.eye_(w)# tensor([[ 1., 0., 0.],# [ 0., 1., 0.]])# 5. Dirac delta 函数初始化，仅适用于 &#123;3, 4, 5&#125;-维的 torch.Tensor# torch.nn.init.dirac_(tensor)w1 = torch.empty(3, 16, 5, 5)nn.init.dirac_(w1)# 6. xavier_uniform 初始化# torch.nn.init.xavier_uniform_(tensor, gain=1)# From - Understanding the difficulty of training deep feedforward neural networks - Bengio 2010nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))# tensor([[ 1.3374, 0.7932, -0.0891],# [-1.3363, -0.0206, -0.9346]])# 7. xavier_normal 初始化# torch.nn.init.xavier_normal_(tensor, gain=1)nn.init.xavier_normal_(w)# tensor([[-0.1777, 0.6740, 0.1139],# [ 0.3018, -0.2443, 0.6824]])# 8. kaiming_uniform 初始化# From - Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - HeKaiming 2015# torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')# tensor([[ 0.6426, -0.9582, -1.1783],# [-0.0515, -0.4975, 1.3237]])# 9. kaiming_normal 初始化# torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu')nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')# tensor([[ 0.2530, -0.4382, 1.5995],# [ 0.0544, 1.6392, -2.0752]])# 10. 正交矩阵 - (semi)orthogonal matrix# From - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe 2013# torch.nn.init.orthogonal_(tensor, gain=1)nn.init.orthogonal_(w)# tensor([[ 0.5786, -0.5642, -0.5890],# [-0.7517, -0.0886, -0.6536]])# 11. 稀疏矩阵 - sparse matrix # 非零元素采用正态分布 N(0, 0.01) 初始化.# From - Deep learning via Hessian-free optimization - Martens 2010# torch.nn.init.sparse_(tensor, sparsity, std=0.01)nn.init.sparse_(w, sparsity=0.1)# tensor(1.00000e-03 *# [[-0.3382, 1.9501, -1.7761],# [ 0.0000, 0.0000, 0.0000]]) Xavier 初始化因为Xavier的推导过程是基于几个假设的，其中一个是激活函数是线性的。这并不适用于ReLU激活函数。另一个是激活值关于0对称，这个不适用于sigmoid函数和ReLU函数。所以可以看到图11中并没有对sogmoid网络应用Xavier初始化。 Xavier初始化针对于激活函数为Tanh()的时候效果比较好。 Kaiming 初始化 (He 初始化)条件：正向传播时，状态值的方差保持不变；反向传播时，关于激活值的梯度的方差保持不变。 Kaiming初始化适用于ReLU()和LeakyReLU()。 pytorch 手动初始化from torch import nnfrom torch.nn import init# 上面的语句是对网络的某一层参数进行初始化self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)init.xavier_uniform(self.conv1.weight)init.constant(self.conv1.bias, 0.1)# 对整个网络的参数进行初始化# def weights_init(m): # classname=m.__class__.__name__ # if classname.find('Conv') != -1: # xavier(m.weight.data) # xavier(m.bias.data) def weights_init(m): if isinstance(m, nn.Conv2d): xavier(m.weight.data) xavier(m.bias.data) net = Net()net.apply(weights_init) # apply函数会递归地搜索网络内的所有module并把参数表示的函数应用到所有的module上。","path":"2019/09/13/随机初始化/","date":"09-13","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"激活函数介绍","text":"激活函数介绍如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。 Sigmoid 函数Sigmoid 是常用的非线性的激活函数，它的数学形式如下： f(z) = \\frac {1} {1 + e^{-z}} 特点：它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.缺点：sigmoid函数曾经被使用的很多，不过近年来，用它的人越来越少了。主要是因为它固有的一些 缺点。缺点1：在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。首先来看Sigmoid函数的导数，如下图所示： 如果我们初始化神经网络的权值为 [0,1]之间的 值，由反向传播算法的数学推导可知，梯度从后向前传播时，每传递一层梯度值都会减小为原来的0.25倍，如果神经网络隐层特别多，那么梯度在穿过多层后将变得非常小接近于0，即出现梯度消失现象；当网络权值初始化为 (1,+∞)区间内的值，则会出现梯度爆炸情况。 缺点2：Sigmoid 的 output 不是0均值（即zero-centered）。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。 当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。缺点3：其解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间。 from torch import nnm = nn.Sigmoid()input = torch.randn(2)output = m(input) ReLU函数ReLU函数的解析式： Relu = max(0, x) ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：1） 解决了gradient vanishing问题 (在正区间)2）计算速度非常快，只需要判断输入是否大于03）收敛速度远快于sigmoid和tanh ReLU也有几个需要特别注意的问题：1）ReLU的输出不是zero-centered2）Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier He化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。 尽管存在这两个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！ from torch import nnm = nn.ReLU()input = torch.randn(2)output = m(input) Leaky ReLU (PReLU)函数表达式： f(x) = max(ax, x) 人们为了解决Dead ReLU Problem，提出了将ReLU的前半段设为αx而非0，通常α=0.01。可由方向传播算法学出来。理论上来讲，Leaky ReLU有ReLU的所有优点，外加不会有Dead ReLU问题，但是在实际操作当中，并没有完全证明Leaky ReLU总是好于ReLU。 from torch import nnm = nn.PReLU(num_parameters=1, init=0.25)# num_parameters：需要学习的a的个数，默认等于1# init：a的初始值，默认等于0.25input = torch.randn(2)output = m(input) ELU(Exponential Linear Units) 函数函数表达式： f(x) = \\begin{cases} x & \\text {if } x>0\\\\ \\alpha(e^x-1) & \\text {otherwise } \\end{cases} ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及： 不会有Dead ReLU问题 输出的均值接近0，zero-centered 它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU，但在实际使用中目前并没有好的证据ELU总是优于ReLU。 from torch import nnm = nn.ELU(alpha=1.0, inplace=False)input = torch.randn(2)output = m(input) MaxOut函数Maxout是深度学习网络中的一层网络，就像池化层、卷积层一样等，我们可以把maxout 看成是网络的激活函数层，我们假设网络某一层的输入特征向量为：X=（x1,x2,……xd），也就是我们输入是d个神经元。Maxout隐藏层每个神经元的计算公式如下： h_i(x) = \\max_{j \\in [1,k]} \\text { }z_{ij}上面的公式就是maxout隐藏层神经元i的计算公式。其中，k就是maxout层所需要的参数了，由我们人为设定大小。就像dropout一样，也有自己的参数p(每个神经元dropout概率)，maxout的参数是k。公式中Z的计算公式为： z_{ij} = x^TW_{...ij} + b_{ij}为了简单起见，假设我们网络第i层有2个神经元x1、x2，第i+1层的神经元个数为1个，如下图所示： (1)以前MLP的方法。我们要计算第i+1层，那个神经元的激活值的时候，传统的MLP计算公式就是： z = WX+b \\\\ out = f(z)其中f就是我们所谓的激活函数，比如Sigmod、Relu、Tanh等。 (2)Maxout 的方法。如果我们设置maxout的参数k=5，maxout层就如下所示： 相当于在每个输出神经元前面又多了一层。这一层有5个神经元，此时maxout网络的输出计算公式为： z1=w1*x+b1 \\\\ z2=w2*x+b2 \\\\ z3=w3*x+b3 \\\\ z4=w4*x+b4 \\\\ z5=w5*x+b5 \\\\ out=max(z1,z2,z3,z4,z5)所以这就是为什么采用maxout的时候，参数个数成k倍增加的原因。本来我们只需要一组参数就够了，采用maxout后，就需要有k组参数。 Tanh 函数函数表达式： Tanh(x) = tanh(x) = \\frac {e^x - e^{-x}} {e^x + e^{-x}} 优点： 函数输出以（0,0）为中学 收敛速度相对于Sigmoid更快 缺点： tanh并没有解决sigmoid梯度消失的问题 from torch import nnm = nn.Tanh()input = torch.randn(2)output = m(input) Softmax 函数softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类！ 假设我们有一个数组，X，xi表示X中的第i个元素，那么这个元素的softmax值就是 S_i = \\frac {e^{x_i}} {\\sum_j e^{x_j}} from torch import nnm = nn.Softmax(dim=1)# dim（int） - 计算Softmax的维度（因此沿着dim的每个切片将总和为1）。input = torch.randn(2, 3)output = m(input) LogSoftmax 函数torch.nn.LogSoftmax(dim=None) LogSoftmax(x_i) = log(\\frac {e^{x_i}} {\\sum_j e^{x_j}})from torch import nnm = nn.LogSoftmax()# Input: (∗) where * means, any number of additional dimensionsinput = torch.randn(2, 3)# Output: (∗) , same shape as the inputoutput = m(input) 如何选择正确的激活函数根据问题的性质，我们可以为神经网络更快更方便地收敛作出更好的选择。 用于分类器时，Sigmoid函数及其组合通常效果更好。 由于梯度消失问题，有时要避免使用sigmoid和tanh函数。 ReLU函数是一个通用的激活函数，目前在大多数情况下使用。 如果神经网络中出现死神经元，那么PReLU函数就是最好的选择。 请记住，ReLU函数只能在隐藏层中使用。 参考https://blog.csdn.net/tyhj_sf/article/details/79932893","path":"2019/09/12/激活函数介绍/","date":"09-12","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"转置卷积 上采样","text":"转置卷积 上采样转置卷积也就是逆卷积 一句话解释：逆卷积相对于卷积在神经网络结构的正向和反向传播中做相反的运算 逆卷积(Deconvolution)比较容易引起误会，转置卷积(Transposed Convolution)是一个更为合适的叫法 输入矩阵可展开为16维向量，记作输出矩阵可展开为4维向量，记作卷积运算可表示为 反向传播时为 所谓逆卷积其实就是正向时左乘，而反向时左乘，即的运算。 转置卷积 上采样 公式推导Input: (N, C_{in}, H_{in}, W_{in})Output: (N, C_{out}, H_{out}, W_{out})转置卷积公式： H_{out} = (H_{in} - 1) * Stride - 2Padding + kernal\\_size转置卷积 上采样 在pytorch中的用法from torch import nnimport torchupsample = nn.ConvTranspose2d(in_channels=3, out_channels=3, kernal_size=3, stride=1, padding=0, output_padding=0, groups=1, bias=True)# in_channels、out_channels为当前输入通道数和输出通道数# 注意，这上面的stride、padding是争对于与原始卷积上的stride和padding# out_padding为最后输出时的填充，即做完所有操作之后的输出最后再做out_paddinginput = torch.randn(1, 3, 2, 2)output = upsample(input)print(output.size()) from torch import nnimport torchmaxpool = nn.MaxPool2d((2, 2), stride=(2, 2),return_indices=True)maxunpool = nn.MaxUnpool2d(kernel_size=(2,2),stride=(2,2),padding=0)input =torch.Tensor(np.arange(64).reshape((8,8)).reshape(1,1,8,8)) x, indices = maxpool(input)output1 = maxunpool(x, indices)output2 = nn.functional.interpolate(x, scale_factor=2, mode=\"bilinear\")# torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)# Down/up samples the input to either the given size or the given scale_factor# The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.# The modes available for resizing are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only), area# input (Tensor) – the input tensor# size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) – output spatial size.# scale_factor (float or Tuple[float]) – multiplier for spatial size. Has to match input size if it is a tuple.# mode (str) – algorithm used for upsampling: 'nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear' | 'area'. Default: 'nearest' 最邻近、线性、双线性、双三次、三线性、区域插值就是图像的插值方法 用得最多的是最邻近插值、双线性插值、双三次插值","path":"2019/09/10/转置卷积-上采样/","date":"09-10","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"卷积 池化","text":"卷积 池化卷积 池化 公式推导一般情况下, 输入的图片矩阵以及后面的卷积核, 特征图矩阵都是方阵, 这里设输入矩阵大小为 w, 卷积核大小为 k, 步幅为 s, 补零层数为 p, 则卷积后产生的特征图大小计算公式为: w' = \\frac {w + 2p -k} {s} + 1卷积 池化 在pytorch中的用法import torchimport torch.nn as nnimport numpy as npx = torch.Tensor([1,10,100,1000,10000,100000]).view(1,2,-1,1,1)# view()函数用来reshape,-1参数意为自动设置,此处计算得6# Conv2d的规定输入数据格式为(batch, channel, Height, Width)# Conv3d的规定输入数据格式为(batch, channel, Depth, Height, Width)conv = nn.Conv3d(in_channels=2, out_channels=6, kernel_size=(2,1,1), stride=1, padding=0, dilation=1, groups=1, bias=False)# 参数group的作用为：将输入数据按通道顺序分组, 每组有in_channel/group个通道.(例:group为2时，输入数据前一半通道为一组).同时, 每组对应的kernel个数, 从原来的out_channel变为outchannel/group.此处的kernel为三维及以上结构,而filter特指二维层状的过滤器。原来的情况中, 每个生成的特征图都有所有通道的贡献.而现在, 特征图仅由其所在group对应的通道卷积构成.# 简而言之, group参数的目的就是将原本的大卷积分成多个并联(side by side)的小卷积# 另: 在in_channel不变的情况下, 当group&gt;1时, kernel总数不变, 而filter总数缩小group倍.而在filter、kernel总数不变的情况下, group增大, 需要的in_channel按同样比例增大.# 参数dilation的作用为： 控制卷积核元素的间隔大小.具体可搜索“空洞卷积”output=conv(x)print('output=',output.data)print('outputsize=',output.data.size())# output.data.size()的返回值：# (batch, out_channels/ or num_of_featurecube, size_of_featurecube) import torchimport torch.nn as nnimport numpy as npx = torch.Tensor([1,10,100,1000,10000,100000]).view(1,2,-1,1,1)# view()函数用来reshape,-1参数意为自动设置,此处计算得6# Conv2d的规定输入数据格式为(batch, channel, Height, Width)# Conv3d的规定输入数据格式为(batch, channel, Depth, Height, Width)# 如果输入的大小是(N,C,D,H,W)，那么输出的大小是(N,C,D_out,H_out,W_out)maxpool = nn.MaxPool3d(kernal_size=2, stride=2, padding=0, dilation=1, retuirn_indices=False, ceil_mode=False)# return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助# ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作# 如果输入的大小是(N,C,D,H,W)，那么输出的大小是(N,C,D_out,H_out,W_out)avgpool = nn.AvgPool3d(kernal_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)# count_include_pad - 如果等于True，计算平均池化时，将包括padding填充的0","path":"2019/09/09/卷积-池化/","date":"09-09","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"数据预处理","text":"数据预处理中心化(零均值)中心化就是零均值化，对于每一个元素减去本图像的平均值即可。 E（X-E（X））=0这样做的意义在于，对于某些激活函数，比如sigmoid，relu，tanh而言，激活函数单调递增，其任意一点导数均大于零。 f(\\sum_{i} w_i x_i + b)而f关于wi的偏导数为xi，如果xi均为正数（或者负数），那么 \\frac {\\partial L_i} {\\partial w_i} = \\frac {\\partial L_i} {\\partial f} \\frac {\\partial f} {\\partial w_i} = \\frac {\\partial L_i} {\\partial f} x_i其正负等同于xi的正负，也就是必然是正数（或者零）。 那么如果想要使得loss函数减小，朝着 \\frac {\\partial L_i} {\\partial w_i}的方向运动的话，就会出现只能朝着每一个wi的正方向或者负方向运动的情况。如果有n个wi的向量，则有2^n个象限，除非最优化wi就在全为正的第一象限，否则优化本身必然比较曲折。 会反复向正方向运行收敛。如果此时纵坐标可以向负方向运动，则可以直接到达最优化点。 所以我们进行零均值化，当x正负数量“差不多”时，那么梯度的变化方向就会不确定，这样就能达到上图中的变化效果，加速了权重的收敛。 归一化归一化是指将原始数据通过线性变化转换为范围在[0, 1]或[-1, 1]之间的数。 范围在[0,1 ]变换公式如下： x = \\frac {x−min} {max−min}其中，min为最小值，max为最大值。 范围在[-1, 1]之间的变换公式如下： x = \\frac {x-\\mu} {max-min}其中，μ为最均值，max为最大值。 标准化标准化也叫Z-Score标准化，是指将原始数据转化为均值为0，标准差为1的数据集，经过标准化处理的数据符合标准的正态分布，变换公式如下： x = \\frac {x-\\mu} {\\sigma}其中μ为数据集的平均值，δ为数据集的标准差。 标准化和归一化的目的归一化/标准化可以去除数据单位对计算带来的影响，也就是所谓的去量纲行为，归一化/标准化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是归一化/标准化的前提。 归一化/标准化的去量纲作用能够带来以下两个好处： 提升模型的精度。一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。 提高收敛速度。对于线性模型来说，数据归一化/标准化后，最优解的寻优过程明显会变得平缓，更容易正确的收敛到最优解。如下图所示： 图像数据扩增若增加训练数据，则能够提升算法的准确率，因为这样可以避免过拟合，更好地泛化；而避免了过拟合你就可以增大你的网络结构了。可以大量使用数据增广。 几何变换 包括：弹性变换（Elastic Transform）、透视变换（Perspective Transform）、分段仿射变换（Piecewise Affine transforms）、枕形畸变（Pincushion Distortion）。 随机改变大小（resize）,随机缩放、旋转、翻转 从原始图像（256,256）中，随机的crop出一些图像（224,224） 不做随机crop，大型网络基本都过拟合(under substantial overfitting)。 水平/竖直翻转，flip。mirror，即水平翻转图像 Rotation变换/旋转变换 加噪声 对主成分做一个(0, 0.1)的高斯扰动。 torchvision 中的数据扩增方法torchvision.transforms中的数据扩增方法是针对于PIL图片 CenterCrop(size)Crops the given PIL Image at the center. size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)Randomly change the brightness, contrast and saturation of an image. 随机改变图像的亮度、对比度、饱和度、色调 FiveCrop(size)Crop the given PIL Image into four corners and the central crop size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop of size (size, size) is made. NOTE: This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this. Grayscale(num_output_channels=1)Convert image to grayscale. num_output_channels (int) – (1 or 3) number of channels desired for output image Pad(padding, fill=0, padding_mode=’constant’)Pad the given PIL Image on all sides with the given “pad” value. padding (int or tuple) – Padding on each border. If a single int is provided this is used to pad all borders. If tuple of length 2 is provided this is the padding on left/right and top/bottom respectively. If a tuple of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. fill (int or tuple) – Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant padding_mode (str) – Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. constant: pads with a constant value, this value is specified with fill edge: pads with the last value at the edge of the image reflect: pads with reflection of image without repeating the last value on the edge For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2] symmetric: pads with reflection of image repeating the last value on the edge For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] RandomAffine(egrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0)Random affine transformation of the image keeping center invariant RandomApply(transforms,p=0.5)Apply randomly a list of transformations with a given probability transforms (list or tuple) – list of transformations p (float) – probability RandomChoise(transforms)Apply single transformation randomly picked from a list RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=’constant’)Crop the given PIL Image at a random location. size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. padding (int or sequence**, optional) – Optional padding on each border of the image. Default is None, i.e no padding. If a sequence of length 4 is provided, it is used to pad left, top, right, bottom borders respectively. If a sequence of length 2 is provided, it is used to pad left/right, top/bottom borders, respectively. pad_if_needed (boolean) – It will pad the image if smaller than the desired size to avoid raising an exception. Since cropping is done after padding, the padding seems to be done at a random offset. fill – Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant padding_mode – Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant. constant: pads with a constant value, this value is specified with fill edge: pads with the last value on the edge of the image reflect: pads with reflection of image (without repeating the last value on the edge) padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2] symmetric: pads with reflection of image (repeating the last value on the edge) padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3] RandomGrayscale(p=0.1)Randomly convert image to grayscale with a probability of p (default 0.1). p (float) – probability that image should be converted to grayscale. RandomHorizontalFlip(p=0.5)Horizontally flip the given PIL Image randomly with a given probability. p (float) – probability of the image being flipped. Default value is 0.5 RandomOrder(transforms)Apply a list of transformations in a random order RamdomPerspective(distortion_scale=0.5, p=0.5, interpolation=3)Performs Perspective transformation of the given PIL Image randomly with a given probability. 透视变换 interpolation – Default- Image.BICUBIC p (float) – probability of the image being perspectively transformed. Default value is 0.5 distortion_scale (float) – it controls the degree of distortion and ranges from 0 to 1. Default value is 0.5. RandomSizeCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2)Crop the given PIL Image to random size and aspect ratio. A crop of random size (default: of 0.08 to 1.0) of the original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop is finally resized to given size. This is popularly used to train the Inception networks. size – expected output size of each edge scale – range of size of the origin size cropped ratio – range of aspect ratio of the origin aspect ratio cropped interpolation – Default: PIL.Image.BILINEAR RandomRotation(degrees, resample=False, expand=False, center=None)Rotate the image by angle. degrees (sequence or float or int) – Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). resample ({PIL.Image.NEAREST**, PIL.Image.BILINEAR**, PIL.Image.BICUBIC},optional) – An optional resampling filter. See filters for more information. If omitted, or if the image has mode “1” or “P”, it is set to PIL.Image.NEAREST. expand (bool, optional) – Optional expansion flag. If true, expands the output to make it large enough to hold the entire rotated image. If false or omitted, make the output image the same size as the input image. Note that the expand flag assumes rotation around the center and no translation. center (2-tuple**, optional) – Optional center of rotation. Origin is the upper left corner. Default is the center of the image. RandomVerticalFilp(p=0.5)Vertically flip the given PIL Image randomly with a given probability. p (float) – probability of the image being flipped. Default value is 0.5 Resize(size, interpolation=2)Resize the input PIL Image to the given size. size (sequence or int) – Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size) interpolation (int, optional) – Desired interpolation. Default isPIL.Image.BILINEAR TenCrop(size, vertical_flip=False)Crop the given PIL Image into four corners and the central crop plus the flipped version of these (horizontal flipping is used by default) size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. vertical_flip (bool) – Use vertical flipping instead of horizontal Normalize(mean, std, inplace=False)这是针对于Tensor的函数 Normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input torch.*Tensor i.e. input[channel] = (input[channel] - mean[channel]) / std[channel] mean (sequence) – Sequence of means for each channel. std (sequence) – Sequence of standard deviations for each channel. inplace (bool,**optional) – Bool to make this operation in-place. ToPILImage(mode=None)将Tensor格式图片或者numpy格式图片转化为PIL格式图片 Convert a tensor or an ndarray to PIL Image. Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape H x W x C to a PIL Image while preserving the value range. mode (PIL.Image mode) – color space and pixel depth of input data (optional). If mode is None (default) there are some assumptions made about the input data: If the input has 4 channels, the mode is assumed to be RGBA. If the input has 3 channels, the mode is assumed to be RGB. If the input has 2 channels, the mode is assumed to be LA. If the input has 1 channel, the mode is determined by the data type (i.e int, float, short). Totensor()将PILImage或者numpy图片转化为tensor Convert a PIL Image or numpy.ndarray to tensor. Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8 In the other cases, tensors are returned without scaling.","path":"2019/09/06/数据预处理/","date":"09-06","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"数据集划分","text":"如何划分训练集,验证集,测试集训练集,验证集,测试集之间的区别 训练集（train set） —— 用于模型拟合的数据样本。 验证集（development set）—— 是模型训练过程中单独留出的样本集，它可以用于调整模型的超参数和用于对模型的能力进行初步评估。 在神经网络中， 我们用验证数据集去寻找最优的网络深度（number of hidden layers)，或者决定反向传播算法的停止点或者在神经网络中选择隐藏层神经元的数量 在普通的机器学习中常用的交叉验证（Cross Validation) 就是把训练数据集本身再细分成不同的验证数据集去训练模型 测试集 —— 用来评估模最终模型的泛化能力。但不能作为调参、选择特征等算法相关的选择的依据。 类别 验证集 测试集 是否用来训练 否 否 作用 用于调超参数，监控模型是否发生过拟合（以决定是否停止训练） 为了评估最终模型泛化能力 使用次数 多次使用，以不断调参 仅仅一次使用 缺陷 模型在一次次重新手动调参并继续训练后所逼近的验证集，可能只代表一部分非训练集，导致最终训练好的模型泛化性能不够 测试集为了具有泛化代表性，往往数据量比较大，测试一轮要很久，所以往往只取测试集的其中一小部分作为训练过程中的验证集 为什么要测试集 训练集直接参与了模型调参的过程，显然不能用来反映模型真实的能力（防止课本死记硬背的学生拥有最好的成绩，即防止过拟合)。 验证集参与了人工调参(超参数)的过程，也不能用来最终评判一个模型（刷题库的学生不能算是学习好的学生）。 所以要通过最终的考试(测试集)来考察一个学(模)生(型)真正的能力（期末考试）。 但是仅凭一次考试就对模型的好坏进行评判显然是不合理的，所以接下来就要介绍交叉验证法 交叉验证法交叉验证法的作用就是尝试利用不同的训练集/验证集划分来对模型做多组不同的训练/验证，来应对单独测试结果过于片面以及训练数据不足的问题。（就像通过多次考试，才通知哪些学生是比较比较牛B的） 交叉验证的做法就是将数据集粗略地分为比较均等不相交的k份 D = D_1 \\cup D_2 \\cup \\cdots \\cup D_k D_i \\neq D_j (i \\neq j)然后取其中的一份进行测试，另外的k-1份进行训练，然后求得error的平均值作为最终的评价 举个例子：假设建立一个BP神经网络，对于隐含层的节点数目，我们并没有很好的方法去确定。此时，一般将节点数设定为某一具体的值，通过训练集训练出相应的参数后，再由交叉验证集去检测该模型的误差； 然后再改变节点数，重复上述过程，直到交叉验证误差最小。 交叉验证算法的具体步骤如下： 随机将训练数据等分成k份，S1, S2, …, Sk。 对于每一个模型Mi，算法执行k次，每次选择一个Sj作为验证集，而其它作为训练集来训练模型Mi，把训练得到的模型在Sj上进行测试，这样一来，每次都会得到一个误差E，最后对k次得到的误差求平均，就可以得到模型Mi的泛化误差。 算法选择具有最小泛化误差的模型作为最终模型，并且在整个训练集上再次训练该模型，从而得到最终的模型。 K值的选择: K值的选取是一个偏差与方差的权衡： K=1时，所有数据用于训练，容易过拟合； K=N时，相当于留一法LOOCV (Leave-one-out cross-validation ). 通常建议K=10 2017年的一项研究给出了另一种经验式的选择方法，作者建议 K \\approx log(n)且保证 n / K > 3d合理划分比例过去，人们运用机器学习传统方法的时候，一般将训练集和测试集划为7：3 若有验证集，则划为6:2:2. 这样划分确实很科学，当数据量不大的时候（万级别及以下） 但到了大数据时代，数据量陡增为百万级别，此时我们不需要那么多的验证集和训练集 假设有100W条数据，只需要拿出1W条来当验证集，1W条来当测试集，就能很好地work了 因此，在深度学习中若是数据很大，我们可以将训练集、验证集、测试集比例调整为98：1：1 参考https://blog.csdn.net/kieven2008/article/details/81582591","path":"2019/09/04/数据集划分/","date":"09-04","excerpt":"","tags":[{"name":"学习深度学习","slug":"学习深度学习","permalink":"https://litianbo243.github.io/tags/学习深度学习/"},{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"}]},{"title":"braTS2019 数据集介绍","text":"braTS2019数据集介绍braTS简介braTS是一个针对MRI脑肿瘤进行分割的数据集,每年都有很多人在braTS上进行各种state-of-art方法的试验 braTS利用MRI扫描,从外观,形状和组织学上对脑肿瘤(神经胶质瘤)进行分割 为了研究分割任务对临床实验的影响,braTS也会致力于研究预测病人总体的生存率 同时,braTS也会对分割的不确定性进行实验研究 braTS任务Task 1: 对MRI脑肿瘤图像进行分割考虑评估的子区域是: 增强肿瘤(enhancing tumor)(ET) 肿瘤核心(tumor core)(TC) 肿瘤整体(whole tumor)(WT) 提供的分段标签的值为NCR和NET为1，ED为2，ET为4，其他所有值为0。 Task 2: 从MRI脑肿瘤图像中预测患者总生存周期注意，将对参与者评估具有GTR切除状态的受试者的预测存活状态（即，总切除总数）。 Task 3: 分割中不确定性的量化这项新任务的重点是探索神经胶质瘤区域分割背景下的不确定性测量，目的是通过以下结果对参与方法进行奖励：（a）在正确时有信心，（b）在不正确时不确定。愿意参与该新任务的参与者上传（除了他们的任务1的分割结果之外）3生成的与每个体素处的所得标签相关联的不确定性图。 参与者被要求上传4个nifti（.nii.gz）卷（3个不确定性图和1个多级分段卷，来自任务1）到CBICA的图像处理门户格式。 例如，对于数据集中的每个ID，参与者需要上传以下4个卷： {ID} .nii.gz（多级标签图） {ID} _unc_whole.nii.gz（与整个肿瘤相关的不确定性图） {ID} _unc_core.nii.gz（与肿瘤核心相关的不确定性图） {ID} _unc_enhance.nii.gz（与增强肿瘤相关的不确定性图） 数据作为今年的训练，验证和测试数据，提供了充分的多机构常规临床获得的胶质母细胞瘤（GBM / HGG）和低级别胶质瘤（LGG）术前多模式MRI扫描，病理确诊和可用OS。 BraTS挑战。具体而言，今年挑战中使用的数据集已经更新，自BraTS’18以来，通过临床获得的更多常规3T多模式MRI扫描，并由专家委员会认证的神经放射学家提供相应的地面真实标签。 所有BraTS多模式扫描均可作为NIfTI文件（.nii.gz）获得，并描述a）原生（T1）和b）对比后T1加权（T1Gd），c）T2加权（T2）和d）T2液体衰减的逆转恢复（T2-FLAIR）卷，并使用不同的临床协议和来自多个（n = 19）机构的各种扫描仪获得，这里被称为数据贡献者。 数据使用协议/引文您可以自由使用和/或参考您自己研究中的BraTS数据集，前提是您总是引用以下三个手稿： [1] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, et al. “The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)”, IEEE Transactions on Medical Imaging 34(10), 1993-2024 (2015) DOI: 10.1109/TMI.2014.2377694 [2] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, et al., “Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features”, Nature Scientific Data, 4:170117 (2017) DOI: 10.1038/sdata.2017.117 [3] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, et al., “Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge”, arXiv preprint arXiv:1811.02629 (2018) In addition, if there are no restrictions imposed from the journal/conference you submit your paper about citing “Data Citations”, please be specific and also cite the following: [4] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., “Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-GBM collection”, The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.KLXWJJ1Q [5] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., “Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-LGG collection”, The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.GJQ7R0EF 评估对于分段任务，并且为了与先前BraTS挑战的配置保持一致，我们将使用“Dice得分”和“Hausdorff距离（95％）”。 扩展此评估方案，自BraTS’17以来，我们还使用“敏感性”和“特异性”的度量，允许通过参与方法确定肿瘤亚区域的潜在过度分割或分割不足。 由于BraTS’12-‘13是BraTS’19测试数据的子集，我们还将计算’12 -13数据的性能，以便与BraTS TMI参考文件中报告的性能进行比较。","path":"2019/09/01/braTS2019-数据集简介/","date":"09-01","excerpt":"","tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"},{"name":"医疗影像分析","slug":"医疗影像分析","permalink":"https://litianbo243.github.io/tags/医疗影像分析/"}]},{"title":"使用 SimpleITK 和 Nibabel 读取医学nii数据","text":"使用 SimpleITK 和 Nibabel 读取医学nii数据SimpleITK 和 Nibabel 的区别： SimpleITK 加载数据是channel_first，即（155，240，240）； Nibabel 是 channel_last，即（240，240，155），其中155是图像通道数，也就是155张图像，可以把nii看成二维图像，也可以看成三维。 SimpleITKimport SimpleITK as sitkimport numpy as npfrom PIL import Image def read_img(path): img = sitk.ReadImage(path) data = sitk.GetArrayFromImage(img) return data# 显示一个系列图def show_imgs(data): for i in range(data.shape[2]): img = Image.fromarray(data[:, :, i]) img.show()# 单张显示def show_img(ori_img): img = Image.fromarray(ori_img[:, :, 80]) img.show()path = '/home/ltb/braTS/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_2_1/BraTS19_2013_2_1_flair.nii.gz'data = read_img(path)data = np.array(data)data = np.transpose(data, (1, 2, 0))show_imgs(data) Nibabelimport nibabel as nibfrom PIL import Imageimport numpy as npdef read_data(path): image_data = nib.load(path).get_data() return image_data# 显示一个系列图def show_imgs(data): for i in range(data.shape[2]): img = Image.fromarray(data[:, :, i]) img.show()# 单张显示def show_img(ori_img): img = Image.fromarray(ori_img[:, :, 80]) img.show()path = '/home/ltb/braTS/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_2013_2_1/BraTS19_2013_2_1_flair.nii.gz'data = read_data(path)print(data.shape)data = np.array(data)show_imgs(data) 参考https://blog.csdn.net/weixin_42338058/article/details/84190420","path":"2019/08/31/使用SimpleITK-和-Nibabel-读取医学nii数据/","date":"08-31","excerpt":"","tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"},{"name":"医疗影像分析","slug":"医疗影像分析","permalink":"https://litianbo243.github.io/tags/医疗影像分析/"}]},{"title":"小样本学习(Few-shot Learning)","text":"小样本学习(Few-shot Learning)问题定义人类非常擅长通过极少量的样本识别一个新物体，比如小孩子只需要书中的一些图片就可以认识什么是“斑马”，什么是“犀牛”。在人类的快速学习能力的启发下，研究人员希望机器学习模型在学习了一定类别的大量数据后，对于新的类别，只需要少量的样本就能快速学习，这就是Few-shot Learning 要解决的问题。 Few-shot Learning 是 Meta Learning 在监督学习领域的应用。Meta Learning，又称为 learningto learn，在 meta training 阶段将数据集分解为不同的 meta task，去学习类别变化的情况下模型的泛化能力，在 meta testing 阶段，面对全新的类别，不需要变动已有的模型，就可以完成分类。 形式化来说，few-shot 的训练集中包含了很多的类别，每个类别中有多个样本。在训练阶段，会在训练集中随机抽取 C 个类别，每个类别 K 个样本（总共CK 个数据），构建一个 meta-task，作为模型的支撑集（support set）输入；再从这 C个类中剩余的数据中抽取一批（batch）样本作为模型的预测对象（batch set）。即要求模型从 CK 个数据中学会如何区分这 C 个类别，这样的任务被称为 *C-way K-shot 问题。 训练过程中，每次训练（episode）都会采样得到不同 meta-task，所以总体来看，训练包含了不同的类别组合，这种机制使得模型学会不同 meta-task 中的共性部分，比如如何提取重要特征及比较样本相似等，忘掉 meta-task 中 task相关部分。通过这种学习机制学到的模型，在面对新的未见过的 meta-task 时，也能较好地进行分类。 在图像领域的研究现状早期的 Few-shot Learning 算法研究多集中在图像领域，如图所示，Few-shot Learning模型大致可分为三类：Mode Based，Metric Based 和 Optimization Based。 其中 Model Based 方法旨在通过模型结构的设计快速在少量样本上更新参数，直接建立输入 x 和预测值 P 的映射函数； Metric Based方法通过度量 batch 集中的样本和 support 集中样本的距离，借助最近邻的思想完成分类； Optimization Based方法认为普通的梯度下降方法难以在 few-shot 场景下拟合，因此通过调整优化方法来完成小样本分类的任务。 Model Base 方法Santoro 等人 [3] 提出使用记忆增强的方法来解决 Few-shot Learning 任务。基于记忆的神经网络方法早在 2001年被证明可以用于 meta-learning。他们通过权重更新来调节 bias，并且通过学习将表达快速缓存到记忆中来调节输出。 然而，利用循环神经网络的内部记忆单元无法扩展到需要对大量新信息进行编码的新任务上。因此，需要让存储在记忆中的表达既要稳定又要是元素粒度访问的，前者是说当需要时就能可靠地访问，后者是说可选择性地访问相关的信息；另外，参数数量不能被内存的大小束缚。神经图灵机（NTMs）和记忆网络就符合这种必要条件。 文章基于神经网络图灵机（NTMs）的思想，因为 NTMs 能通过外部存储（externalmemory）进行短时记忆，并能通过缓慢权值更新来进行长时记忆，NTMs可以学习将表达存入记忆的策略，并如何用这些表达来进行预测。由此，文章方法可以快速准确地预测那些只出现过一次的数据。 文章基于 LSTM 等 RNN 的模型，将数据看成序列来训练，在测试时输入新的类的样本进行分类。 具体地，在 t时刻，模型输入，也就是在当前时刻预测输入样本的类别，并在下一时刻给出真实的label，并且添加了 external memory 存储上一次的 x 输入，这使得下一次输入后进行反向传播时，可以让 y (label) 和 x建立联系，使得之后的 x 能够通过外部记忆获取相关图像进行比对来实现更好的预测。 Meta NetworkMeta Network的快速泛化能力源自其“快速权重”的机制，在训练过程中产生的梯度被用来作为快速权重的生成。模型包含一个meta learner 和一个 base learner，meta learner 用于学习 meta task 之间的泛化信息，并使用 memory机制保存这种信息，base learner 用于快速适应新的 task，并和 meta learner 交互产生预测输出。 Metric Based如果在 Few-shot Learning 的任务中去训练普通的基于 cross-entropy的神经网络分类器，那么几乎肯定是会过拟合，因为神经网络分类器中有数以万计的参数需要优化。 相反，很多非参数化的方法（最近邻、K-近邻、Kmeans）是不需要优化参数的，因此可以在 meta-learning 的框架下构造一种可以端到端训练的few-shot 分类器。该方法是对样本间距离分布进行建模，使得同类样本靠近，异类样本远离。下面介绍相关的方法。 孪生网络（Siamese Network） 通过有监督的方式训练孪生网络来学习，然后重用网络所提取的特征进行 one/few-shot 学习。 具体的网络是一个双路的神经网络，训练时，通过组合的方式构造不同的成对样本，输入网络进行训练，在最上层通过样本对的距离判断他们是否属于同一个类，并产生对应的概率分布。在预测阶段，孪生网络处理测试样本和支撑集之间每一个样本对，最终预测结果为支撑集上概率最高的类别。 相比孪生网络，匹配网络（Match Network）为支撑集和 Batch 集构建不同的编码器，最终分类器的输出是支撑集样本和query 之间预测值的加权求和。 该文章也是在不改变网络模型的前提下能对未知类别生成标签，其主要创新体现在建模过程和训练过程上。对于建模过程的创新，文章提出了基于memory 和 attention 的 matching nets，使得可以快速学习。","path":"2019/08/31/小样本学习(Few-shot-Learning)/","date":"08-31","excerpt":"","tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"},{"name":"Few-shot Learning","slug":"Few-shot-Learning","permalink":"https://litianbo243.github.io/tags/Few-shot-Learning/"}]},{"title":"vim使用指南","text":"vim使用指南移动光标可以使用方向键,也可以使用hjkl四个按键 h← j↓ k↑ l→ 模式 退出vim 保存修改并退出 :wq 直接退出 :q! :q 删除命令在插入模式使用退格进行删除 在普通模式下输入x进行删除 按键 含义 d0 删除光标从当前位置（不包含）到该行行首的所有字符 d$ 删除从光标当前位置（包含）到该行行尾的所有字符 dd 删除光标所在行的所有字符 dgg 删除光标所在行（包含）到文件开头的所有字符 dG 删除光标所在行（包含）到文件末尾的所有字符 撤销和恢复命令 按键 含义 u 撤销最后一次修改 U 撤销对整行的修改 Ctrl + r 恢复撤销的内容 剪切命令使用 p 命令可以将最后一次删除的内容粘贴到光标之后。（大写的 P 则是粘贴到光标之前） 拷贝命令语法跟删除的 d 命令一样：y motion 拷贝完成之后同样使用 p 命令进行粘贴 替换命令VIM 还提供了一个简单的替换命令：r 命令。r 用于替换光标所在的字符，做法是先将光标移动到需要替换的字符处，按一下 r 键，然后输入新的字符。注意，全程无需进入插入模式，也不会进入插入模式。 替换模式: ​ 对于需要替换多个字符，更好的方案是直接进入替换模式。按下大写的 R 键，屏幕左下角出现 — REPLACE — 字样，说明你已经处于替换模式。此时输入字符可以连续替换光标及其后边的内容。注意：退格键（Backspace）在替换模式中被解释为如果左边内容被替换过，则恢复到原来的样子；如果没有被替换过，则简单的向左移动。修改完毕后，按下 Esc 回到普通模式。 文件信息快捷键 ctrl + g 可以解决你的需求 跳转当你的光标在文件中随意徘徊时，突然想到目标就在第 333 行的位置，你应该怎么做？假设你当前光标位于文件第 1333 行，你的做法是在普通模式下按 1000 次 k 键？ 在 VIM 有两种方式可以将光标跳转到指定的位置： 行号 + G :行号 比如将光标跳转到第 333 行的位置，你就输入数字 333，再输入大写字母 G 即可见证奇迹；或者输入冒号（:）进入命令行模式，再输入数字 333，最后回车，也可以跳转到目的地。 定位括号VIM 有个按键可以帮你快速定位到另一半括号，别说我没告诉你，就是 % 键。将光标移动到 ()，[]，{}，中的任何一半括号上，按下 % 键，便可看到此时光标已经跳转到另外一半的括号上了。 缩进你可以按一下 v 进入可视模式（左下角出现 — VISUAL — 字样），然后通过 h、j、k、l 或 其他 motion 来移动你的光标，此时光标所到之处必被一道亮光所包围（表示被选中），选择好需要缩进的目标后，只需按一下 &gt; 即可完成任务。 搜索在普通模式下按下斜杠（/）也是进入命令行模式，此时该字符和光标均出现在屏幕的底部，这跟冒号（:）一样。 紧挨着斜杠（/）的是搜索目标，比如 /love，说明你找的是 love 这个字符串在光标后边第一次出现的位置，当然你也可以输入中文，比如 /你瞅啥 那如果要找下一个目标怎么办？这时你只需按 n 键即可定位到下一个符合的目标（向下查找），而按 N 键则返回上一个（向上查找）。 注意：第一个搜索到的目标不是文件中的第一个目标，而是从你的光标所在处开始找到的那个目标。所以你如果想要搜索文件中第一个匹配的目标，你应该先 gg 将光标移动到文件头，然后再使用搜索命令。 替换通过搜索功能，我们将光标定位到目标位置，如果你确定这个目标是可恶的，需要被替换的，你可以输入 :s/old/new，这样即可将光标所在行的第一个 old 替换为 new；你如果输入的是 :s/old/new/g，则表示将光标所在行的所有 old 替换为 new。 但如果要替换整个文件的所有匹配字符串怎么办？总不能每一行来一下吧？只要你能想到的，VIM 就有办法！输入 :%s/old/new/g表示替换整个文件中每个匹配的字符串。 执行shell命令在输入冒号（:）进入命令行模式，输入感叹号（!），在其后便可以加上 shell 命令。此后 VIM 将临时跳转回 shell，并执行命令。再次按下 Enter 键回到 VIM。 文件另存为一般的文本编辑工具都会有“另存为”的功能，用于将文件重新存放到一个新的文件中（旧文件保留不变）。VIM 也可以这么干，做法是输入 :w 新文件名","path":"2019/08/30/vim使用指南/","date":"08-30","excerpt":"","tags":[{"name":"linux","slug":"linux","permalink":"https://litianbo243.github.io/tags/linux/"},{"name":"vim","slug":"vim","permalink":"https://litianbo243.github.io/tags/vim/"}]},{"title":"python基础","text":"python基础listPython内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素。 比如，列出班里所有同学的名字，就可以用一个list表示： &gt;&gt;&gt; classmates = ['Michael', 'Bob', 'Tracy']&gt;&gt;&gt; classmates['Michael', 'Bob', 'Tracy'] tuple另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改，比如同样是列出同学的名字： &gt;&gt;&gt; classmates = ('Michael', 'Bob', 'Tracy') dictPython内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。 举个例子，假设要根据同学的名字查找对应的成绩，如果用list实现，需要两个list： 如果用dict实现，只需要一个“名字”-“成绩”的对照表，直接根据名字查找成绩，无论这个表有多大，查找速度都不会变慢。用Python写一个dict如下： &gt;&gt;&gt; d = &#123;'Michael': 95, 'Bob': 75, 'Tracy': 85&#125;&gt;&gt;&gt; d['Michael']95 setset和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。 要创建一个set，需要提供一个list作为输入集合： &gt;&gt;&gt; s = set([1, 1, 2, 2, 3, 3])&gt;&gt;&gt; s&#123;1, 2, 3&#125; 函数的参数 位置参数 def power(x, n): s = 1 while n &gt; 0: n = n - 1 s = s * x return s power(x, n)函数有两个参数：x和n，这两个参数都是位置参数，调用函数时，传入的两个值按照位置顺序依次赋给参数x和n。 默认参数 def power(x, n=2): s = 1 while n &gt; 0: n = n - 1 s = s * x return s 设置默认参数时，有几点要注意： 一是必选参数在前，默认参数在后，否则Python的解释器会报错（思考一下为什么默认参数不能放在必选参数前面）； 二是如何设置默认参数。 当函数有多个参数时，把变化大的参数放前面，变化小的参数放后面。变化小的参数就可以作为默认参数。 可变参数 def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum 定义可变参数和定义一个list或tuple参数相比，仅仅在参数前面加了一个*号。在函数内部，参数numbers接收到的是一个tuple，因此，函数代码完全不变。但是，调用该函数时，可以传入任意个参数，包括0个参数。 Python允许你在list或tuple前面加一个*号，把list或tuple的元素变成可变参数传进去： &gt;&gt;&gt; nums = [1, 2, 3]&gt;&gt;&gt; calc(*nums)14 *nums表示把nums这个list的所有元素作为可变参数传进去。这种写法相当有用，而且很常见。 关键字参数 而关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict。 def person(name, age, **kw): print('name:', name, 'age:', age, 'other:', kw) &gt;&gt;&gt; person('Bob', 35, city='Beijing')name: Bob age: 35 other: &#123;'city': 'Beijing'&#125;&gt;&gt;&gt; person('Adam', 45, gender='M', job='Engineer')name: Adam age: 45 other: &#123;'gender': 'M', 'job': 'Engineer'&#125; &gt;&gt;&gt; extra = &#123;'city': 'Beijing', 'job': 'Engineer'&#125;&gt;&gt;&gt; person('Jack', 24, **extra)name: Jack age: 24 other: &#123;'city': 'Beijing', 'job': 'Engineer'&#125; **extra表示把extra这个dict的所有key-value用关键字参数传入到函数的**kw参数，kw将获得一个dict，注意kw获得的dict是extra的一份拷贝，对kw的改动不会影响到函数外的extra。 命名关键字参数 如果要限制关键字参数的名字，就可以用命名关键字参数，例如，只接收city和job作为关键字参数。这种方式定义的函数如下： def person(name, age, *, city, job): print(name, age, city, job) 和关键字参数**kw不同，命名关键字参数需要一个特殊分隔符*，*后面的参数被视为命名关键字参数。 &gt;&gt;&gt; person('Jack', 24, city='Beijing', job='Engineer')Jack 24 Beijing Engineer 如果函数定义中已经有了一个可变参数，后面跟着的命名关键字参数就不再需要一个特殊分隔符*了。 参数组合在Python中定义函数，可以用必选参数、默认参数、可变参数、关键字参数和命名关键字参数，这5种参数都可以组合使用。但是请注意，参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数。 def f1(a, b, c=0, *args, **kw): print('a =', a, 'b =', b, 'c =', c, 'args =', args, 'kw =', kw)def f2(a, b, c=0, *, d, **kw): print('a =', a, 'b =', b, 'c =', c, 'd =', d, 'kw =', kw) 最神奇的是通过一个tuple和dict，你也可以调用上述函数： &gt;&gt;&gt; args = (1, 2, 3, 4)&gt;&gt;&gt; kw = &#123;'d': 99, 'x': '#'&#125;&gt;&gt;&gt; f1(*args, **kw)a = 1 b = 2 c = 3 args = (4,) kw = &#123;'d': 99, 'x': '#'&#125;&gt;&gt;&gt; args = (1, 2, 3)&gt;&gt;&gt; kw = &#123;'d': 88, 'x': '#'&#125;&gt;&gt;&gt; f2(*args, **kw)a = 1 b = 2 c = 3 d = 88 kw = &#123;'x': '#'&#125; 所以，对于任意函数，都可以通过类似func(*args, **kw)的形式调用它，无论它的参数是如何定义的。 迭代默认情况下，dict迭代的是key。如果要迭代value，可以用for value in d.values()，如果要同时迭代key和value，可以用for k, v in d.items()。 Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身： &gt;&gt;&gt; for i, value in enumerate(['A', 'B', 'C']):... print(i, value)...0 A1 B2 C 列表生成式&gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 运用列表生成式，可以写出非常简洁的代码。例如，列出当前目录下的所有文件和目录名，可以通过一行代码实现： &gt;&gt;&gt; import os # 导入os模块，模块的概念后面讲到&gt;&gt;&gt; [d for d in os.listdir('.')] # os.listdir可以列出文件和目录['.emacs.d', '.ssh', '.Trash', 'Adlm', 'Applications', 'Desktop', 'Documents', 'Downloads', 'Library', 'Movies', 'Music', 'Pictures', 'Public', 'VirtualBox VMs', 'Workspace', 'XCode'] 生成器通过列表生成式，我们可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。 所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator。 要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： &gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 创建L和g的区别仅在于最外层的[]和()，L是一个list，而g是一个generator。 所以，我们创建了一个generator后，基本上永远不会调用next()，而是通过for循环来迭代它，并且不需要关心StopIteration的错误。 要把fib函数变成generator，只需要把print(b)改为yield b就可以了： def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a + b n = n + 1 return 'done' 这里，最难理解的就是generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 闭包 def lazy_sum(*args): def sum(): ax = 0 for n in args: ax = ax + n return ax return sum &gt;&gt;&gt; f = lazy_sum(1, 3, 5, 7, 9)&gt;&gt;&gt; f&lt;function lazy_sum.&lt;locals&gt;.sum at 0x101c6ed90&gt;&gt;&gt;&gt; f()25 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量。 lambda函数匿名函数lambda x: x * x实际上就是： def f(x): return x * x 关键字lambda表示匿名函数，冒号前面的x表示函数参数。 装饰器现在，假设我们要增强now()函数的功能，比如，在函数调用前后自动打印日志，但又不希望修改now()函数的定义，这种在代码运行期间动态增加功能的方式，称之为“装饰器”（Decorator）。 本质上，decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator，可以定义如下： def log(func): def wrapper(*args, **kw): print('call %s():' % func.__name__) return func(*args, **kw) return wrapper 观察上面的log，因为它是一个decorator，所以接受一个函数作为参数，并返回一个函数。我们要借助Python的@语法，把decorator置于函数的定义处： @logdef now(): print('2015-3-25') 调用now()函数，不仅会运行now()函数本身，还会在运行now()函数前打印一行日志： &gt;&gt;&gt; now()call now():2015-3-25 wrapper()函数的参数定义是(*args, **kw)，因此，wrapper()函数可以接受任意参数的调用。在wrapper()函数内，首先打印日志，再紧接着调用原始函数。 错误处理try: print('try...') r = 10 / 0 print('result:', r)except ZeroDivisionError as e: print('except:', e)finally: print('finally...')print('END') 当我们认为某些代码可能会出错时，就可以用try来运行这段代码，如果执行出错，则后续代码不会继续执行，而是直接跳转至错误处理代码，即except语句块，执行完except后，如果有finally语句块，则执行finally语句块，至此，执行完毕。 由于没有错误发生，所以except语句块不会被执行，但是finally如果有，则一定会被执行（可以没有finally语句）。","path":"2019/08/29/python基础/","date":"08-29","excerpt":"","tags":[{"name":"python","slug":"python","permalink":"https://litianbo243.github.io/tags/python/"},{"name":"基础","slug":"基础","permalink":"https://litianbo243.github.io/tags/基础/"}]},{"title":"dockerfile构建指南","text":"dockerfile构建指南dockerfile官方文档https://docs.docker.com/engine/reference/builder/ dockerfile的基本结构Dockerfile 一般分为四部分： 基础镜像信息 维护者信息 镜像操作指令 容器启动时执行指令 ’#’ 为 Dockerfile 中的注释 FROM指定基于哪个基础镜像,必须为第一个命令 格式： FROM &lt;image&gt; FROM &lt;image&gt;:&lt;tag&gt; FROM &lt;image&gt;@&lt;digest&gt;示例： FROM mysql:5.6注： tag或digest是可选的，如果不使用这两个值时，会使用latest版本的基础镜像 MAINTAINER维护者信息 格式： MAINTAINER &lt;name&gt;示例： MAINTAINER fendo fendo MAINTAINER fendo.com MAINTAINER fendo fendo &lt;fendo@163.com&gt; RUN构建镜像时执行的命令 RUN用于在镜像容器中执行命令，其有以下两种命令执行方式： 1.shell执行格式： RUN &lt;command&gt; 2.exec执行格式： RUN [\"executable\", \"param1\", \"param2\"]示例： RUN yum update RUN [\"/etc/execfile\", \"arg1\", \"arg1\"]注： RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定--no-cache参数，如：docker build --no-cache CMD构建容器后调用，也就是在容器启动时才进行调用 格式： CMD [\"executable\",\"param1\",\"param2\"] (执行可执行文件，优先) CMD [\"param1\",\"param2\"] (设置了ENTRYPOINT，则直接调用ENTRYPOINT添加参数) CMD command param1 param2 (执行shell内部命令)示例： CMD [\"/bin/bash\", \"/usr/local/nginx/sbin/nginx\", \"-c\", \"/usr/local/nginx/conf/nginx.conf\"]注： CMD不同于RUN,CMD用于指定在容器启动时所要执行的命令,而RUN用于指定镜像构建时所要执行的命令,只能有一条。 COPY复制文件到镜像中 使用copy命令时,必须将需要copy的文件放在和dockerfile同一目录下 格式: COPY &lt;src&gt; &lt;dest&gt;示例: COPY ltb-tutorial /home/ltb/tutorial ENTRYPOINT配置容器，容器启动时要执行的命令，它和CMD很像，也是只有一条生效 和CMD不同是：CMD 是可以被 docker run 指令覆盖的，而ENTRYPOINT不能覆盖 格式： ENTRYPOINT [\"executable\", \"param1\", \"param2\"] (可执行文件, 优先) ENTRYPOINT command param1 param2 (shell内部命令)示例： FROM ubuntu ENTRYPOINT [\"top\", \"-b\"] CMD [\"-c\"]注： ENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖ENTRYPOINT， 而docker run命令中指定的任何参数，都会被当做参数再次传递给ENTRYPOINT。 Dockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，而只执行最后的ENTRYPOINT指令。 LABEL用于为镜像添加元数据 格式： LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...示例： LABEL version=\"1.0\" description=\"这是描述\" by=\"fendo\"注： 使用LABEL指定元数据时，一条LABEL指定可以指定一或多条元数据， 指定多条元数据时不同元数据之间通过空格分隔。推荐将所有的元数据通过一条LABEL指令指定，以免生成过多的中间镜像。 EVN设置环境变量,它主要是为后续的RUN指令提供一个环境变量 格式： ENV &lt;key&gt; &lt;value&gt; ENV &lt;key&gt;=&lt;value&gt; ...示例： ENV myName fendo ENV myDog fendo Dog ENV myCat fendo EXPOSE格式： EXPOSE &lt;port&gt; [&lt;port&gt;...]示例： EXPOSE 80 443 EXPOSE 8080说明: 用来指定要映射出去的端口，比如容器内部我们启动了sshd和nginx，所以我们需要把22和80端口暴漏出去。这个需要配合-P（大写）来工作， 也就是说在启动容器时，需要加上-P，让它自动分配。如果想指定具体的端口，也可以使用-p（小写）来指定。注： EXPOSE并不会让容器的端口访问到主机。要使其可访问，需要在docker run运行容器时通过-p来发布这些端口，或通过-P参数来发布EXPOSE导出的所有端口 VOLUME用于指定持久化目录 格式： VOLUME [\"/path/to/dir\"]示例： VOLUME [\"/data\"] VOLUME [\"/var/www\", \"/var/log/apache2\", \"/etc/apache2\"]说明: 创建一个可以从本地主机或其他容器挂载的挂载点。注： 一个卷可以存在于一个或多个容器的指定目录，该目录可以绕过联合文件系统，并具有以下功能： 1 卷可以容器间共享和重用 2 容器并不一定要和其它容器共享卷 3 修改卷后会立即生效 4 对卷的修改不会对镜像产生影响 5 卷会一直存在，直到没有任何容器在使用它 WORKDIR工作目录,类似于cd命令 格式： WORKDIR /fendo/nginx示例： WORKDIR /a (这时工作目录为/a) WORKDIR b (这时工作目录为/a/b) WORKDIR c (这时工作目录为/a/b/c)说明: 为后续的RUN、CMD或者ENTRYPOINT指定工作目录注： 通过WORKDIR设置工作目录后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT、ADD、COPY等命令都会在该目录下执行。 在使用docker run运行容器时，可以通过-w参数覆盖构建时所设置的工作目录。 dockerfile实例# This my first nginx Dockerfile# Version 1.0# Base images 基础镜像FROM centos#MAINTAINER 维护者信息MAINTAINER tianfeiyu #ENV 设置环境变量ENV PATH /usr/local/nginx/sbin:$PATH#ADD 文件放在当前目录下，拷过去会自动解压ADD nginx-1.8.0.tar.gz /usr/local/ ADD epel-release-latest-7.noarch.rpm /usr/local/ #RUN 执行以下命令 RUN rpm -ivh /usr/local/epel-release-latest-7.noarch.rpmRUN yum install -y wget lftp gcc gcc-c++ make openssl-devel pcre-devel pcre &amp;&amp; yum clean allRUN useradd -s /sbin/nologin -M www#WORKDIR 相当于cdWORKDIR /usr/local/nginx-1.8.0 RUN ./configure --prefix=/usr/local/nginx --user=www --group=www --with-http_ssl_module --with-pcre &amp;&amp; make &amp;&amp; make installRUN echo \"daemon off;\" &gt;&gt; /etc/nginx.conf#EXPOSE 映射端口EXPOSE 80#CMD 运行以下命令CMD [\"nginx\"] dockerfile构建流程 新建文件夹DIR,进入并编写dockerfile文件 使用docker build命令构建docker镜像 docker build --network ltb_net -t leeskywave/nginx:v1.0 . # 不加--network参数,大概率构建的时候没网 参考https://blog.csdn.net/u011781521/article/details/80464065","path":"2019/08/10/dockerfile构建指南/","date":"08-10","excerpt":"","tags":[{"name":"docker","slug":"docker","permalink":"https://litianbo243.github.io/tags/docker/"},{"name":"虚拟化","slug":"虚拟化","permalink":"https://litianbo243.github.io/tags/虚拟化/"},{"name":"dockerfile","slug":"dockerfile","permalink":"https://litianbo243.github.io/tags/dockerfile/"}]},{"title":"docker 网络指南","text":"docker 网络指南Docker自身有4种网络工作方式，和一些自定义网络模式 安装Docker时，它会自动创建三个网络，bridge（创建容器默认连接到此网络）、 none 、host host：容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口 Container：创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围 None：该模式关闭了容器的网络功能 Bridge：此模式会为每一个容器分配、设置IP等，并将容器连接到一个docker0虚拟网桥，通过docker0网桥以及Iptables nat表配置与宿主机通信 以上都是不用动手的，真正需要配置的是自定义网络 默认网络使用docker network ls命令列出这些网络 docker network ls\"\"\"NETWORK ID NAME DRIVER SCOPE1e4b5374565f bridge bridge localf48367672056 host host locala2545c48ca42 none null local\"\"\" Docker内置这三个网络，运行容器时，你可以使用该—network标志来指定容器应连接到哪些网络 bridge网络代表docker0所有Docker安装中存在的网络。除非你使用该docker run —network=选项指定，否则Docker守护程序默认将容器连接到此网络 我们在使用docker run创建Docker容器时，可以用 —net 选项指定容器的网络模式，Docker可以有以下4种网络模式： host模式：使用 —net=host 指定 none模式：使用 —net=none 指定 bridge模式：使用 —net=bridge 指定，默认设置 container模式：使用 —net=container:NAME_or_ID 指定 Host相当于Vmware中的桥接模式，与宿主机在同一个网络中，但没有独立IP地址 容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口 Container这个模式指定新创建的容器和已经存在的一个容器共享一个Network Namespace，而不是和宿主机共享 新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等 同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的 两个容器的进程可以通过lo网卡设备通信 None实际上，该模式关闭了容器的网络功能 Bridge相当于Vmware中的Nat模式，容器使用独立network Namespace，并连接到docker0虚拟网卡 通过docker0网桥以及Iptables nat表配置与宿主机通信 bridge模式是Docker默认的网络设置，此模式会为每一个容器分配Network Namespace、设置IP等，并将一个主机上的Docker容器连接到一个虚拟网桥上 自定义网络建议使用自定义的网桥来控制哪些容器可以相互通信，还可以自动DNS解析容器名称到IP地址 Docker提供了创建这些网络的默认网络驱动程序，你可以创建一个新的Bridge网络 docker network create --driver bridge ltb_test_net 创建网络后，可以看到新增加了一个网桥br-762457e516bc 可以通过docker network inspect查看网络配置信息 docker network inspect ltb 容器要使用新的网络，需要在启动时通过 --network 指定 docker run -it --network=ltb_test_net 7c1fe /bin/bash 同一网络中的容器、网关之间都是可以通信的 不同网络容器通信可以用docker network connect命令实现 docker network connect ltb_net 7c1fe 从 Docker 1.10 版本开始，docker daemon 实现了一个内嵌的 DNS server，使容器可以直接通过“容器名”通信。方法很简单，只要在启动时用 --name 为容器命名就可以了 docker run -it --network=my_net2 --name=bbox1 busyboxdocker run -it --network=my_net2 --name=bbox2 busybox 然后，bbox2 就可以直接 ping 到 bbox1 了 使用 docker DNS 有个限制：只能在 user-defined 网络中使用。也就是说，默认的 bridge 网络是无法使用 DNS 的 可以通过docker network rm 实现 docker network rm ltb_test_net 再ifconfig时,网桥已经不见了 参考https://www.cnblogs.com/zuxing/articles/8780661.html","path":"2019/08/10/docker 网络指南/","date":"08-10","excerpt":"","tags":[{"name":"网络","slug":"网络","permalink":"https://litianbo243.github.io/tags/网络/"},{"name":"docker","slug":"docker","permalink":"https://litianbo243.github.io/tags/docker/"},{"name":"虚拟化","slug":"虚拟化","permalink":"https://litianbo243.github.io/tags/虚拟化/"}]},{"title":"Ubuntu 软件和软件源","text":"Ubuntu 软件和软件源什么是软件源源,在Ubuntu下,它相当于软件库,需要什么软件,只要记得正确的软件名就可以用命令安装: sudo apt-get install 软件名 例如:你要安装gcc软件,那么你就可以再终端中输sudo apt-get install gcc,这样就能帮你装好gcc软件,如果源里没有这个软件时,此命令就没法完成 当然,如果你要删除软件时,可以再终端中输入: sudo apt-get remove软件名 Ubuntu中安装软件的方式它主要可以分为下面三种安装方式： 通过 apt 包管理工具从软件源中安装 通过 deb 格式的软件包安装 从软件源码手动编译安装 通过软件源安装软件的原理Ubuntu 系统中自带了一个名为 apt的软件包管理工具，它的主要功能就是负责 Ubuntu 系统中所有软件包的管理，包括从软件源中下载软件，卸载系统中已经安装的软件等。而每当我们通过 apt-get install ```命令去安装一个软件时，apt 包管理工具就会从系统中一个叫做源列表的文件中去读取软件源的网址信息，查找可以包含该软件的软件源的网址，然后就从这个网址上把指定的软件给下载下来，然后由 apt 包管理工具在本地进行一些解压和安装的其他操作## 软件源的分类在 Ubuntu 中软件源其实还细分为下面两种：- Ubuntu 官方软件源- PPA 软件源**Ubuntu 官方软件源:**Ubuntu 官方软件源中包含了 Ubuntu 系统中所用到的绝大部分的软件，它对应的源列表是 **/etc/apt/sources.list****PPA 软件源:**PPA 源出现的背景是因为系统自带的源是很有限的，我们肯定需要一些其他的软件包然而如果是直接下载deb格式的文件的话，又不能获取到更新和维护，所以这就用到了十分重要的 PPA 源了。所谓 PPA 源，就是指 “Personal Package Archives” ，也就是个人软件包集。这其实是一个网站，即－launchpad.net。Launchpad 是 Ubuntu 母公司 Canonical 有限公司所架设的网站，是一个提供维护、支援或联络 Ubuntu 开发者的平台。由于不是所有的软件都能进入 Ubuntu 的官方的软件库，launchpad.net 提供了 PPA，允许开发者建立自己的软件仓库，自由的上传软件。供用户安装和查看更新。## 如何替换官方软件源在终端输入`sudo vim /etc/apt/sources.list`(文件 etc/apt/sources.list是一个普通可编辑的文本文件，保存了ubuntu软件更新的源服务器的地址。)sources.list就是添加源的文件,只要把你在网上找到的源地址加在最后一行就行了,然后保存。回到终端下,更新一下软件列表,输入:`sudo apt-get update`## 如何添加和删除PPA源**添加 PPA 软件源的命令**：`sudo add-apt-repository ppa:user/ppa-name`**删除 PPA 软件源的命令**：`sudo add-apt-repository --remove ppa:user/ppa-name`当我们添加完 PPA 源之后，系统就会在 */etc/apt/sources.list.d/* 文件夹里创建了两个文件：```bashcd /etc/apt/sources.list.dls&quot;&quot;&quot;cuda-8-0-local-ga2.listcuda-8-0-local-ga2.list.save&quot;&quot;&quot; 参考https://www.jianshu.com/p/57a91bc0c594","path":"2019/08/09/Ubuntu 软件和软件源/","date":"08-09","excerpt":"","tags":[{"name":"linux","slug":"linux","permalink":"https://litianbo243.github.io/tags/linux/"},{"name":"软件","slug":"软件","permalink":"https://litianbo243.github.io/tags/软件/"},{"name":"源","slug":"源","permalink":"https://litianbo243.github.io/tags/源/"}]},{"title":"Ubuntu ss 配置及其使用","text":"Ubuntu ss 配置及其使用安装sssudo pip3 install shadowsocks 配置服务器并连接需要创建一个.json的配置文件，用配置文件里的参数进行连接 mkdir shadowsockstouch shadowsocks/ss.jsonvim shadowsocks/ss.json# 然后在里面加入\"\"\"&#123; \"server\":\"1.1.1.1\", \"server_port\":8388, \"local_address\": \"127.0.0.1\", \"local_port\":1080, \"password\":\"your passwd\", \"timeout\":300, \"method\":\"aes-256-cfb\"&#125;\"\"\"# 其中，server填你的服务器ip，sever_port填远程端口号，local_address本地ip，local_part本地端口，password填密码，timeout是延迟时间，method是加密方式，按照实际情况填写并保存# 然后运行sssslocal -c shadowsocks/ss.json proxychains 命令行也走代理安装proxychainssudo apt-get install proxychains 配置proxychains# 先创建配置文件touch /etc/proxychains.confvim /etc/proxychains.conf# 找到 [ProxyList]，在其后面追加如下如下的代理服务器配置信息\"\"\"socks5 127.0.0.1 1080\"\"\"","path":"2019/08/07/Ubuntu ss配置及其使用/","date":"08-07","excerpt":"","tags":[{"name":"linux","slug":"linux","permalink":"https://litianbo243.github.io/tags/linux/"},{"name":"ss","slug":"ss","permalink":"https://litianbo243.github.io/tags/ss/"}]},{"title":"Ubuntu 基础网络排错","text":"Ubuntu 基础网络排错查看网卡信息ifconfig\"\"\"br-8cfad14f3af4 Link encap:以太网 硬件地址 02:42:83:ad:67:64 inet 地址:172.18.0.1 广播:172.18.255.255 掩码:255.255.0.0 UP BROADCAST MULTICAST MTU:1500 跃点数:1 接收数据包:0 错误:0 丢弃:0 过载:0 帧数:0 发送数据包:0 错误:0 丢弃:0 过载:0 载波:0 碰撞:0 发送队列长度:0 接收字节:0 (0.0 B) 发送字节:0 (0.0 B)docker0 Link encap:以太网 硬件地址 02:42:1c:64:56:8f inet 地址:172.17.0.1 广播:172.17.255.255 掩码:255.255.0.0 UP BROADCAST MULTICAST MTU:1500 跃点数:1 接收数据包:0 错误:0 丢弃:0 过载:0 帧数:0 发送数据包:0 错误:0 丢弃:0 过载:0 载波:0 碰撞:0 发送队列长度:0 接收字节:0 (0.0 B) 发送字节:0 (0.0 B)enp6s0 Link encap:以太网 硬件地址 34:97:f6:8d:55:37 inet 地址:192.168.50.228 广播:192.168.50.255 掩码:255.255.255.0 inet6 地址: fe80::5805:f69:d7e9:4d93/64 Scope:Link inet6 地址: 2001:da8:a800:1007:6163:e5b5:ee4d:ef9/64 Scope:Global inet6 地址: 2001:da8:a800:1007:a960:ef5a:58ad:86f9/64 Scope:Global UP BROADCAST RUNNING MULTICAST MTU:1500 跃点数:1 接收数据包:714830 错误:0 丢弃:0 过载:0 帧数:0 发送数据包:271085 错误:4 丢弃:0 过载:0 载波:0 碰撞:0 发送队列长度:1000 接收字节:393839354 (393.8 MB) 发送字节:51512172 (51.5 MB)lo Link encap:本地环回 inet 地址:127.0.0.1 掩码:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 跃点数:1 接收数据包:75959 错误:0 丢弃:0 过载:0 帧数:0 发送数据包:75959 错误:0 丢弃:0 过载:0 载波:0 碰撞:0 发送队列长度:1000 接收字节:38997685 (38.9 MB) 发送字节:38997685 (38.9 MB)\"\"\" 其中ip地址为enp6s0的inet地址：192.168.50.228 enp6s0是我们真实的网卡 lo是本地回环表示本地 docker0和br-XXX是和docker有关的网络和网桥，会分配给docker网络 防火墙# ufw防火墙状态sudo ufw status# 关闭防火墙sudo ufw disabel 重启网络service networking restart 配置文件网卡配置文件： /etc/network/interfaces DNS配置文件： /etc/resolv.conf 或者 /etc/resolvconf/resolv.conf.d/head 文件 ufw防火墙配置文件： /etc/ufw/before.rules 网络排错的基本思路基本思路如下，这跟网上的大多数人写的应该是差不多的。 （1）检查物理链路是否有问题 （2）查看本机IP地址、路由、DNS的设置是否有问题 （3）测试网关或路由器的通畅情况。先测网关然后再测路由器，一级一级地测试 （4）测试ping公网ip的通畅情况（平时要记几个外部IP） （5）测试DNS的通畅情况，可以直接ping网站地址 下面的第三部分，就针对上面的基本思路来进行详细的说明。 检查物理链路是否有问题看看网线有没有接通 查看本机IP地址、路由、DNS的设置是否有问题如果采用的是DHCP自动获取的方法，那么这时候只需要看自己本机的设置上有没有开启自动获取IP的设置以及有没有开启相关的服务 如果用的是静态IP，那么就必须要注意IP地址的填写有没有错（一般网络管理人员给的）、IP地址的子网掩码有没有问题 # 查看ip地址、MAC地址、子网掩码、广播地址ifconfig# 查看dns地址cat /etc/resolv.conf 测试网关或路由的通畅情况# 检查数据的走向sudo traceroute -d www.baidu.com 先测试到网关的通畅情况 ping -d 192.168.50.1 再测试到其他路由器的通畅情况 sudo traceroute 101.4.130.34 测试ping公网IP的通畅情况ping 114.114.114.114 测试DNS的通畅情况可以直接ping网站地址，看有没有回显示IP地址，通不通是了另外一回事 还有nslookup命令 nslookup&gt; baidu.com\"\"\"之后会出现解析的结果\"\"\" 用netstat查看开放端口及服务netstat -tnp 显示路由表route -n","path":"2019/08/07/Ubuntu 基础网络排错/","date":"08-07","excerpt":"","tags":[{"name":"linux","slug":"linux","permalink":"https://litianbo243.github.io/tags/linux/"},{"name":"网络","slug":"网络","permalink":"https://litianbo243.github.io/tags/网络/"}]},{"title":"Ubuntu 进程管理","text":"UBuntu 进程管理psps命令可以列出正在运行的进程 ps -ef\"\"\"-f: 显示多列信息-e: 显示系统中运行的所有的进程信息\"\"\"# 查看所有运行的进程 ，常常和grep连用ps -aux toptop命令是一个常用的查看系统资源使用情况和查看占用系统资源最多的进程的命令 top以列形式显示所有的进程，占最多CPU资源的进程会显示在最上面 top htophtop命令是top的改进版 sudo apt-get install htophtop killkill命令可以根据进程ID来杀死进程 你可以使用ps -A，top，或者grep命令获取到进程ID 从技术层面来讲，kill命令可以发送任何信号给一个进程 你可以使用kill -9 [id] 来杀死顽固的进程 kill -9 PID","path":"2019/08/07/Ubuntu 进程管理/","date":"08-07","excerpt":"","tags":[{"name":"linux","slug":"linux","permalink":"https://litianbo243.github.io/tags/linux/"},{"name":"进程","slug":"进程","permalink":"https://litianbo243.github.io/tags/进程/"}]},{"title":"ssh操作指南","text":"ssh 操作指南检查ssh服务是否启动SSH 为 Secure Shell 的缩写，为建立在应用层基础上的安全通信协议。 ps -e | grep ssh\"\"\"7529 ? 00:00:00 sshd7852 pts/1 00:00:00 ssh\"\"\" 若输入指令后显示类似于上图所示，则说明SSH服务已启动其中sshd表示ssh-server已启动，ssh表示ssh-client已启动 安装ssh服务sudo apt-get install openssh-clientsudo apt-get install openssh-server 启动ssh服务sudo /etc/init.d/ssh startps -e | grep ssh 修改ssh端口号sudo vim /etc/ssh/sshd_config\"\"\"# Package generated configuration file# See the sshd_config(5) manpage for details# What ports, IPs and protocols we listen forPort 22# Use these options to restrict which interfaces/protocols sshd will bind to#ListenAddress ::#ListenAddress 0.0.0.0Protocol 2# HostKeys for protocol version 2HostKey /etc/ssh/ssh_host_rsa_keyHostKey /etc/ssh/ssh_host_dsa_keyHostKey /etc/ssh/ssh_host_ecdsa_keyHostKey /etc/ssh/ssh_host_ed25519_key#Privilege Separation is turned on for securityUsePrivilegeSeparation yes# Lifetime and size of ephemeral version 1 server keyKeyRegenerationInterval 3600ServerKeyBits 1024# LoggingSyslogFacility AUTHLogLevel INFO# Authentication:LoginGraceTime 120PermitRootLogin prohibit-passwordStrictModes yesRSAAuthentication yesPubkeyAuthentication yes#AuthorizedKeysFile %h/.ssh/authorized_keys# Don't read the user's ~/.rhosts and ~/.shosts filesIgnoreRhosts yes# For this to work you will also need host keys in /etc/ssh_known_hostsRhostsRSAAuthentication no# similar for protocol version 2HostbasedAuthentication no# Uncomment if you don't trust ~/.ssh/known_hosts for RhostsRSAAuthentication#IgnoreUserKnownHosts yes# To enable empty passwords, change to yes (NOT RECOMMENDED)PermitEmptyPasswords no# Change to yes to enable challenge-response passwords (beware issues with# some PAM modules and threads)ChallengeResponseAuthentication no# Change to no to disable tunnelled clear text passwords#PasswordAuthentication yes# Kerberos options#KerberosAuthentication no#KerberosGetAFSToken no#KerberosOrLocalPasswd yes#KerberosTicketCleanup yes# GSSAPI options#GSSAPIAuthentication no#GSSAPICleanupCredentials yesX11Forwarding yesX11DisplayOffset 10PrintMotd noPrintLastLog yesTCPKeepAlive yes#UseLogin no#MaxStartups 10:30:60#Banner /etc/issue.net# Allow client to pass locale environment variablesAcceptEnv LANG LC_*Subsystem sftp /usr/lib/openssh/sftp-server# Set this to 'yes' to enable PAM authentication, account processing,# and session processing. If this is enabled, PAM authentication will# be allowed through the ChallengeResponseAuthentication and# PasswordAuthentication. Depending on your PAM configuration,# PAM authentication via ChallengeResponseAuthentication may bypass# the setting of \"PermitRootLogin without-password\".# If you just want the PAM account and session checks to run without# PAM authentication, then enable this but set PasswordAuthentication# and ChallengeResponseAuthentication to 'no'.UsePAM yes\"\"\" 修改端口号port后，重启ssh服务即可生效 sudo /etc/init.d/ssh restart ssh远程登录ssh user@X.X.X.X# X.X.X.X 为ip地址 数据传输完成SSH服务配置之后即可实现基于SSH的数据传输，最常用方便的指令便是scp，以下是常用scp指令： scp SOURCE DESTINATION# -r : 递归复制目录 ssh无密码自动登录需要两步 在本地主机上生成ssh密钥 将生成的公钥传给远程主机 # 在本地主机上生成密钥ssh-keygen -t rsa# 将公钥传给远程主机ssh-copy-id USER@REMOTE_HOST ssh实现端口转发SSH的端口转发包括，本地端口转发和远程端口转发两部分 本地端口转发 # 在本地(运行命令的机器上)起一个监听端口，把所有对该本地端口的访问转发到服务器ssh -L &lt;local-port-to-listen&gt;:&lt;remote-host&gt;:&lt;remote-port&gt; &lt;sshserver&gt;# &lt;local-port-to-listen&gt; 本地端口，也就是命令运行的机器的端口# &lt;remote-host&gt;:&lt;remote-port&gt; 目标机器和端口，是真正提供服务的端口# &lt;sshserver&gt; ssh服务器，是提供端口转发功能的服务器 这个命令的功能就是把”本地机器:“映射到”:“，这个功能通过sshserver实现 本地端口转发通常执行在防火墙外的机器上，用来给防火墙外的其他机器访问防火墙内的资源 # 在proxyserver机器上执行下面的命令ssh -L 30000：webserver：8080 user@SSHserver 远程端口转发 和本地端口转发的区别是： 本地端口转发，起一个监听端口在本地(执行命令的机器)；然后转发所有向这个新起本地端口的请求， 到 sshserver，再到目标机器。 远程端口转发，起一个监听端口在远程(sshserver机器)；然后转发所有向这个新起远程端口的请求，到本地机器(执行命令的机器)，再到目标机器。 ssh -R &lt;local-port-to-listen&gt;:&lt;remote-host&gt;:&lt;remote-port&gt; &lt;sshserver&gt;# &lt;local-port-to-listen&gt; 本地端口，也就是命令运行的机器的端口# &lt;remote-host&gt;:&lt;remote-port&gt; 目标机器和端口，是真正提供服务的端口# &lt;sshserver&gt; ssh服务器，是提供端口转发功能的服务器 所不同的是端口是起在sshserver上，而不是在运行命令的proxyserver机器上。然后把所有对:的请求都转发到:上去远程端口转发通常执行在防火墙内的机器上，用来给防火墙外的其他机器访问防火墙内的资源 # 在proxyserveer机器上执行下面的命令ssh -R 30000:webserver:8080 user@sshserver 借助服务器访问内网主机 sshfs实现本地挂载点挂载远程驱动器# 首先安装sshfssudo apt-get install sshfs# 必须sudo执行命令，否则权限出错sudo sshf -o allow_other ltb@X.X.X.X:/home/ltb ~/AMAX# 使用下面命令卸载umount ~/AMAX","path":"2019/08/06/ssh 操作指南/","date":"08-06","excerpt":"","tags":[{"name":"linux","slug":"linux","permalink":"https://litianbo243.github.io/tags/linux/"},{"name":"ssh","slug":"ssh","permalink":"https://litianbo243.github.io/tags/ssh/"}]},{"title":"Git使用指南","text":"简介Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。Git 与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持。 Git安装配置Ubuntu$ apt-get install libcurl4-gnutls-dev libexpat1-dev gettext \\ libz-dev libssl-dev$ apt-get install git$ git --versiongit version 1.8.1.2 Windows在 Windows 平台上安装 Git 同样轻松，有个叫做 msysGit 的项目提供了安装包，可以到 GitHub 的页面上下载 exe 安装文件并运行：安装包下载地址：https://gitforwindows.org/完成安装之后，就可以使用命令行的 git 工具（已经自带了 ssh 客户端）了，另外还有一个图形界面的 Git 项目管理工具。在开始菜单里找到”Git”-&gt;”Git Bash”，会弹出 Git 命令窗口，你可以在该窗口进行 Git 操作。 Git工作流程一般工作流程如下： 克隆 Git 资源作为工作目录。 在克隆的资源上添加或修改文件。 如果其他人修改了，你可以更新资源。 在提交前查看修改。 提交修改。 在修改完成后，如果发现错误，可以撤回提交并再次修改并提交。 下图展示了 Git 的工作流程： Git工作区、暂存区和版本库基本概念我们先来理解下Git 工作区、暂存区和版本库概念 工作区：就是你在电脑里能看到的目录。 暂存区：英文叫stage, 或index。一般存放在 “.git目录下” 下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。 版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 下面这个图展示了工作区、版本库中的暂存区和版本库之间的关系：图中左侧为工作区，右侧为版本库。在版本库中标记为 “index” 的区域是暂存区（stage, index），标记为 “master” 的是 master 分支所代表的目录树。图中我们可以看出此时 “HEAD” 实际是指向 master 分支的一个”游标”。所以图示的命令中出现 HEAD 的地方可以用 master 来替换。当对工作区修改（或新增）的文件执行 “git add” 命令时，暂存区的目录树被更新，同时工作区修改（或新增）的文件内容被写入到对象库中的一个新的对象中，而该对象的ID被记录在暂存区的文件索引中。当执行提交操作（git commit）时，暂存区的目录树写到版本库（对象库）中，master 分支会做相应的更新。即 master 指向的目录树就是提交时暂存区的目录树。当执行 “git reset HEAD” 命令时，暂存区的目录树会被重写，被 master 分支指向的目录树所替换，但是工作区不受影响。当执行 “git rm —cached “ 命令时，会直接从暂存区删除文件，工作区则不做出改变。当执行 “git checkout .” 或者 “git checkout — “ 命令时，会用暂存区全部或指定的文件替换工作区的文件。这个操作很危险，会清除工作区中未添加到暂存区的改动。当执行 “git checkout HEAD .” 或者 “git checkout HEAD “ 命令时，会用 HEAD 指向的 master 分支中的全部或者部分文件替换暂存区和以及工作区中的文件。这个命令也是极具危险性的，因为不但会清除工作区中未提交的改动，也会清除暂存区中未提交的改动。 Git创建仓库git init# 先cd带制定目录$ git init# 就初始化成功了 git clone# 克隆仓库$ git clone &lt;repo&gt;# 克隆到制定目录$ git clone &lt;repo&gt; &lt;directory&gt;# repo:Git仓库# directory:本地目录 Git基本操作git add# 查看文件状态$ git status -s?? README?? hello.php$ git add README hello.php$ git status -sA READMEA hello.php# 在README中添加一些内容，保存后推出，再执行git status$ git status -sAM READMEA hello.php# &quot;AM&quot; 状态的意思是，这个文件在我们将它添加到缓存之后又有改动# 改动后我们再执行 git add 命令将其添加到缓存中$ git add .$ git status -sA READMEA hello.php# 当你要将你的修改包含在即将提交的缓存中的时候，需要执行git add git commit使用 git add 命令将想要快照的内容写入缓存区， 而执行 git commit 将缓存区内容添加到仓库中。Git 为你的每一个提交都记录你的名字与电子邮箱地址，所以第一步需要配置用户名和邮箱地址。$ git config --global user.name &apos;ltb&apos;$ git config --global user.email 351526199@qq.com 接下来我们写入缓存，并提交对 hello.php 的所有改动。在首个例子中，我们使用 -m 选项以在命令行中提供提交注释。$ git add hello.php$ git status -sA READMEA hello.php$ git commit -m &apos;第一次版本提交&apos;[master (root-commit) d32cf1f] 第一次版本提交 2 files changed, 4 insertions(+) create mode 100644 README create mode 100644 hello.php 现在我们已经记录了快照。如果我们再执行 git status:$ git status# On branch masternothing to comxmit (working directory clean) git reset HEADgit reset HEAD 命令用于取消已缓存的内容。我们先改动文件 README 文件，内容如下：# Runoob Git 测试# 菜鸟教程 hello.php 文件修改为：&lt;?phpecho &apos;菜鸟教程：www.runoob.com&apos;;echo &apos;菜鸟教程：www.runoob.com&apos;;echo &apos;菜鸟教程：www.runoob.com&apos;;?&gt; 现在两个文件修改后，都提交到了缓存区，我们现在要取消其中一个的缓存，操作如下：$ git status -s M README M hello.php $ git add .$ git status -sM READMEM hello.php$ git reset HEAD hello.php Unstaged changes after reset:M hello.php$ git status -sM README M hello.php 现在你执行 git commit，只会将 README 文件的改动提交，而 hello.php 是没有的。$ git commit -m &apos;修改&apos;[master f50cfda] 修改 1 file changed, 1 insertion(+)$ git status -s M hello.php git rm如果只是简单地从工作目录中手工删除文件，运行 git status 时就会在 Changes not staged for commit 的提示。要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除，然后提交。可以用以下命令完成此项工作$ git rm &lt;file&gt; 如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f$ git rm -f &lt;file&gt; 如果把文件从暂存区域移除，但仍然希望保留在当前工作目录中，换句话说，仅是从跟踪清单中删除，使用 —cached 选项即可$ git rm --cached &lt;file&gt; 如我们删除 hello.php文件：$ git rm hello.php rm &apos;hello.php&apos;$ lsREADME 不从工作区中删除文件：$ git rm --cached README rm &apos;README&apos;$ lsREADME 可以递归删除，即如果后面跟的是一个目录做为参数，则会递归删除整个目录中的所有子目录和文件：git rm –r * 进入某个目录中，执行此语句，会删除该目录下的所有文件和子目录。 Git分支管理几乎每一种版本控制系统都以某种形式支持分支。使用分支意味着你可以从开发主线上分离开来，然后在不影响主线的同时继续工作。创建分支命令：$ git branch (branchname) 切换分支命令：# git checkout (branchname) 当你切换分支的时候，Git 会用该分支的最后提交的快照替换你的工作目录的内容， 所以多个分支不需要多个目录。合并分支命令:$ git merge 列出分支$ git branch* master$ git branch testing$ git branch* master testing 现在我们可以看到，有了一个新分支 testing。当你以此方式在上次提交更新之后创建了新分支，如果后来又有更新提交， 然后又切换到了 testing 分支，Git 将还原你的工作目录到你创建分支时候的样子。接下来我们将演示如何切换分支，我们用 git checkout (branch) 切换到我们要修改的分支。$ lsREADME$ echo &apos;runoob.com&apos; &gt; test.txt$ git add .$ git commit -m &apos;add test.txt&apos;[master 3e92c19] add test.txt 1 file changed, 1 insertion(+) create mode 100644 test.txt$ lsREADME test.txt$ git checkout testingSwitched to branch &apos;testing&apos;$ lsREADME 当我们切换到 testing 分支的时候，我们添加的新文件 test.txt 被移除了。切换回 master 分支的时候，它们有重新出现了。$ git checkout masterSwitched to branch &apos;master&apos;$ lsREADME test.txt 我们也可以使用 git checkout -b (branchname) 命令来创建新分支并立即切换到该分支下，从而在该分支中操作。$ git checkout -b newtestSwitched to a new branch &apos;newtest&apos;$ git rm test.txt rm &apos;test.txt&apos;$ lsREADME$ touch hello.php$ git add .$ git commit -am &apos;removed test.txt、add runoob.php&apos;[newtest c1501a2] removed test.txt、add runoob.php 2 files changed, 1 deletion(-) create mode 100644 runoob.php delete mode 100644 test.txt$ lsREADME runoob.php$ git checkout masterSwitched to branch &apos;master&apos;$ lsREADME test.txt 如你所见，我们创建了一个分支，在该分支的上移除了一些文件 test.txt，并添加了 runoob.php 文件，然后切换回我们的主分支，删除的 test.txt 文件又回来了，且新增加的 runoob.php 不存在主分支中。使用分支将工作切分开来，从而让我们能够在不同开发环境中做事，并来回切换。 删除分支$ git branch* master testing$ git branch -d testingDeleted branch testing (was 85fc7e7).$ git branch* master 分支合并一旦某分支有了独立内容，你终究会希望将它合并回到你的主分支。 你可以使用以下命令将任何分支合并到当前分支中去：$ git branch* master newtest$ lsREADME test.txt$ git merge newtestUpdating 3e92c19..c1501a2Fast-forward runoob.php | 0 test.txt | 1 - 2 files changed, 1 deletion(-) create mode 100644 runoob.php delete mode 100644 test.txt$ lsREADME runoob.php 以上实例中我们将 newtest 分支合并到主分支去，test.txt 文件被删除。合并完后就可以删除分支:$ git branch -d newtestDeleted branch newtest (was c1501a2). 删除后， 就只剩下 master 分支了：$ git branch* master 合并冲突合并并不仅仅是简单的文件添加、移除的操作，Git 也会合并修改。$ git branch* master$ cat runoob.php 首先，我们创建一个叫做 change_site 的分支，切换过去，我们将 runoob.php 内容改为:&lt;?phpecho &apos;runoob&apos;;?&gt; 创建 change_site 分支：$ git checkout -b change_siteSwitched to a new branch &apos;change_site&apos;$ vim runoob.php$ head -3 runoob.php&lt;?phpecho &apos;runoob&apos;;?&gt;$ git commit -am &apos;changed the runoob.php&apos;[change_site 7774248] changed the runoob.php 1 file changed, 3 insertions(+) 将修改的内容提交到 change_site 分支中。 现在，假如切换回 master 分支我们可以看内容恢复到我们修改前的(空文件，没有代码)，我们再次修改 runoob.php 文件。$ git checkout masterSwitched to branch &apos;master&apos;$ cat runoob.php$ vim runoob.php # 修改内容如下$ cat runoob.php&lt;?phpecho 1;?&gt;$ git diffdiff --git a/runoob.php b/runoob.phpindex e69de29..ac60739 100644--- a/runoob.php+++ b/runoob.php@@ -0,0 +1,3 @@+&lt;?php+echo 1;+?&gt;$ git commit -am &apos;修改代码&apos;[master c68142b] 修改代码 1 file changed, 3 insertions(+) 现在这些改变已经记录到我的 “master” 分支了。接下来我们将 “change_site” 分支合并过来。$ git merge change_siteAuto-merging runoob.phpCONFLICT (content): Merge conflict in runoob.phpAutomatic merge failed; fix conflicts and then commit the result.$ cat runoob.php # 代开文件，看到冲突内容&lt;?php&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADecho 1;=======echo &apos;runoob&apos;;&gt;&gt;&gt;&gt;&gt;&gt;&gt; change_site?&gt; 我们将前一个分支合并到 master 分支，一个合并冲突就出现了，接下来我们需要手动去修改它。$ vim runoob.php $ cat runoob.php&lt;?phpecho 1;echo &apos;runoob&apos;;?&gt;$ git diffdiff --cc runoob.phpindex ac60739,b63d7d7..0000000--- a/runoob.php+++ b/runoob.php@@@ -1,3 -1,3 +1,4 @@@ &lt;?php +echo 1;+ echo &apos;runoob&apos;; ?&gt; 在 Git 中，我们可以用 git add 要告诉 Git 文件冲突已经解决$ git status -sUU runoob.php$ git add runoob.php$ git status -sM runoob.php$ git commit[master 88afe0e] Merge branch &apos;change_site&apos; Git查看提交历史我们可以用 —oneline 选项来查看历史记录的简洁的版本。$ git log --oneline$ git log --onelined5e9fc2 (HEAD -&gt; master) Merge branch &apos;change_site&apos;c68142b 修改代码7774248 (change_site) changed the runoob.phpc1501a2 removed test.txt、add runoob.php3e92c19 add test.txt3b58100 第一次版本提交 这告诉我们的是，此项目的开发历史。我们还可以用 —graph 选项，查看历史中什么时候出现了分支、合并。以下为相同的命令，开启了拓扑图选项：* d5e9fc2 (HEAD -&gt; master) Merge branch &apos;change_site&apos;|\\ | * 7774248 (change_site) changed the runoob.php* | c68142b 修改代码|/ * c1501a2 removed test.txt、add runoob.php* 3e92c19 add test.txt* 3b58100 第一次版本提交 Git标签如果你达到一个重要的阶段，并希望永远记住那个特别的提交快照，你可以使用 git tag 给它打上标签。比如说，我们想为我们的 runoob 项目发布一个”1.0”版本。 我们可以用 git tag -a v1.0 命令给最新一次提交打上（HEAD）”v1.0”的标签。-a 选项意为”创建一个带注解的标签”。 不用 -a 选项也可以执行的，但它不会记录这标签是啥时候打的，谁打的，也不会让你添加个标签的注解。 我推荐一直创建带注解的标签。$ git tag -a v1.0 当你执行 git tag -a 命令时，Git 会打开你的编辑器，让你写一句标签注解，就像你给提交写注解一样。现在，注意当我们执行 git log —decorate 时，我们可以看到我们的标签了：* d5e9fc2 (HEAD -&gt; master) Merge branch &apos;change_site&apos;|\\ | * 7774248 (change_site) changed the runoob.php* | c68142b 修改代码|/ * c1501a2 removed test.txt、add runoob.php* 3e92c19 add test.txt* 3b58100 第一次版本提交 如果我们忘了给某个提交打标签，又将它发布了，我们可以给它追加标签。例如，假设我们发布了提交 85fc7e7(上面实例最后一行)，但是那时候忘了给它打标签。 我们现在也可以：$ git tag -a v0.9 3b58100$ git log --oneline --decorate --graph* d5e9fc2 (HEAD -&gt; master) Merge branch &apos;change_site&apos;|\\ | * 7774248 (change_site) changed the runoob.php* | c68142b 修改代码|/ * c1501a2 removed test.txt、add runoob.php* 3e92c19 add test.txt* 3b58100 (tag: v0.9) 第一次版本提交 如果我们要查看所有标签可以使用以下命令：$ git tagv0.9v1.0 Github要添加一个新的远程仓库，可以指定一个简单的名字，以便将来引用,命令格式如下：$ git remote add [shortname] [url] 本例以 Github 为例作为远程仓库，如果你没有 Github 可以在官网 https://github.com/注册。由于你的本地 Git 仓库和 GitHub 仓库之间的传输是通过SSH加密的，所以我们需要配置验证信息：使用以下命令生成 SSH Key：$ ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 后面的 your_email@youremail.com 改为你在 Github 上注册的邮箱，之后会要求确认路径和输入密码，我们这使用默认的一路回车就行。成功的话会在 ~/ 下生成 .ssh 文件夹，进去，打开 id_rsa.pub，复制里面的 key。回到 github 上，进入 Account =&gt; Settings（账户配置）。左边选择 SSH and GPG keys，然后点击 New SSH key 按钮,title 设置标题，可以随便填，粘贴在你电脑上生成的 key。添加成功后界面如下所示为了验证是否成功，输入以下命令：$ ssh -T git@github.comHi tianqixin! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 以下命令说明我们已成功连上 Github。之后登录后点击” New repository “ 如下图所示：创建成功后，显示如下信息：以上信息告诉我们可以从这个仓库克隆出新的仓库，也可以把本地仓库的内容推送到GitHub仓库。现在，我们根据 GitHub 的提示，在本地的仓库下运行命令：$ mkdir runoob-git-test # 创建测试目录$ cd runoob-git-test/ # 进入测试目录$ echo &quot;# 菜鸟教程 Git 测试&quot; &gt;&gt; README.md # 创建 README.md 文件并写入内容$ ls # 查看目录下的文件README$ git init # 初始化$ git add README.md # 添加文件$ git commit -m &quot;添加 README.md 文件&quot; # 提交并备注信息[master (root-commit) 0205aab] 添加 README.md 文件 1 file changed, 1 insertion(+) create mode 100644 README.md# 提交到 Github$ git remote add origin git@github.com:tianqixin/runoob-git-test.git$ git push -u origin master 以下命令请根据你在Github成功创建新仓库的地方复制，而不是根据我提供的命令，因为我们的Github用户名不一样，仓库名也不一样。接下来我们返回 Github 创建的仓库，就可以看到文件已上传到 Github上： fork如上图所示找到想要pull request的项目，然后点击fork按钮，此时变会在你的仓库中多出来一个仓库，名字为：自己的账户名/想要pull request的项目的名称 clone通过运行命令：git clone https://github.com/fengyanWang/first-pr.git（后面的网址为你自己账户下刚刚生成的那个新的仓库的地址），将该仓库克隆到当前的开发环境中 branch通过在终端运行命令：git branch -a查看当前所在的分支，通常我们都是在查看分支后再进行代码的修改，这是一个好的习惯。在应用github修改代码时，我们常常采用的策略是在主分支下在创建一个特性分支，在该特性分支下进行代码的修改然后通过该分支执行pull request操作。通过命令：git checkout -b work master(其中work为你新建的特性分支，master为你当前坐在的分支)创建新的特性分支并自动切换 添加修改在刚刚新创建的分支下对fork下的工程进行修改 首先使用命令：git diff查看修改的内容是否正确，然后使用命令：git add readMe.md（其中readMe.md为要添加的文件）向仓库中添加文件，在执行命令：git commit -m “add readMe.md文件”提交说明 要从github发送pull request，github端的仓库中必须有一个包含了修改后的代码的分支，所以需要创建一个与刚刚创建的特性分支（修改所在的分支）相对应的远程分支，执行命令：git push origin work1(其中origin为当时fork的远程主分支的名称，一般默认为origin,work1为本地工作的特性分支)，然后执行：git branch -a进行查看是否创建成功 发送pull request进入到自己的github账户下，并切换到创建的特性分支下，然后点击create pull request后，确定没问题，填写相关内容，然后点击send pull requests","path":"2019/08/05/Git使用指南/","date":"08-05","excerpt":"","tags":[{"name":"git","slug":"git","permalink":"https://litianbo243.github.io/tags/git/"},{"name":"github","slug":"github","permalink":"https://litianbo243.github.io/tags/github/"}]},{"title":"专利申请指南","text":"专利申请流程专利类别专利分为发明专利、外观专利和实用新型。外观专利3-4个月授权2个多月下证书，总共6个月左右。实用新型4-6个月会接到授权通知，交费后2个多月会受到证书，总共8个月左右。注：外观专利和实用新型现在比较快，6个月以内都会出结果，快的2-3个月。发明专利时间不一定，想较于外观专利和实用新型时间较长，要求提前公开的情况下，有1年多、2年授权的。不要求提前公开的话4-5年授权的也有，6年以上的也有，但不是很多。 具体流程依据专利法，发明专利申请的审批程序包括受理、初审、公布、实审以及授权五个阶段。实用新型或者外观设计专利申请在审批中不进行从早期公布和实质审查，只有受理、初审和授权三个阶段。发明、实用新型和外观设计专利的申请、审查流程图如下： 如何写专利专利检索系统 innojoy 润桐 几乎所有的专利都分为以下几块：说明书摘要、摘要附图、权利要求书、技术领域、背景技术、发明内容、附图说明、具体实施方案、说明书附图 说明书摘要就是本发明的方案概述以及达到的技术效果，不超过300字，写作套路就是把权利要求书概括一下，再增加一些技术效果，这个一般写完权利要求再写。 摘要附图就是本发明最具有代表性的一张图，这个等你吧说明书附图都画好了，从里面选一个最具有代表性的应该不是什么难事。 权利要求书要依照本发明方案具体来写，一般分为若干独立权利要求以及一系列的从属权利要求，这一部分也是专利最为重要的部分，怎么写后面会详细说明。 技术领域就是本发明所在的领域，写过论文的都应该知道怎么写，专利里的技术领域可以概括的去写，比如手机的技术领域就是电子技术领域、数据传输方法为通信领域、，当然，也可以再跟上一句“尤其涉及一种……领域”，进行进一步的技术领域限定。 背景技术就是介绍一下该技术领域现在的发展状况以及遇到的问题，这跟你论文里面的背景技术一模一样。 发明内容这部分也有套路，等把权利要求写好之后copy一下粘贴过来，然后把每个权利要求中“其特征在于”之前的语句（包括其特征在于这几个字）改成“可选地”，然后再最后另起一段说一下本发明的有益效果，OK,发明内容部分也写完了。 附图说明就是说一下每个说明书的附图事项表示什么意思，比如图1为本发明实施例一提供的方法流程图，图2为本发明实施例一提供的装置结构示意图。 具体实施方式就是拿出一个具体的例子来解释本发明的技术方案，专利中为何需要这一部分呢？因为权利要求书看起来不太像人话，具体实施方式就是要用人话把权利要求解释清楚，在具体实施方式中，你可以把权利要求中的每一句话用日常用语解释清楚，可以举例说明也可以对权利要求中的一些用词进行详细解释，如果你把每个技术特征解释完之后还能再说一下采用该技术特征的优势，那你这个专利的具体实施方式就已经堪称完美了。 说明书附图就是为了能更清楚地解释具体实施方式，可能你会画几个图对应着来说，也就是说你把写具体实施方式的时候画的几张图片放在这里，就是说明书附图了。 详细说明从上面的内容可以看出，一个专利中最核心的内容就是权力要求书，其他所有的内容都是胃药权利要求进行撰写的，把权利要求写好之后按照套路来完成专利的其他部分就可以，下面具体介绍如何撰写专利的权力要求。 如何撰写权力要求书举个例子，比如你要申请一件发明专利，想要解决两个问题：第一，如何把大象装进冰箱，第二，如何防止大象自己从冰箱中逃出来。为此，你设计了一种把大象关进冰箱的方法：第一步，把冰箱门打开，第二步，把大象放进冰箱，第三步，把冰箱门关上，第四步，把冰箱门锁上。现在方案很清楚了，可以写专利了，以上方案转化为专利权力要求就变成： 1.一种把大象关进冰箱的方法，其中，所述冰箱至少包括门体、腔体和锁体，其特征在于，包括以下步骤：步骤一、将所述冰箱的所述门体打开；步骤二、强所述大象放置于所述冰箱的所述腔体内部；步骤三、将所述冰箱的所述门体关闭；步骤四、将所述冰箱的所述锁体切换至锁定状态。 上面这段话可能看起来很别扭，单专利只有这么写才能清楚、完整、毫无疑问地表述我们的发明方案，有些方案通过专利语言表达出来也可能会有歧义，比如讲锁体切换至锁定状态具体是指什么意思？这个时候就需要在具体实施方式中进行详细解释了。以上，权利要求就写好了，根据上面说的“套路”，整个专利围绕权利要求应该能够很快写出。 专利局官网信息专利的提交形式申请人应当以电子形式或者书面形式提交专利申请。1.申请人以电子文件形式申请专利的，应当事先办理电子申请用户注册手续，通过专利局专利电子申请系统向专利局提交申请文件及其他文件。2.申请人以书面形式申请专利的，可以将申请文件及其他文件当面交到专利局的受理窗口或寄交至“国家知识产权局专利局受理处”（以下简称专利局受理处），也可以当面交到设在地方的专利局代办处的受理窗口或寄交至“国家知识产权局专利局×××代办处”。目前专利局在北京、沈阳、济南、长沙、成都、南京、上海、广州、西安、武汉、郑州、天津、石家庄、哈尔滨、长春、昆明、贵阳、杭州、重庆、深圳、福州、南宁、乌鲁木齐、南昌、银川、合肥、苏州、海口、兰州、太原等城市设立代办处。查询专利局代办处信息可登陆http://www.cnipa.gov.cn/zldbc/ 。国防知识产权局专门受理国防专利申请。 申请专利应当提交哪些文件（1）申请发明专利的，申请文件应当包括：发明专利请求书、说明书摘要（必要时应当提交摘要附图）、权利要求书、说明书（必要时应当提交说明书附图）。 涉及氨基酸或者核苷酸序列的发明专利申请，说明书中应当包括该序列表，把该序列表作为说明书的一个单独部分提交，并单独编写页码，同时还应提交符合国家知识产权局专利局（以下简称专利局）规定的记载有该序列表的光盘或软盘。 依赖遗传资源完成的发明创造申请专利的，申请人应当在请求书中对遗传资源的来源予以说明，并填写遗传资源来源披露登记表，写明该遗传资源的直接来源和原始来源。申请人无法说明原始来源的，应当陈述理由。 （2）申请实用新型专利的，申请文件应当包括：实用新型专利请求书、说明书摘要及其摘要附图、权利要求书、说明书、说明书附图。 示例1：实用新型专利申请撰写示例 （3）申请外观设计专利的，申请文件应当包括：外观设计专利请求书、图片或者照片（要求保护色彩的，应当提交彩色图片或者照片）以及对该外观设计的简要说明。 示例1：外观设计申请撰写示例 示例2：相似外观设计申请撰写示例 申请文件使用统一制定的表格申请文件应当使用专利局统一制定的表格。这些表格可以从国家知识产权局网站下载，下载地址http://www.cnipa.gov.cn/bgxz/ ，或者在专利局受理大厅的咨询处索取或以信函方式索取（信函寄至：国家知识产权局专利局初审及流程管理部发文处），也可以向各地的国家知识产权局专利局代办处（以下简称专利局代办处）索取。一张表格只能用于一件专利申请。 申请文件的纸张质量应当相当于复印机用纸的质量。纸面不得有无用的文字、记号、框、线等。各种文件一律采用A4尺寸（210毫米×297毫米）的纸张。申请文件的纸张应当单面、纵向使用。文字应当自左向右排列，纸张左边和上边应当各留25毫米空白，右边和下边应当各留15毫米空白。 提交申请时如何排列申请文件发明或者实用新型专利申请文件应当按照下列顺序排列：请求书、说明书摘要、摘要附图、权利要求书、说明书（含氨基酸或核苷酸序列表）、说明书附图。 外观设计专利申请文件应当按照下列顺序排列：请求书、图片或照片、简要说明。申请文件各部分都应当分别用阿拉伯数字顺序编写页码。 申请文件的字数和书写要求申请文件各部分一律使用中文。外国人名、地名和科技术语如没有统一中文译文，应当在中文译文后的括号内注明原文。申请文件都应当用宋体、仿宋体或楷体打字或印刷，字迹呈黑色，字高应当在3.5～4.5毫米之间，行距应当在2.5～3.5毫米之间。申请文件中有附图的，线条应当均匀清晰，不得涂改。不得使用工程蓝图作为附图。 证明文件办理专利申请相关手续要附具证明文件的，各种证明文件应当由有关主管部门出具或者由当事人签署。各种证明文件应当是原件；证明文件是复印件的，应当经公证或者由出具证明文件的主管部门加盖公章予以确认（原件在专利局备案确认的除外）。申请人提供的证明文件是外文的，应当附有中文题录译文。 TODO","path":"2019/08/05/专利申请/","date":"08-05","excerpt":"","tags":[{"name":"专利","slug":"专利","permalink":"https://litianbo243.github.io/tags/专利/"}]},{"title":"visdom使用指南","text":"安装visdompip install visdom 打开visompython -m visdom.server# 然后打开浏览器 http://localhost:8097/ 指定端口vis = visdom.Visdom(env=&apos;名称&apos;, port=上面开启visdom服务的指定接口)# eg:vis = visdom.Visdom(env=&apos;faster rcnn&apos;, port=15024) 打印lossdef plot(self, name, y): &quot;&quot;&quot; self.plot(&apos;loss&apos;,1.00) &quot;&quot;&quot; x = self.index.get(name, 0) self.vis.line(Y=np.array([y]), X=np.array([x]), win=(name), opts=dict(title=name), update=None if x == 0 else &apos;append&apos; ) self.index[name] = x + 1 vis.plot(&apos;errord&apos;, error_d)vis.plot(&apos;errorg&apos;, error_g) 打印图片# (n, c, h, w)带batch，打印出一个batch里所有图片vis.images(fix_fake_imgs.detach().cpu().numpy()[:64] * 0.5 + 0.5, win=&apos;fixfake&apos;)vis.images(real_img.data.cpu().numpy()[:64] * 0.5 + 0.5, win=&apos;real&apos;)","path":"2019/08/05/visdom使用指南/","date":"08-05","excerpt":"","tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://litianbo243.github.io/tags/pytorch/"},{"name":"visdom","slug":"visdom","permalink":"https://litianbo243.github.io/tags/visdom/"},{"name":"可视化","slug":"可视化","permalink":"https://litianbo243.github.io/tags/可视化/"}]},{"title":"卷积神经网络原理","text":"人类视觉原理人类的视觉原理如下：从原始信号摄入开始（瞳孔摄入像素 Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。下面是人脑进行人脸识别的一个示例：对于不同的物体，人类视觉也是通过这样逐层分级，来进行认知的：我们可以看到，在最底层特征基本上是类似的，就是各种边缘，越往上，越能提取出此类物体的一些特征（轮子、眼睛、躯干等），到最上层，不同的高级特征最终组合成相应的图像，从而能够让人类准确的区分不同的物体。 那么我们可以很自然的想到：可以不可以模仿人类大脑的这个特点，构造多层的神经网络，较低层的识别初级的图像特征，若干底层特征组成更上一层特征，最终通过多个层级的组合，最终在顶层做出分类呢？答案是肯定的，这也是许多深度学习算法（包括CNN）的灵感来源。 卷积神经网络这是一个最典型的卷积网络，由卷积层、池化层、全连接层组成。其中卷积层与池化层配合，组成多个卷积组，逐层提取特征，最终通过若干个全连接层完成分类。卷积层完成的操作，可以认为是受局部感受野概念的启发，而池化层，主要是为了降低数据维度。综合起来说，CNN通过卷积来模拟特征区分，并且通过卷积的权值共享及池化，来降低网络参数的数量级，最后通过传统神经网络完成分类等任务。 卷积（Convolution）卷积运算的定义如下图所示：在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是24种不同的卷积核的示例： 池化（Pooling）池化听起来很高深，其实简单的说就是下采样。池化的过程如下图所示：上图中，我们可以看到，原始图片是20x20的，我们对其进行下采样，采样窗口为10x10，最终将其下采样成为一个2x2大小的特征图。之所以这么做的原因，是因为即使做完了卷积，图像仍然很大（因为卷积核比较小），所以为了降低数据维度，就进行下采样。之所以能这么做，是因为即使减少了许多数据，特征的统计属性仍能够描述图像，而且由于降低了数据维度，有效地避免了过拟合。在实际应用中，池化根据下采样的方法，分为最大值下采样（Max-Pooling）与平均值下采样（Mean-Pooling）。 反向传播原理为多元函数微积分中的链式求导规则，通过链式求导规则，把梯度传给每个卷积核的权重，更新使他们学习到新的特征。 pooling层的反向求导为：对于mean pooling，假设pooling的窗大小是2x2, 在forward的时候啊，就是在前面卷积完的输出上依次不重合的取2x2的窗平均，得到一个值就是当前mean pooling之后的值。backward的时候，把一个值分成四等分放到前面2x2的格子里面就好了。如下 forward: [1 3; 2 2] -&gt; [2]backward: [2] -&gt; [0.5 0.5; 0.5 0.5] max pooling就稍微复杂一点，forward的时候你只需要把2x2窗子里面那个最大的拿走就好了，backward的时候你要把当前的值放到之前那个最大的位置，其他的三个位置都弄成0。如下 forward: [1 3; 2 2] -&gt; 3backward: [3] -&gt; [0 3; 0 0] 卷积神经网络中的trick分组卷积Group convolution在AlexNet中被提出，可以节省显存，减少参数量。 3*3卷积核在VGGNet中被提出，使用3*3的卷积核可以减少参数数量，效果比大卷积核更好。 每层使用多个尺寸的卷积核在Inception中被提出，事实上，同一层feature map可以分别使用多个不同尺寸的卷积核，以获得不同尺度的特征，再把这些特征结合起来，得到的特征往往比使用单一卷积核的要好。 减少卷积层参数量—Bottleneck在Inception中被提出，如果仅仅引入多个尺寸的卷积核，会带来大量的额外的参数，受到Network In Network中1×1卷积核的启发，为了解决这个问题，他们往Inception结构中加入了一些1×1的卷积核。1×1卷积核也被认为是影响深远的操作，往后大型的网络为了降低参数量都会应用上1×1卷积核。 训练更深的网络—ResNet传统的卷积层层叠网络会遇到一个问题，当层数加深时，网络的表现越来越差，很大程度上的原因是因为当层数加深时，梯度消散得越来越严重，以至于反向传播很难训练到浅层的网络。为了解决这个问题，何凯明大神想出了一个“残差网络”，使得梯度更容易地流动到浅层的网络当中去。 DepthWise操作标准的卷积过程可以看上图，一个2×2的卷积核在卷积时，对应图像区域中的所有通道均被同时考虑，问题在于，为什么一定要同时考虑图像区域和通道？我们为什么不能把通道和空间区域分开考虑？Xception网络就是基于以上的问题发明而来。我们首先对每一个通道进行各自的卷积操作，有多少个通道就有多少个过滤器。得到新的通道feature maps之后，这时再对这批新的通道feature maps进行标准的1×1跨通道卷积操作。这种操作被称为 “DepthWise convolution” ，缩写“DW”。这种操作是相当有效的，在imagenet 1000类分类任务中已经超过了InceptionV3的表现，而且也同时减少了大量的参数。因此，一个depthwise操作比标准的卷积操作降低不少的参数量，同时论文中指出这个模型得到了更好的分类效果。 分组卷积对通道进行随机分组— ShuffleNet在AlexNet的Group Convolution当中，特征的通道被平均分到不同组里面，最后再通过两个全连接层来融合特征，这样一来，就只能在最后时刻才融合不同组之间的特征，对模型的泛化性是相当不利的。为了解决这个问题，ShuffleNet在每一次层叠这种Group conv层前，都进行一次channel shuffle，shuffle过的通道被分配到不同组当中。进行完一次group conv之后，再一次channel shuffle，然后分到下一层组卷积当中，以此循环。经过channel shuffle之后，Group conv输出的特征能考虑到更多通道，输出的特征自然代表性就更高。另外，AlexNet的分组卷积，实际上是标准卷积操作，而在ShuffleNet里面的分组卷积操作是depthwise卷积，因此结合了通道洗牌和分组depthwise卷积的ShuffleNet，能得到超少量的参数以及超越mobilenet、媲美AlexNet的准确率！ 给通道加上权重—SENet无论是在Inception、DenseNet或者ShuffleNet里面，我们对所有通道产生的特征都是不分权重直接结合的，那为什么要认为所有通道的特征对模型的作用就是相等的呢？ 这是一个好问题，于是，ImageNet2017 冠军SEnet就出来了。一组特征在上一层被输出，这时候分两条路线，第一条直接通过，第二条首先进行Squeeze操作（Global Average Pooling），把每个通道2维的特征压缩成一个1维，从而得到一个特征通道向量（每个数字代表对应通道的特征）。然后进行Excitation操作，把这一列特征通道向量输入两个全连接层和sigmoid，建模出特征通道间的相关性，得到的输出其实就是每个通道对应的权重，把这些权重通过Scale乘法通道加权到原来的特征上（第一条路），这样就完成了特征通道的权重分配。 更大的接受域—Dilated convolution标准的3×3卷积核只能看到对应区域3×3的大小，但是为了能让卷积核看到更大的范围，dilated conv使其成为了可能。上图b可以理解为卷积核大小依然是3×3，但是每个卷积点之间有1个空洞，也就是在绿色7×7区域里面，只有9个红色点位置作了卷积处理，其余点权重为0。这样即使卷积核大小不变，但它看到的区域变得更大了。 可变的卷积核—Deformable convolutuin传统的卷积核一般都是长方形或正方形，但MSRA提出了一个相当反直觉的见解，认为卷积核的形状可以是变化的，变形的卷积核能让它只看感兴趣的图像区域 ，这样识别出来的特征更佳。要做到这个操作，可以直接在原来的过滤器前面再加一层过滤器，这层过滤器学习的是下一层卷积核的位置偏移量（offset），这样只是增加了一层过滤器，或者直接把原网络中的某一层过滤器当成学习offset的过滤器，这样实际增加的计算量是相当少的，但能实现可变形卷积核，识别特征的效果更好。 启发现在越来越多的CNN模型从巨型网络到轻量化网络一步步演变，模型准确率也越来越高。现在工业界追求的重点已经不是准确率的提升（因为都已经很高了），都聚焦于速度与准确率的trade off，都希望模型又快又准。因此从原来AlexNet、VGGnet，到体积小一点的Inception、Resnet系列，到目前能移植到移动端的mobilenet、ShuffleNet（体积能降低到0.5mb！），我们可以看到这样一些趋势： 卷积核方面：大卷积核用多个小卷积核代替；单一尺寸卷积核用多尺寸卷积核代替；固定形状卷积核趋于使用可变形卷积核；使用1×1卷积核（bottleneck结构）。 卷积层通道方面：标准卷积用depthwise卷积代替；使用分组卷积；分组卷积前使用channel shuffle；通道加权计算。 卷积层连接方面：使用skip connection，让模型更深；densely connection，使每一层都融合上其它层的特征输出（DenseNet） 启发类比到通道加权操作，卷积层跨层连接能否也进行加权处理？bottleneck + Group conv + channel shuffle + depthwise的结合会不会成为以后降低参数量的标准配置？ 上采样在看图像语义分割方面的论文时，发现在网络解码器结构中有的时候使用反卷积、而有的时候使用unpooling或或者unsampling，查了下资料，发现三者还是有不同的。这里记录一下。图（a）表示UnPooling的过程，特点是在Maxpooling的时候保留最大值的位置信息，之后在unPooling阶段使用该信息扩充Feature Map，除最大值位置以外，其余补0。与之相对的是图（b），两者的区别在于UnSampling阶段没有使用MaxPooling时的位置信息，而是直接将内容复制来扩充Feature Map。从图中即可看到两者结果的不同。图（c）为反卷积的过程，反卷积是卷积的逆过程，又称作转置卷积。最大的区别在于反卷积过程是有参数要进行学习的（类似卷积过程），理论是反卷积可以实现UnPooling和unSampling，只要卷积核的参数设置的合理。","path":"2019/08/05/卷积神经网络/","date":"08-05","excerpt":"","tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"},{"name":"卷积神经网络","slug":"卷积神经网络","permalink":"https://litianbo243.github.io/tags/卷积神经网络/"},{"name":"cv","slug":"cv","permalink":"https://litianbo243.github.io/tags/cv/"}]},{"title":"GANs生成对抗网络","text":"GAN的历程左边部分主要是改进模型解决实际的图片转换，文本转图像，生成图片，视频转换等实际问题；右边部分呢则是主要解决GAN框架本身存在的一些问题。学习路径 GANIan Goodfellow 的原始 GAN 论文肯定是必读之作。涉及了GAN框架、“非饱和”损失函数以及最优判别器的推导。图1：训练对抗神经网络时，同时更新判别分布（D，蓝色虚线）使D能区分数据分布px（REAL，黑色虚线）中的样本和生成分布pg(G，绿色实线) 中的样本。下面的水平线为均匀采样z的区域，上面的水平线为x的部分区域。朝上的箭头显示映射x=G(z)如何将非均匀分布pg作用在转换后的样本上。G在pg高密度区域收缩，且在pg地的低密度区域扩散。(a)考虑一个接近收敛的对抗的模型对：pg 与pdata相似，且D是个部分准确的分类器。(b)在算法的内循环中，训练D来判断数据中的样本，收敛到D*(x)=pdata(x)/[pdata(x)+pg(x)](c)在G的一次更新后，D的梯度引导G(z)流向更可能分类为数据的区域。(d)训练若干步后，如果G和D有足够的容量，他们将会接近某个点，由于pg=pdata 两者都无法提高性能。判别器将不能区别出训练数据分布和生成数据分布，即D(x)= 1/2。 DCGAN顾名思义，DCGAN主要讨论CNN与GAN如何结合使用并给出了一系列建议。另外还讨论了GAN特征的可视化、潜在空间插值等问题。 ImprovedGANIan Goodfellow等人提供了诸多训练稳定GAN的建议，包括特征匹配、mini-batch识别、历史平均、单边标签平滑以及虚拟批标准化等技巧。讨论了GAN不稳定性的最佳假设。 PACGANPACGAN讨论的是的如何分析model collapse，以及提出了PAC判别器的方法用于解决model collapse。思想其实就是将判别器的输入改成多个样本，这样判别器可以同时看到多个样本可以从一定程度上防止model collapse。 WGANWGAN首先从理论上分析了原始GAN模型存在的训练不稳定、生成器和判别器的loss无法只是训练进程、生成样本缺乏多样性等问题，并通过改进算法流程针对性的给出了改进要点。 ConditionalGAN同一般形式的GAN类似，也是先训练判别网络，再训练生成网络，然后再训练判别网络，两个网络交替训练。只是训练判别网络的样本稍有不同，训练判别网络的时候需要这三种样本，分别是：（1）条件和与条件相符的真实图片，期望输出为1；（2）条件和与条件不符的真实图片，期望输出为0；（3）条件和生成网络生成的输出，期望输出为0。 CycleGANCycleGAN讨论的是image2image的转换问题，提出了Cycle consistency loss来处理缺乏成对训练样本来做image2image的转换问题。Cycle Consistency Loss 背后的主要想法，图片A转化得到图片B，再从图片B转换得到图片A’，那么图片A和图片A’应该是图一张图片。 Vid2vidVid2Vid通过在生成器中加入光流约束，判别器中加入光流信息以及对前景和背景分别建模重点解决了视频转换过程中前后帧图像的不一致性问题。 PGGANPGGAN创造性地提出了以一种渐进增大（Progressive growing）的方式训练GAN，利用逐渐增大的PGGAN网络实现了效果令人惊叹的生成图像。“Progressive Growing” 指的是先训练 4x4 的网络，然后训练 8x8，不断增大，最终达到 1024x1024。这既加快了训练速度，又大大稳定了训练速度，并且生成的图像质量非常高。 SeqGANSeqGAN用对抗网络实现了离散序列数据的生成模型。解决了对抗生成网络难应用于nlp领域的问题，并且在文本生成任务上有优异表现。相比以往刻意用增强学习解决生成模型的训练不一样，在NLP上因为存在误差无法回进行梯度更新的问题，对抗生成网络的训练中只能用增强学习，所以模型结构显得如此的恰到好处。 StackGANStackGAN是由文本生成图像，StackGAN模型与PGGAN工作的原理很像，StackGAN 首先输出分辨率为64×64 的图像，然后将其作为先验信息生成一个 256×256 分辨率的图像。 BigGANBigGAN模型是基于 ImageNet 生成图像质量最高的模型之一。该模型很难在本地机器上实现，而且 有许多组件，如 Self-Attention、 Spectral Normalization 和带有投影鉴别器的 cGAN等。 StyleGANStyleGAN应该是截至目前最复杂的GAN模型，该模型借鉴了一种称为自适应实例标准化 (AdaIN) 的机制来控制潜在空间向量 z。虽然很难自己实现一个StyleGAN，但是它提供了很多有趣的想法。","path":"2019/08/05/GAN生成对抗网络学习/","date":"08-05","excerpt":"","tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"},{"name":"GANs","slug":"GANs","permalink":"https://litianbo243.github.io/tags/GANs/"}]},{"title":"服务器管理指南","text":"1.给root用户设置一个密码sudo passwd root 2.查看所有用户和用户组cat /etc/passwdcat /etc/group 3.添加用户sudo suadduser ltb 4.删除用户deluser --remove-all-files ltb 5.添加用户到用户组sudo usermod -aG iccd ltb 6.创建用户组sudo groupadd iccd 7.删除用户组sudo groupdel iccd 8.修改用户组权限vim /etc/sudoers 9.nfs等操作# 下载nfssudo apt-get install nfs-kernal-server# 重启nfssudo /etc/init.d/nfs-kernal-server restart# 修改配置文件sudo vi /etc/export&quot;&quot;&quot;加上/amax-share *(rw,sync,no_root_squash,no_subtree_check)&quot;&quot;&quot;# mount操作sudo mount X.X.X.X：/amax-share /home/ltb/AMAX# umount操作sudo umount /home/ltb/AMAX sshfs实现本地挂载点挂载远程驱动器# 首先安装sshfssudo apt-get install sshfs# 必须sudo执行命令，否则权限出错sudo sshf -o allow_other ltb@X.X.X.X:/home/ltb ~/AMAX# 使用下面命令卸载umount ~/AMAX","path":"2019/08/05/服务器管理指南/","date":"08-05","excerpt":"","tags":[{"name":"linux","slug":"linux","permalink":"https://litianbo243.github.io/tags/linux/"},{"name":"管理","slug":"管理","permalink":"https://litianbo243.github.io/tags/管理/"},{"name":"常用指令","slug":"常用指令","permalink":"https://litianbo243.github.io/tags/常用指令/"}]},{"title":"Ubuntu常用指令","text":"Ubuntu常用指令石墨版: Ubuntu指南. su和sudo：得到root权限sudo passwd [root] # 设定root密码su [root] # 切换到root用户sudo COMMAND # 切换到root用户，执行COMMAND，然后切换回当前用户su USER # 切换回普通用户USER apt和dpkg：用于安装软件包apt list # 根据名称列出软件包apt show PACKAGE # 显示软件包细节sudo apt-get install PACKAGE # 安装软件包sudo apt-get remove PACKAGE # 卸载软件包sudo apt-get check PACKAGE # 检查依赖sudo apt-get update # 更新可用软件包列表sudo apt upgrade # 通过升级来更新系统sudo apt-get install -f # 修复依赖sudo dpkg -i xxx.deb # 运行deb程序进行安装 pip：用于安装python模块（不是linux里的默认指令，需自行安装sudo apt-get install pip）sudo apt-get install python3-pip # 安装python3下的pip# 更新pip到指定版本python3 -m pip install --user --upgrade pip==9.0.3 # (换成你想要的版本编号)# pip临时加速pip 加参数-i https://pypi.tuna.tsinghua.edu.cn/simple # pip永久加速# Linux下，修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹)\"\"\"[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host=mirrors.aliyun.com\"\"\"# windows下，直接在user目录中创建一个pip目录，如：C:\\Users\\xx\\pip，新建文件pip.ini。内容同上。pip install PACKAGE # 安装包pip uninstall PACKAGE # 卸载包pip search PACKAGE # 搜索能安装的包pip list # 例出所有的已安装的包pip show PACKAGE # 展示已安装包的信息，可以指定单个包名展示 pip check PACKAGE # 检查依赖pip --version # 显示pip版本和位置pip help # 查看帮助pip help install # 查看install指令的相关option介绍 ssh和scp：用于主机间的通信ssh USERNAME@IP -p PORTNUM # 远程登录到主机scp LOCAL_FILE USERNAME@IP:REMOTE_DIRECTORY # 复制本地文件到远程主机scp -r LOCAL_FILE USERNAME@IP:REMOTE_DIRECTORY # 复制本地文件夹到远程主机sudo ufw dissable # 关闭防火墙sudo ufw enable # 开启防火墙sudo apt-get install vsftpd # 安装sftpservice vsftpd starrt # 启用sftp zip,unzip,tar,rar,unrar：压缩和解压gzip # gzip是Linux使用最广的压缩指令gzip [-cdtv#] FILENAME\"\"\"-c ：将压缩的数据输出到屏幕上-d ：解压缩-t ：检验压缩文件是否出错-v ：显示压缩比等信息-# ： # 为数字的意思，代表压缩等级，数字越大压缩比越高，默认为 6\"\"\"bzip2 # 提供比gzip更高的压缩比bzip2 [-cdkzv#] FILENAME\"\"\"-k ：保留源文件\"\"\"xz2 # 提供比bzip2更佳的压缩比xz [-dtlkc#] FILENAMEtar 压缩打包tar [-z|-j|-J][cv][-f NEW_TAR_FILE] FILE # 打包压缩tar [-z|-j|-J][tv][-f TAR_FILE] # 查看tar [-z|-j|-J][xv][-f TAR_FILE] [-C DIR] # 解压\"\"\"-z ：使用 zip-j ：使用 bzip2-J ：使用 xz-c ：新建打包文件-t ：查看打包文件里面有哪些文件-x ：解打包或解压缩的功能-v ：在压缩/解压缩的过程中，显示正在处理的文件名-f : filename：要处理的文件-C DIR ： 在特定目录解压缩\"\"\"# 例1tar -jcv -f file.tar.bz2 file # 压缩tar -jxv -f file.tar.bz2 -C ./file # 解压tar -zcv -f dir.tar.gz dir1 dir2 ... # 多个文件压缩# 例2tar -zcv file | split -b 4000M -d -a 1 - file.tar.gz. # 压缩成多个压缩包# 使用split命令\"\"\"-b 4000M 表示设置每个分割包的大小，单位还是可以k-d 参数指定生成的分割包后缀为数字的形式-a x来设定序列的长度(默认值是2)\"\"\"cat file.tar.gz.* | tar -zxv file 多个压缩包解压 du和df：查看磁盘的存储占用和查看文件系统的存储占用du -s # 仅显示总计，只列出最后加的总值du -c # 除了显示个别文件或目录的大小外，还显示一行目录和文件的总和大小du -h # 以k,m,g为单位，提高信息的可读性du --max-depth=1 # 查看当前目录所有（一级）文件的大小du -sh *.mkv # 显示每个mkv文件的大小du -csh *.mkv # 显示每个mkv文件的大小和总和df -h # 以k,m,g为单位，提高信息的可读性df -T # 显示文件系统的类型 which,whereis,locate,find：查找命令#which在PATH变量制定的路径中，搜索某个系统命令的位置，并返回第一个搜索结果#whereis只能用于程序名的搜索，而且只能搜索二进制文件（-b），man说明文件（-m），源代码文件（-s）。若省略，则返回所有信息#locate配合数据库查看文件位置。linux会将系统内所有文件记录在一个数据库文件中，电视该数据库并不是实时更新#find实际搜寻硬盘查询文件名称find . -name \"*.conf\" 当前目录查找扩展名为.conf的文件updatedb 更新数据库locate STRING 快速查找系统数据库中指定的内容which python 查询pythonwhereis python 查询python 文件权限chmod [ugoa][+-=][rwx] FILE|DIR # 改变指定目录或文件的权限# u代表该文件所有者，g代表该文件所有者的同组用户，o代表其他，a表示这三者皆是# r可读，w可写，x可执行chomd u+x test # 该文件的所有者增加可执行的权限# 用数字表示，规定r=4,w=2,x=1chmod 777 file # 三种用户都赋予可读可写可执行的权限chown -R 用户名[:组名] 文件名或目录 # 改变指定目录或文件的所属用户，-R代表递归地改变目录下的所有文件 文件和文本操作ls [-adl] FILE|DIR # 列出文件或者目录信息\"\"\"-a ：列出全部的文件-d ：仅列出目录本身-l ：以长数据串行列出，包含文件的属性与权限等等数据\"\"\"cd DIR # 更换当前路径mkdir [-mp] DIR # 创建目录\"\"\"-m ：配置目录权限-p ：递归创建目录\"\"\"rmdir [-p] DIR # 删除目录，目录必须为空\"\"\"-p ：递归删除目录\"\"\"touch [-acdmt] FILENAME # 更新文件时间或者建立新文件\"\"\"-a ： 更新 atime-c ： 更新 ctime，若该文件不存在则不建立新文件-m ： 更新 mtime-d ： 后面可以接更新日期而不使用当前日期，也可以使用 --date=\"日期或时间\"-t ： 后面可以接更新时间而不使用当前时间，格式为[YYYYMMDDhhmm]\"\"\"cp [-adfilprsu] SOURCE DESTINATION # 复制文件\"\"\"-d ：若来源文件为链接文件，则复制链接文件属性而非文件本身-i ：若目标文件已经存在时，在覆盖前会先询问-p ：连同文件的属性一起复制过去-r ：递归持续复制\"\"\"rm [-fir] FILE|DIR # 删除文件或者目录\"\"\"-r ：递归删除\"\"\"mv [-fiu] SOURCE DESTINATION # 移动文件\"\"\"-f ： force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖\"\"\"ln [-sf] SOURCE DESTINATION # 创建链接\"\"\"-s ：默认是 hard link，加 -s 为 symbolic link-f ：如果目标文件存在时，先删除目标文件\"\"\"cat [-n] FILENAME # 获取文件内容\"\"\"-n ：打印出行号，连同空白行也会有行号，-b 不会\"\"\"grep str /tmp/test # 在文件/tmp/test中查找strgrep ^str /tmp/test # 在文件/tmp/test中查找以str开始的行ls -ld */ # 显示当前目录的所有目录文件ls -l | grep '^d' # 显示当前目录的所有目录文件wc -l # 统计文件行数wc -w # 统计单词数量ls -l | wc -l # 统计当前目前的文件数量，注意要减去“总用量”那一行cp -a dir1 dir2 # 复制目录mv dir1 dir2 # 移动/重命名目录mkdir -p /tmp/dir1/dir2 # 创建一个目录树rm -f file1 # 删除文件rm -rf dir1 # 删除目录ln -s /real_file /soft_link # 创建软链接 进程ps -e # 显示所有进程ps -f # 全格式显示进程ps -u 'liaohuqiang' | grep 'tmux' # 显示指定用户执行的进程，并匹配出包含'tmux'的那一行进程ps -ef | grep 'python' | grep -v grep # 把grep命令排除掉ps -ef | grep 'python' | grep -v grep | awk '&#123;print $2&#125;' # 提取进程号pstree -A #查看进程树kill -2 PID # 类似ctrl+C，在程序结束之前能够保存相关数据，再退出kill -9 PID # 直接强制结束进程reboot # 重启shutdown -r now # 立刻重启shutdown -r 30 # 30分钟后重启shutdown -r 23:00 # 晚上11点重启halt # 立刻关机poweroff # 立刻关机shutdown -h now # 立刻关机shutdown -h 30 # 30分钟后关机shutdown -c # 取消shutdwon的重启或关机command &amp; # 使指令成为后台任务nohup # 不挂断地运行命令，通常和&amp;一起用，输出将附加到目录的nohup.outjobs -l # 列出任务列表，l表示显示进程号（只查看当前终端的，关闭终端后无法看到）fg # 把最近一个job切换到前台fg n # 把某个job切换到前台ctrl z # 挂起某个前台进程bg # 把作业放到后台运行top # 动态显示进程信息top -i # 不显示任何闲置或无用的进程\"\"\"k 杀死某进程n 改变显示的进程数量u 显示指定用户P 按CPU使用情况排序q 退出\"\"\"lscpu # 查看cpu信息概要# 监视cpu# 先安装matstatsudo apt install sysstat# 每两秒查看一次mpstat -P ALL 2 网络netstat # 显示网络情况netstat -a # 列出所有端口netstat -l # 只显示监听端口netstat -t # 列出所有tcp端口netstat -p # 显示使用该端口的pid和程序名称netstat -n # 直接使用ip地址，不通过域名服务器# 找出程序运行的端口 netstat -anp | grep ssh# 找出运行在指定端口的进程 netstat -anp | grep ':80'ifconfig # 查看网卡信息 查看电脑配置free -m # 查看内存使用情况df # 查看磁盘使用情况cat /proc/cpuinfo # 查看cpu信息lspci | grep VGA # 查看显卡nvidia-smi # 查看英伟达系列显卡的使用情况nvidia-smi -L # 查看显卡型号watch -n 1 -d nvidia-smi # 实时监控nvidia-smi，每隔一秒刷新一次，d表示高亮 cat /proc/driver/nvidia/version # 查看显卡驱动cat /etc/issue # 查看ubuntu版本 vim简单操作vim可以分为三种模式：命令模式（Command mode）、插入模式（Insert mode）和底线命令模式（Last line mode）Comand mode：控制屏幕光标的移动，字符或光标的删除，移动复制某区段及进入Insert mode下，或者到Last line mode。Insert mode：唯有在Insert mode下，才可做文字数据输入，按Esc等可回到Comand mode。Last line mode：将储存文件或离开编辑器，也可设置编辑环境，如寻找字符串、列出行号等。 在Command mode下按‘i’、‘a’或‘o’三键就可进入Insert mode。这时候您就可以开始输入文字了i: 插入，从目前光标所在之处插入所输入的文字a: 增加，目前光标所在的下一个字开始输入文字o: 插入新的一行，从行首开始输入文字Insert的切换→Command mode，按Esc键您目前处于Insert mode，您就只能一直打字。假如您发现打错字了，想用光标键往回移动，将该字删除，就要按ESC键转换回Command mode，再删除文字 在Command mode下，可按冒号“：”键入入Last line mode，例如：:w filename (输入“w filename”，将文章存入指定的文件名filename):wq (输入“wq”，因为进入之时已经指定文件名testfile，所以会写入testfile并离开vi):q! (输入“q!”，强制离开并放弃编辑的文件) x：每按一次删除光标所在位置的后面一个字符dd：删除光标所在行yy：复制光标所在行u：假如您误操作一个指令，可以马上按u，回复到上一个操作 其他date # 显示时间whoami # 显示当前用户名who # 目前登录系统的用户信息curl 'url' -O --progress # 下载文件,-O代表保存文件（如果没有则输出到屏幕）, --progress表示会显示进度条 # (curl不是linux的默认自行，需自行安装apt install curl)echo $SHELL # 查看系统使用的是哪种shellecho $PATH # 查看环境变量xdg-open filename # 用默认应用程序打开文件python run_generation.py | tee vggTrainResult # 重定向到文件 cat /usr/local/cuda/version.txt # 查看cuda版本cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 # 查看cudnn版本 参考https://www.cnblogs.com/liaohuiqiang/p/7191462.html","path":"2019/08/05/Ubuntu shell 常用指令 指南/","date":"08-05","excerpt":"","tags":[{"name":"linux","slug":"linux","permalink":"https://litianbo243.github.io/tags/linux/"},{"name":"常用指令","slug":"常用指令","permalink":"https://litianbo243.github.io/tags/常用指令/"},{"name":"shell","slug":"shell","permalink":"https://litianbo243.github.io/tags/shell/"}]},{"title":"语义分割学习","text":"什么是语义分割图像语义分割通俗的来讲就是，将一张图片的每个像素点进行分类，分类到每个语义，即类别。 常用的数据集对于深度学习来说，数据集至关重要，数据集很大程度影响最后模型的性能。现在在研究方面有几个公开的优秀数据集，大家都在这些公开的数据集上进行实验研究，相互比较来使构建的模型更优。Pascal VOC 2012：有 20 类目标，这些目标包括人类、机动车类以及其他类，可用于目标类别或背景的分割Cityscapes：50 个城市的城市场景语义理解数据集Pascal Context：有 400 多类的室内和室外场景Stanford Background Dataset：至少有一个前景物体的一组户外场景。 评判指标对于一个模型来说，需要制定一个评判指标来评判这个模型是否优秀。在语义分割中，常常使用平均IOU来评判模型的性能。平均IOU是基于每个类别来计算的，即先计算每个类单独的IOU，在把每个类的IOU计算平均，就是平均IOU。一般大于0.5就比较好了。 语义分割的发展历程2014 FCN(Fully Convolutional Networks)2015 SegNet2015 空洞卷积(Dilated Convolution)2015 DeepLab2016 RefineNet2016 ENet2017 PSPNet2017 大内核GCN FCNFCN的主要思想是：1.几乎借鉴AlexNet的网络结构，但是把AlexNet最后几层全连接网络改成了卷积网络，这样就可以适应任意大小的图片。2.使用了反卷积（上采样）将底层信息放大，采用的是将feature map周围填0，然后卷积回去，卷积过后的feature map会更大。3.使用了skip结构，融合多层的特征，使结果更准确。 损失函数是在最后一层的 spatial map上的 pixel 的 loss 和，在每一个 pixel 使用 softmax loss 。mIOU1=62.2mIOU2=67.2 SegNet采用encoder-decoder的思想，把图片先encoder为高维的特征，再通过decoder还原为原图的大小，其中上采样部分采用了一种叫做pooling Indices的trick。在encoder时，网络经过几层卷积层后，需要max pooling层将特征图缩小，此时记录max pooling 最大元素所在的位置，叫做pooling Indices。当decoder时，进行上采样时，用pooling Indices反池化，还原成原图的大小。mIOU=59.9 Dilated Convolution使用了空洞卷积，这是一种可用于密集预测的卷积层；提出在多尺度聚集条件下使用空洞卷积的“背景模块”。池化操作增大了感受野，有助于实现分类网络。但是池化操作在分割过程中也降低了分辨率。因此，该论文所提出的空洞卷积层是如此工作的：mIOU=71.3-75.3这篇论文还是很需要看的。 DeepLab使用了空洞卷积；提出了在空间维度上实现金字塔型的空洞池化atrous spatial pyramid pooling(ASPP)；使用了全连接条件随机场。空洞卷积在不增加参数数量的情况下增大了感受野，按照上文提到的空洞卷积论文的做法，可以改善分割网络。我们可以通过将原始图像的多个重新缩放版本传递到CNN网络的并行分支(即图像金字塔)中，或是可使用不同采样率(ASPP)的多个并行空洞卷积层，这两种方法均可实现多尺度处理。我们也可通过全连接条件随机场实现结构化预测，需将条件随机场的训练和微调单独作为一个后期处理步骤。mIOU=79.7 RefineNet使用空洞卷积的方法也存在一定的缺点，它的计算成本比较高，同时由于需处理大量高分辨率特征图谱，会占用大量内存，这个问题阻碍了高分辨率预测的计算研究。DeepLab得到的预测结果只有原始输入的1/8大小。所以，这篇论文提出了相应的编码器-解码器结构，其中编码器是ResNet-101模块，解码器为能融合编码器高分辨率特征和先前RefineNet模块低分辨率特征的RefineNet模块。mIOU=84.2 PSPNet全局场景分类很重要，由于它提供了分割类别分布的线索。金字塔池化模块使用大内核池化层来捕获这些信息。和上文提到的空洞卷积论文一样，PSPNet也用空洞卷积来改善Resnet结构，并添加了一个金字塔池化模块。该模块将ResNet的特征图谱连接到并行池化层的上采样输出，其中内核分别覆盖了图像的整个区域、半各区域和小块区域。在ResNet网络的第四阶段(即输入到金字塔池化模块后)，除了主分支的损失之外又新增了附加损失，这种思想在其他研究中也被称为中级监督(intermediate supervision)。mIOU=82.6-85.4 大内核Large Kernel Matters这项研究通过全局卷积网络来提高语义分割的效果。语义分割不仅需要图像分割，而且需要对分割目标进行分类。在分割结构中不能使用全连接层，这项研究发现可以使用大维度内核来替代。采用大内核结构的另一个原因是，尽管ResNet等多种深层网络具有很大的感受野，有相关研究发现网络倾向于在一个小得多的区域来获取信息，并提出了有效感受野的概念。大内核结构计算成本高，且具有很多结构参数。因此，k×k卷积可近似成1×k＋k×1和k×1＋1×k的两种分布组合。这个模块称为全局卷积网络(Global Convolutional Network, GCN)。接下来谈结构，ResNet(不带空洞卷积)组成了整个结构的编码器部分，同时GCN网络和反卷积层组成了解码器部分。该结构还使用了一种称作边界细化(Boundary Refinement，BR)的简单残差模块。mIOU=82.2-83.6 Deeplabv3与在DeepLab v2网络、空洞卷积中一样，这项研究也用空洞卷积/多空卷积来改善ResNet模型。这篇论文还提出了三种改善ASPP的方法，涉及了像素级特征的连接、加入1×1的卷积层和三个不同比率下3×3的空洞卷积，还在每个并行卷积层之后加入了批量归一化操作。级联模块实际上是一个残差网络模块，但其中的空洞卷积层是以不同比率构建的。这个模块与空洞卷积论文中提到的背景模块相似，但直接应用到中间特征图谱中，而不是置信图谱。置信图谱是指其通道数与类别数相同的CNN网络顶层特征图谱。该论文独立评估了这两个所提出的模型，尝试结合将两者结合起来并没有提高实际性能。两者在验证集上的实际性能相近，带有ASPP结构的模型表现略好一些，且没有加入CRF结构。这两种模型的性能优于DeepLabv2模型的最优值，文章中还提到性能的提高是由于加入了批量归一化层和使用了更优的方法来编码多尺度背景。mIOU=85.7","path":"2019/08/05/语义分割/","date":"08-05","excerpt":"","tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"},{"name":"语义分割","slug":"语义分割","permalink":"https://litianbo243.github.io/tags/语义分割/"}]},{"title":"Docker使用指南","text":"1. 查看docker服务器是否正常sudo docker info 2. 运行容器sudo docker run -i -t ubuntu /bin/bashsudo docker run -i -t 镜像名称 命令代码 3. 使用容器# 查看系统中存在的容器docker ps -a 4. 容器命名sudo docker run --name litianbo_test -i -t ubuntu /bin/bashsudo docker run --name 容器名称 -i -t 镜像名称 命令代码 5. 启动已停止的容器# 根据容器名称启动sudo docker start litianbo_test# 根据容器id启动sudo docker start 41225bc38698 6. 进行容器内部命令行sudo docker attach litianbo_test 7. 创建守护式容器sudo docker run --name litianbo_test -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\" 8. 查看容器日志sudo docker logs litianbo_testsudo docker logs -f litianbo_test 9. 查看容器内进程sudo docker top litianbo_test 10. 在容器内运行进程sudo docker exec -it litianbo_test /bin/bash 11. 停止守护式容器sudo docker stop litianbo_test 12. 自动重启容器sudo docker run --restart=always --name litianbo_test -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\" 13. 查看容器详细信息sudo docker inspect litianbo_test 14. 删除容器sudo docker rm litianbo_test 15. 查看镜像列表sudo docker images 16. 拉取镜像sudo docker pull ubuntu# 运行时指定tagsudo docker run -it ubuntu:16.04 /bin/bash# 对于一个仓库而言，完整的格式如下：[用户名称/]仓库名称:tag 17. 查找镜像sudo docker search keywords 18. 构建镜像 使用docker commit构建 使用docker build和Dockerfile文件来构建# 创建docker hub账号# 通过命令在本地绑定对应的Docker Hub账号sudo docker login# commit# 根据某个镜像创建一个容器# 对该容器进行一些修改# 提交该容器并生成一个新的镜像sudo docker run -it ubuntu /bin/bashroot@b3f9427a5039:/# apt-get -yqq updateroot@b3f9427a5039:/# apt-get -y install apache2root@b3f9427a5039:/# exitsudo docker commit b3f9427a5039 ltb/apache2sudo docker images# -m=”message” 可以用于对提交的镜像添加一些文本描述# –author=”person” 可以用于添加提交人信息# 提交时，我们可以设置tagsudo docker commit -m=&quot;install apache&quot; --author=&quot;ltb&quot; b3f9427a5039 ltb/apache:v1# 查看镜像详细信息sudo inspect ltb/apache2:v1# 用心镜像创建容器sudo docker run -it ltb/apache2:v1 /bin/bash# dockerfile# 先创建一个工作目录，并在目录中创建一个dockerfile文件mkdir workdircd ./workdirtouch dockerfile# 编辑dockerfile文件FROM ubuntu:14.04MAINTAINER Wangzhe0912 &quot;Wangzhe0912@tju.edu.cn&quot;RUN apt-get updateRUN apt-get install -y nginxRUN echo &quot;Hi, I am your contrainer&quot; &gt; /usr/share/nginx/html/index.htmlEXPOSE 80# 基于dockerfile构建新镜像# 写完dockerfile文件后，可以执行docker build命令生成一个新的镜像# 在本地当前目录下寻找dockerfile文件docker build -t=&quot;ltb/nginx:v1&quot;` .# 指定git仓库寻找dockfile文件docker build -t=&quot;wangzhe0912/nginx:v1&quot;` git@github.com:wangzhe0912/docker_web# dockerfile构建缓存# 希望强制忽略缓存时，可以额外添加--no-cache参数进行设置sudo docker build --no-cache -t=&quot;ltb/nginx:v1&quot;# 基于构建缓存的dockfile模板# 添加了一行环境变量ENVFROM ubuntu:14.04MAINTAINER Wangzhe0912 &quot;Wangzhe0912@tju.edu.cn&quot;ENV REFRESHED_AT 2018-01-28RUN apt-get update...# 查看新镜像构建过程docker history image_name/id# 从新镜像启动容器# -d表示后台程序进行运行# -p 80表示运行时公开80端口给宿主机docker run -d -p 80 --name ltb_nginx ltb/nginx:v1 nginx -g &quot;daemon off;&quot;# 端口查询docker port ltb_nginx 80# 访问地址curl localhost:32768 19. 将镜像推送至Docker Hubsudo docker push ltb/nginx 20. 删除镜像sudo docker rmi 镜像名称 21.登录DockerHubsudo docker login# 输入账号密码 22.Docker免sudo操作sudo groupadd dockercat /ect/groupsudo gpasswd -a $&#123;USER&#125; dockernewgrp - docker 23.管理网络# 创建自定义网络docker network create iccd# 删除自定义网络docker network rm iccd","path":"2019/08/05/docker使用指南/","date":"08-05","excerpt":"","tags":[{"name":"docker","slug":"docker","permalink":"https://litianbo243.github.io/tags/docker/"},{"name":"虚拟化","slug":"虚拟化","permalink":"https://litianbo243.github.io/tags/虚拟化/"}]},{"title":"opencv-python常用函数及其介绍","text":"读取图片、显示图片、写回图片# -*- coding: utf-8 -*-import numpy as npimport cv2print(cv2.__version__)img = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;, cv2.IMREAD_COLOR) #读入一副彩色图像。图像的透明度会被忽略 默认参数。# img = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;, cv2.IMREAD_GRAYSCALE) # Load an color image in grayscale 灰度# img = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;, cv2.IMREAD_UNCHANGED) # 包括图像的 alpha 通道img = cv2.resize(img, (640, 480))#rows, cols, ch = img.shapeprint(&apos;行/高:&apos;, rows, &apos;列/宽:&apos;, cols, &apos;通道:&apos;, ch)# 图像的宽对应的是列数, 高对应的是行数。cv2.namedWindow(&apos;image&apos;, cv2.WINDOW_NORMAL) # 可以调整窗口大小# cv2.namedWindow(&apos;image&apos;, cv2.WINDOW_AUTOSIZE)#自动调整# cv2.namedWindow(&apos;image&apos;, cv2.WINDOW_KEEPRATIO)#保持图片比例# cv2.resizeWindow(&apos;image&apos;, 200, 200) # 不起作用？cv2.imshow(&apos;image&apos;, img) # 窗口会自动调整为图像大小# 在窗口上按任意键退出cv2.waitKey(delay=0) # 返回按键的 ASCII 码值cv2.destroyAllWindows()# # cv2.imwrite(‘/home/ltb/图片/cv2-tutorial/000001.png’, img)颜色转化# -*- coding: utf-8 -*-# @Time : 2018/1/20 17:15# @Author : play4fun# @File : 颜色转换.py# @Software: PyCharm&quot;&quot;&quot;颜色转换.py:&quot;&quot;&quot;import cv2img = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;, cv2.IMREAD_COLOR)cv2.imshow(&quot;BGR&quot;, img)gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)cv2.imshow(&quot;GRAY&quot;, gray)temp = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # 灰色转RGBcv2.imshow(&quot;RGB&quot;, temp)cv2.waitKey(0) 图像通道顺序转换import cv2import numpy as npfrom PIL import Imagepath = \"/home/ltb/1.jpg\"data = cv2.imread(path)data = np.array(data)data = np.transpose(data, (1, 2, 0))img = Image.fromarray(data)img.show() 读取视频流（单摄像头）# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Fri Jan 3 21:06:22 2014@author: duan &quot;&quot;&quot;&apos;&apos;&apos; 注意 当你的程序报错时 你 先检查的是你的摄像头是否能够在其他程 序中正常工作 比如 linux 下的 Cheese 。&apos;&apos;&apos;import numpy as npimport cv2cap = cv2.VideoCapture(0) # 一般的笔 本电脑 有内置摄像头。所以参数就是 0。你可以 设置成 1 或 者其他的来 择别的摄像头&apos;&apos;&apos;你可以使用函数 cap.get(propId) 来获得 的一些参数信息。propId 可以是 0 到 18 之 的任何整数。其中的一些值可以使用 cap.set(propId,value) 来修改 value 就是 你想 置成的新值。例如 我可以使用 cap.get(3) cv2.CAP_PROP_FRAME_WIDTH和 cap.get(4) cv2.CAP_PROP_FRAME_HEIGHT来查看每一帧的宽和高。默认情况下得到的值是 640X480。但是我可以使用 ret=cap.set(3,320) 和 ret=cap.set(4,240) 来把宽和高改成 320X240。&apos;&apos;&apos;# ret=cap.set(3,320)# ret=cap.set(4,240)# ret = cap.set(cv2.CAP_PROP_FRAME_WIDTH, 480)#避免计算量过大# ret = cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 270)## 等比缩放frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # 4 ，720frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) # 3 ，1280frame_height = int(480 / frame_width * frame_height) # 270ret = cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height) # 高ret = cap.set(cv2.CAP_PROP_FRAME_WIDTH, 480)# while (True):while cap.isOpened(): # 检查是否成功初始化，否则就 使用函数 cap.open() # Capture frame-by-frame ret, frame = cap.read() # ret 返回一个布尔值 True/False # print(&apos;frame shape:&apos;,frame.shape)#(720, 1280, 3) frame = cv2.flip(frame, flipCode=1) # 左右翻转,使用笔记本电脑摄像头才有用。 # flipCode：翻转方向：1：水平翻转；0：垂直翻转；-1：水平垂直翻转 # Our operations on the frame come here gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Display the resulting frame cv2.imshow(&apos;frame&apos;, gray) cv2.setWindowTitle(&apos;frame&apos;, &apos;COLOR_BGR2GRAY&apos;) # Property=cv2.getWindowProperty(&apos;frame&apos;,0)#无用 # if cv2.waitKey(1) &amp; 0xFF == ord(&apos;q&apos;):#不行 # break key = cv2.waitKey(delay=10) if key == ord(&quot;q&quot;): break# When everything done, release the capturecap.release()cv2.destroyAllWindows() 读取视频流（双摄像头）# -*- coding: utf-8 -*-# @Time : 2017/8/15 00:19# @Author : play4fun# @File : two_camera.py# @Software: PyCharm&quot;&quot;&quot;two_camera.py:&quot;&quot;&quot;import cv2import numpy as npcap0 = cv2.VideoCapture(0)cap1 = cv2.VideoCapture(1)ret = cap0.set(3, 320)ret = cap0.set(4, 240)ret = cap1.set(3, 320)ret = cap1.set(4, 240)while cap0.isOpened() and cap1.isOpened(): ret0, frame0 = cap0.read() ret1, frame1 = cap1.read() if ret0: cv2.imshow(&apos;frame0&apos;, frame0) cv2.setWindowTitle(&apos;frame0&apos;, &apos;On Top&apos;) if ret1: cv2.imshow(&apos;frame1&apos;, frame1) # cv2.moveWindow(&apos;frame1&apos;, x=frame0.shape[1], y=0) cv2.moveWindow(&apos;frame1&apos;, x=320, y=40) key = cv2.waitKey(delay=2) if key == ord(&quot;q&quot;): break# When everything done, release the capturecap0.release()cap1.release()cv2.destroyAllWindows() 读取视频（影片）import numpy as npimport cv2cap = cv2.VideoCapture(&apos;/home/ltb/视频/simplescreenrecorder-2019-04-14_00.52.45.mkv&apos;)# cap = cv2.VideoCapture(&apos;output.avi&apos;)# cap = cv2.VideoCapture(&apos;Minions_banana.mp4&apos;)# 帧率fps = cap.get(cv2.CAP_PROP_FPS) # 25.0print(&quot;Frames per second using video.get(cv2.CAP_PROP_FPS) : &#123;0&#125;&quot;.format(fps))# 总共有多少帧num_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)print(&apos;共有&apos;, num_frames, &apos;帧&apos;)#frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)print(&apos;高：&apos;, frame_height, &apos;宽：&apos;, frame_width)FRAME_NOW = cap.get(cv2.CAP_PROP_POS_FRAMES) # 第0帧print(&apos;当前帧数&apos;, FRAME_NOW) # 当前帧数 0.0# 读取指定帧,对视频文件才有效，对摄像头无效？？frame_no = 121cap.set(1, frame_no) # Where frame_no is the frame you wantret, frame = cap.read() # Read the framecv2.imshow(&apos;frame_no&apos;+str(frame_no), frame)FRAME_NOW = cap.get(cv2.CAP_PROP_POS_FRAMES)print(&apos;当前帧数&apos;, FRAME_NOW) # 当前帧数 122.0while cap.isOpened(): ret, frame = cap.read() FRAME_NOW = cap.get(cv2.CAP_PROP_POS_FRAMES) # 当前帧数 print(&apos;当前帧数&apos;, FRAME_NOW) gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) cv2.imshow(&apos;frame&apos;, gray) key = cv2.waitKey(1) if key == ord(&quot;q&quot;): breakcap.release()cv2.destroyAllWindows() 写回视频import numpy as npimport cv2cap = cv2.VideoCapture(0)width = 640ret = cap.set(3, width)height = 480ret = cap.set(4, height)# Define the codec and create VideoWriter objectfourcc = cv2.VideoWriter_fourcc(*&apos;XVID&apos;) # opencv 3.0# Error: &apos;module&apos; object has no attribute &apos;VideoWriter_fourcc&apos;# fourcc=cv2.VideoWriter_fourcc(&apos;X&apos;, &apos;V&apos;, &apos;I&apos;, &apos;D&apos;)#jpeg,h263,&apos;m&apos;, &apos;p&apos;, &apos;4&apos;, &apos;v&apos;#out = cv2.VideoWriter(&apos;output.avi&apos;, fourcc, 20.0, (width, height))while cap.isOpened(): ret, frame = cap.read() if ret is True: frame = cv2.resize(frame, (640, 480)) # write the flipped frame out.write(frame) cv2.imshow(&apos;frame&apos;, frame) else: break key = cv2.waitKey(1) if key == ord(&quot;q&quot;): break# Release everything if job is finishedcap.release()out.release()cv2.destroyAllWindows() 绘图函数# -*- coding: utf-8 -*-import numpy as npimport cv2&apos;&apos;&apos;• img: 你想 绘制图形的 幅图像。• color: 形状的颜色。以RGB为例 需要传入一个元组BGR 例如 255,0,0 代表蓝色，第一个是蓝色通道，第二个是绿色通道，第三个是红色通道。对于灰度图只需要传入灰度值。• thickness 线条的粗细。如果给一个闭合图形 置为 -1 那么这个图形就会被填充。 默认值是 1.• linetype 线条的类型， 8 连接，抗锯齿等。 默认情况是8 连接。cv2.LINE_AA 为抗锯齿 这样看起来会非常平滑。&apos;&apos;&apos;# Create a black imageimg = np.zeros((512, 512, 3), np.uint8)# Draw a diagonal blue line with thickness of 5 pxcv2.line(img, pt1=(0, 0), pt2=(511, 511), color=(255, 0, 0), thickness=5) # pt1, pt2, color, thickness=# cv2.polylines() 可以 用来画很多条线。只需要把想 画的线放在一 个列表中， 将 列表传给函数就可以了。每条线 会被独立绘制。 这会比用 cv2.line() 一条一条的绘制 要快一些。# cv2.polylines(img, pts, isClosed, color, thickness=None, lineType=None, shift=None)cv2.arrowedLine(img, pt1=(21, 13), pt2=(151, 401), color=(255, 0, 0), thickness=5)cv2.rectangle(img, (384, 0), (510, 128), (0, 255, 0), 3)cv2.circle(img, center=(447, 63), radius=63, color=(0, 0, 255), thickness=-1) # center, radius, color, thickness=None# 一个参数是中心点的位置坐标。 下一个参数是长轴和短轴的长度。椭圆沿逆时针方向旋转的角度。# 椭圆弧演顺时针方向起始的角度和结束角度 如果是 0 很 360 就是整个椭圆cv2.ellipse(img, center=(256, 256), axes=(100, 50), angle=0, startAngle=0, endAngle=180, color=255, thickness=-1) # center, axes, angle, startAngle, endAngle, color, thickness=pts = np.array([[10, 5], [20, 30], [70, 20], [50, 10]], np.int32)pts = pts.reshape((-1, 1, 2))# 这里 reshape 的第一个参数为-1, 表明这一维的长度是根据后面的维度的计算出来的。# 注意 如果第三个参数是 False 我们得到的多边形是不闭合的 ，首 尾不相 连 。font = cv2.FONT_HERSHEY_SIMPLEX# org :Bottom-left corner of the text string in the image.左下角# 或使用 bottomLeftOrigin=True,文字会上下颠倒cv2.putText(img, text=&apos;bottomLeftOrigin&apos;, org=(10, 400), fontFace=font, fontScale=1, color=(255, 255, 255), thickness=1, bottomLeftOrigin=True) # text, org, fontFace, fontScale, color, thickness=cv2.putText(img, text=&apos;OpenCV&apos;, org=(10, 500), fontFace=font, fontScale=4, color=(255, 255, 255), thickness=2) # text, org, fontFace, fontScale, color, thickness=# 所有的绘图函数的返回值都是 None ，所以不能使用 img = cv2.line(img,(0,0),(5winname = &apos;example&apos;cv2.namedWindow(winname, 0)cv2.imshow(winname, img)cv2.imwrite(&quot;example.png&quot;, img)cv2.waitKey(0)cv2.destroyAllWindows() 拆分通道、合并通道# -*- coding: utf-8 -*-import cv2import numpy as np# 拆分及合并图像通道img = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;)#b, g, r = cv2.split(img) # 比较耗时的操作，请使用numpy 索引img = cv2.merge((b, g, r))#b = img[:, :, 0]# 使所有像素的红色通道值都为 0,你不必先拆分再赋值。# 你可以 直接使用 Numpy 索引,这会更快。img[:, :, 2] = 0# 保存到文件，看下效果cv2.imwrite(filename=&apos;split_color2.jpg&apos;, img=img) 图像相加# -*- coding: utf-8 -*-import cv2import numpy as np# 学习图像上的算术运算 加法 减法 位运算等# 你可以使用函数 cv2.add() 将两幅图像进行加法运算 当然也可以直接使 用 numpy ，# res=img1+img# 两幅图像的大小 类型必须一致 ，或者第二个 图像可以使一个简单的标量值。x = np.uint8([250])y = np.uint8([10])print(cv2.add(x, y)) # 250+10 = 260 =&gt; 255# [[255]]print(x + y) # 250+10=260%256=4# [4]# 图像混合img1 = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;)img2 = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.png&apos;)dst = cv2.addWeighted(img1, 0.1, img2, 0.9, 0) # 第一幅图的权重是 0.7 第二幅图的权重是 0.3cv2.imshow(&apos;dst&apos;, dst)cv2.waitKey(0)cv2.destroyAllWindows() 图像扩展缩放# -*- coding: utf-8 -*-&apos;&apos;&apos;扩展缩放在缩放时我们推荐使用 cv2.INTER_AREA在扩展时我们推荐使用 v2.INTER_CUBIC 慢) 和 v2.INTER_LINEAR。默认情况下所有改变图像尺寸大小的操作使用的插值方法 是 cv2.INTER_LINEAR。Resize(src, dst, interpolation=CV_INTER_LINEAR)&apos;&apos;&apos;import cv2import numpy as npimg = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;)# 下面的 None 本应 是 出图像的尺寸 但是因为后边我们设置了缩放因子# 因此这里为 Noneres = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)# OR# 我们直接设置输出图像的尺寸 所以不用设置缩放因子# height, width = img.shape[:2]# res = cv2.resize(img, (2 * width, 2 * height), interpolation=cv2.INTER_CUBIC)cv2.imshow(&apos;resize&apos;, res)cv2.imshow(&apos;src img&apos;, img)cv2.waitKey(0)cv2.destroyAllWindows() Otsu二值化# -*- coding: utf-8 -*-&apos;&apos;&apos;Otsu&apos;s 二值化在第一 分中我们提到 retVal 当我们使用 Otsu 二值化时会用到它。 么它到底是什么呢在使用全局 值时 我们就是 便给了一个数来做 值 我们怎么知 我们 取的 个数的好坏呢? 答案就是不停的尝 。 如果是一副双峰图像 ，简 单来 双峰图像是指图像直方图中存在两个峰 呢 ？ 我们岂不是应 在两个峰 之 的峰 一个值作为阈值 。 就是 Otsu 二值化 做的。 简单来说，就是对一副双峰图像自动根据其直方图计算出一个阈值。 对于非双峰图像 这 种方法 得到的结果可能会不理想 。 这里 用到的函数 是 cv2.threshold() 但是 需要多传入一个参数 flag cv2.THRESH_OTSU。 这时 把 值 为 0。然后算法会找到最 优阈值 ，这 个最优 值就是 回值 retVal。 如果不使用 Otsu 二值化 返回的retVal 值与 设定的 阈值相等。下 的例子中 输入图像是一副带有噪声的图像。第一种方法 我们 设127 为全局 阈值。第二种方法 我们直接使用 Otsu 二值化。第三种方法 我 们 先使用一个 5x5 的 高斯核 去噪 然后再使用 Otsu 二值化。看看噪音 去除对结果的影响有多大吧。&apos;&apos;&apos;import cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;, 0)# global thresholdingret1, th1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)# Otsu&apos;s thresholdingret2, th2 = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)# Otsu&apos;s thresholding after Gaussian filtering# 5,5 为 斯核的大小 0 为标准差blur = cv2.GaussianBlur(img, (5, 5), 0)# 阀值一定为 0ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)# plot all the images and their histogramsimages = [img, 0, th1, img, 0, th2, blur, 0, th3]titles = [&apos;Original Noisy Image&apos;, &apos;Histogram&apos;, &apos;Global Thresholding (v=127)&apos;, &apos;Original Noisy Image&apos;, &apos;Histogram&apos;, &quot;Otsu&apos;s Thresholding&quot;, &apos;Gaussian filtered Image&apos;, &apos;Histogram&apos;, &quot;Otsu&apos;s Thresholding&quot;]# 使用了 pyplot 中画直方图的方法 plt.hist,# 注意的是它的参数是一维数组# 所以使用了 numpy ravel 方法 将多维数组 换成一维 也可以使用 flatten 方法# ndarray.flat 1-D iterator over an array.# ndarray.flatten 1-D array copy of the elements of an array in row-major order.for i in range(3): plt.subplot(3, 3, i * 3 + 1), plt.imshow(images[i * 3], &apos;gray&apos;) plt.title(titles[i * 3]), plt.xticks([]), plt.yticks([]) plt.subplot(3, 3, i * 3 + 2), plt.hist(images[i * 3].ravel(), 256) plt.title(titles[i * 3 + 1]), plt.xticks([]), plt.yticks([]) plt.subplot(3, 3, i * 3 + 3), plt.imshow(images[i * 3 + 2], &apos;gray&apos;) plt.title(titles[i * 3 + 2]), plt.xticks([]), plt.yticks([])plt.show() 自适应二值化# -*- coding: utf-8 -*-&apos;&apos;&apos;自适应阈值Adaptive Method- 指定 算阈值的方法。– cv2.ADPTIVE_THRESH_MEAN_C 值取自相邻区域的平均值– cv2.ADPTIVE_THRESH_GAUSSIAN_C 值取值相邻区域 的加权和 ，权重为一个高斯窗口&apos;&apos;&apos;import cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;, 0)# 中值滤波img = cv2.medianBlur(img, 5)ret, th1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)# 11 为 Block size 邻域大小 用来计算阈值的区域大小 ,# 2 为 C值，常数， 阈值就等于的平均值或者加权平均值减去这个常数。th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)th3 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)titles = [&apos;Original Image&apos;, &apos;Global Thresholding (v = 127)&apos;, &apos;Adaptive Mean Thresholding&apos;, &apos;Adaptive Gaussian Thresholding&apos;]images = [img, th1, th2, th3]for i in range(4): plt.subplot(2, 2, i + 1), plt.imshow(images[i], &apos;gray&apos;) plt.title(titles[i]) plt.xticks([]), plt.yticks([])plt.show() 简单二值化# -*- coding: utf-8 -*-&apos;&apos;&apos;简单阈值像素值高于阈值时 我们给这个像素 赋予一个新值， 可能是白色 ， 否则我们给它赋予另外一种颜色， 或是黑色 。 这个函数就是 cv2.threshhold()。 这个函数的第一个参数就是原图像 原图像应 是灰度图。 第二个参数就是用来对像素值进行分类的阈值。 第三个参数 就是当像素值高于， 有时是小于 阈值时应该被赋予的新的像素值。 OpenCV 提供了多种不同的阈值方法 ， 是由第四个参数来决定的。 些方法包括• cv2.THRESH_BINARY• cv2.THRESH_BINARY_INV • cv2.THRESH_TRUNC• cv2.THRESH_TOZERO• cv2.THRESH_TOZERO_INV&apos;&apos;&apos;import cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;, 0)ret, thresh1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)ret, thresh2 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)ret, thresh3 = cv2.threshold(img, 127, 255, cv2.THRESH_TRUNC)ret, thresh4 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO)ret, thresh5 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO_INV)titles = [&apos;Original Image&apos;, &apos;BINARY&apos;, &apos;BINARY_INV&apos;, &apos;TRUNC&apos;, &apos;TOZERO&apos;, &apos;TOZERO_INV&apos;]images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]for i in range(6): plt.subplot(2, 3, i + 1), plt.imshow(images[i], &apos;gray&apos;) plt.title(titles[i]) plt.xticks([]), plt.yticks([])plt.show() 图像腐蚀# -*- coding: utf-8 -*-&apos;&apos;&apos;两个基本的形态学操作是腐蚀和膨胀。他们 的变体构成了开运算 ，闭运算， 梯度等。根据卷积核的大小 前景的所有像素 会 腐 掉 变为 0 ，所以前景物体会变小 整幅图像的白色区域会减少。对于去除白噪声很有用 也可以用来断开两个 在一块的物体等。&apos;&apos;&apos;import cv2import numpy as npimg = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;, 0)cv2.imshow(&apos;Origin&apos;, img)print(img.shape)#您可以将内核看作是一个小矩阵，我们在图像上滑动以进行（卷积）操作，例如模糊，锐化，边缘检测或其他图像处理操作。kernel = np.ones((5, 5), np.uint8)erosion = cv2.erode(img, kernel, iterations=1)cv2.imshow(&apos;erode&apos;, erosion)cv2.moveWindow(&apos;erode&apos;, x=img.shape[1], y=0)cv2.waitKey(0)cv2.destroyAllWindows() 图像膨胀# -*- coding: utf-8 -*-&apos;&apos;&apos;与腐 相反 与卷积核对应的原图像的像素值中只 有一个是 1 中心元 素的像素值就是 1。所以 个操作会增加图像中的白色区域 前景 。一般在去 噪声时先用腐 再用膨胀。因为腐 在去掉白噪声的同时 也会使前景对 变 小。所以我们再对他 膨胀。 时噪声已经 去 了 不会再回来了 但是 前景 在并会增加。膨胀也可以用来 接两个分开的物体。&apos;&apos;&apos;import cv2import numpy as npimg = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;, 0)cv2.imshow(&apos;Origin&apos;, img)print(img.shape)kernel = np.ones((5, 5), np.uint8)dilation = cv2.dilate(img, kernel, iterations=1)cv2.imshow(&apos;dilation&apos;, dilation)cv2.moveWindow(&apos;dilation&apos;, x=img.shape[1], y=0)cv2.waitKey(0)cv2.destroyAllWindows() 图像匹配# -*-coding:utf8-*-#__author__ = &apos;play4fun&apos;&quot;&quot;&quot;create time:15-10-24 下午5:46原理模板匹配是用来在一副大图中搜寻查找模版图像位置的方法。OpenCV 为 我们提供了函数 cv2.matchTemplate()。和 2D 卷积一样 它也是用模板图像在输入图像 大图 上滑动 并在每一个位置对模板图像和与其对应的 输入图像的子区域 比较。OpenCV 提供了几种不同的比较方法 细节 看 文档 。返回的结果是一个灰度图像 每一个像素值 示了此区域与模板的匹配 程度。如果输入图像的大小是 WxH模板的大小是 wxh 输出的结果 的大小就是 W-w+1 H-h+1 。当你得到这幅图之后 就可以使用函数 cv2.minMaxLoc() 来找到其中的最小值和最大值的位置了。第一个值为矩形左上角的点 位置w h 为 moban 模板矩形的宽和 。这个矩形就是 找到的模板区域了。&quot;&quot;&quot;import cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread(&apos;../data/messi5.jpg&apos;, 0)img2 = img.copy()template = cv2.imread(&apos;../data/messi_face.jpg&apos;, 0)w, h = template.shape[::-1]# All the 6 methods for comparison in a listmethods = [&apos;cv2.TM_CCOEFF&apos;, &apos;cv2.TM_CCOEFF_NORMED&apos;, &apos;cv2.TM_CCORR&apos;, &apos;cv2.TM_CCORR_NORMED&apos;, &apos;cv2.TM_SQDIFF&apos;, &apos;cv2.TM_SQDIFF_NORMED&apos;]for meth in methods: img = img2.copy() # exec 语句用来执行储存在字符串或文件中的 Python 语句。 # 例如,我们可以在运行时生成一个包含 Python 代码的字符串, # 然后使用 exec 语句执行这些语句。 # eval 语句用来计算存储在字符串中的有效 Python 表达式 method = eval(meth) # Apply template Matching res = cv2.matchTemplate(img, template, method) min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res) # 使用不同的比较方法,对结果的解释不同 # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]: top_left = min_loc else: top_left = max_loc bottom_right = (top_left[0] + w, top_left[1] + h) cv2.rectangle(img, top_left, bottom_right, 255, 2) plt.subplot(121), plt.imshow(res, cmap=&apos;gray&apos;) plt.title(&apos;Matching Result&apos;), plt.xticks([]), plt.yticks([]) plt.subplot(122), plt.imshow(img, cmap=&apos;gray&apos;) plt.title(&apos;Detected Point&apos;), plt.xticks([]), plt.yticks([]) plt.suptitle(&apos;method: &apos; + meth) plt.show() SIFT算法# -*- coding: utf-8 -*-# @Time : 2017/7/13 下午2:23# @Author : play4fun# @File : sift.py# @Software: PyCharm&quot;&quot;&quot;sift.py:尺度不变特征变换关键点 极值点 定位&quot;&quot;&quot;import cv2import numpy as npimg = cv2.imread(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;)gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)sift = cv2.xfeatures2d.SIFT_create()kp = sift.detect(gray, None)img = cv2.drawKeypoints(gray, kp, img)# 计算关键点描述符# 使用函数 sift.compute() 来 计算 些关键点的描述符。例如# kp, des = sift.compute(gray, kp)kp, des = sift.detectAndCompute(gray, None)cv2.imwrite(&apos;sift_keypoints.jpg&apos;, img)cv2.imshow(&apos;sift_keypoints.jpg&apos;, img)cv2.waitKey(0)","path":"2019/08/05/opencv-python常用函数及介绍/","date":"08-05","excerpt":"","tags":[{"name":"python","slug":"python","permalink":"https://litianbo243.github.io/tags/python/"},{"name":"opencv","slug":"opencv","permalink":"https://litianbo243.github.io/tags/opencv/"}]},{"title":"python 多线程、多进程","text":"导读真正的并行执行多任务只能在多核CPU上实现，但是，由于任务数量远远多于CPU的核心数量，所以，操作系统也会自动把很多任务轮流调度到每个核心上执行。 对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。 有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。当然，真正地同时执行多线程需要多核CPU才可能实现。 我们前面编写的所有的Python程序，都是执行单任务的进程，也就是只有一个线程。 线程是最小的执行单元，而进程由至少一个线程组成。如何调度进程和线程，完全由操作系统决定，程序自己不能决定什么时候执行，执行多长时间。多进程和多线程的程序涉及到同步、数据共享的问题，编写起来更复杂。 多线程添加线程# 导入模块import threading# 获取已激活的线程数threading.active_count()# 查看所有线程信息threading.enumerate()# 查看现在正在运行的线程threading.current_thread()# 添加线程，threading.Thread()接收参数target代表这个线程要完成的任务def thread_job(): print(&apos;This is a thread of %s&apos; % threading.current_thread())def main(): thread = threading.Thread(target=thread_job,) # 定义线程 thread.start() # 让线程开始工作 if __name__ == &apos;__main__&apos;: main() join功能使用join对控制多个线程的执行顺序非常关键。join功能是等待调用线程完成再继续下面的操作。推荐如下这种1221的V型排布。示例代码def T1_job(): print(&quot;T1 start\\n&quot;) for i in range(10): time.sleep(0.1) print(&quot;T1 finish\\n&quot;)def T2_job(): print(&quot;T2 start\\n&quot;) print(&quot;T2 finish\\n&quot;)thread_1 = threading.Thread(target=T1_job, name=&apos;T1&apos;)thread_2 = threading.Thread(target=T2_job, name=&apos;T2&apos;)----------------------------------------------------------------------thread_1.start() # start T1thread_2.start() # start T2thread_2.join() # join for T2thread_1.join() # join for T1print(&quot;all done\\n&quot;)&quot;&quot;&quot;T1 startT2 startT2 finishT1 finishall done&quot;&quot;&quot; setDaemon功能setDaemon()方法。主线程A中，创建了子线程B，并且在主线程A中调用了B.setDaemon(),这个的意思是，把主线程A设置为守护线程，这时候，要是主线程A执行结束了，就不管子线程B是否完成,一并和主线程A退出.这就是setDaemon方法的含义，这基本和join是相反的。示例代码import threading import time class MyThread(threading.Thread): def __init__(self,id): threading.Thread.__init__(self) def run(self): time.sleep(5) print &quot;This is &quot; + self.getName() if __name__ == &quot;__main__&quot;: t1=MyThread(999) t1.setDaemon(True) t1.start() print &quot;I am the father thread.&quot; &quot;&quot;&quot;I am the father thread.&quot;&quot;&quot;# 可以看出，子线程t1中的内容并未打出。 线程锁Locklock在不同线程使用同一共享内存时，能够确保线程之间互不影响，使用lock的方法是， 在每个线程执行运算修改共享内存之前，执行lock.acquire()将共享内存上锁， 确保当前线程执行时，内存不会被其他线程访问，执行运算完毕后，使用lock.release()将锁打开， 保证其他的线程可以使用该共享内存。示例代码import threadingdef job1(): global A,lock lock.acquire() for i in range(10): A+=1 print(&apos;job1&apos;,A) lock.release()def job2(): global A,lock lock.acquire() for i in range(10): A+=10 print(&apos;job2&apos;,A) lock.release()if __name__== &apos;__main__&apos;: lock=threading.Lock() A=0 t1=threading.Thread(target=job1) t2=threading.Thread(target=job2) t1.start() t2.start() t1.join() t2.join() &quot;&quot;&quot;job1 1job1 2job1 3job1 4job1 5job1 6job1 7job1 8job1 9job1 10job2 20job2 30job2 40job2 50job2 60job2 70job2 80job2 90job2 100job2 110&quot;&quot;&quot; 其他常用操作import threading# 队列from queue import Queuedef job(i, q): print(&quot;i&apos;m thread &#123;&#125; &quot;.format(i)) i = i**2 # 将结果存入队列中，因为多线程函数不能有return q.put(i) q =Queue()threads = []for i in range(4): t = threading.Thread(target=job,args=(i, q)) t.start() # 将进程加入列表 threads.append(t)# 逐个joinfor thread in threads: thread.join() # 得到队列中元素，result# 测试后发现结果顺序有一定概率不准确。。。result = []for _ in range(4): result.append(q.get()) print(result) GILPython 的设计上, 有一个必要的环节, 就是 Global Interpreter Lock (GIL). 这个东西让 Python 还是一次性只能处理一个东西. GIL最大的问题就是Python的多线程程序并不能利用多核CPU的优势 （比如一个使用了多个线程的计算密集型程序只会在一个单CPU上面运行）。 所以Python的多线程就是假的。。。。 多进程Python提供多进程的原因很简单, 就是用来弥补 threading 的一些劣势, 比如在 GIL. 添加进程示例代码# 多进程import multiprocessing as mpdef job(a,d): print(&apos;aaaaa&apos;) print(mp.current_process())if __name__==&apos;__main__&apos;: p1 = mp.Process(target=job,args=(1,2)) print(mp.current_process()) p1.start() p1.join() 进程池进程池就是我们将所要运行的东西，放到池子里，Python会自行解决多进程的问题示例代码import multiprocessing as mpdef job(x): return x*x # 定义一个poolpool = mp.Pool()# 有了池子之后，就可以让池子对应某一个函数，我们向池子里丢数据，池子就会返回函数返回的值。Pool和之前的Process的不同点是丢向Pool的函数有返回值，而Process的没有返回值。# 接下来用map()获取结果，在map()中需要放入函数和需要迭代运算的值，然后它会自动分配给CPU核res = pool.map(job, range(10))print(res) 自定义核数量Pool默认大小是CPU的核数，我们也可以通过在Pool中传入processes参数即可自定义需要的核数量。示例代码def multicore(): pool = mp.Pool(processes=3) # 定义CPU核数量为3 res = pool.map(job, range(10)) print(res) apply_async示例代码import multiprocessing as mpdef job(x): return x*xpool = mp.Pool()multi_res = [pool.apply_async(job, (i,)) for i in range(10)]print([res.get() for res in multi_res])# 和map差不多 共享内存我们可以通过使用Value数据存储在一个共享的内存表中。import multiprocessing as mpvalue1 = mp.Value(&apos;i&apos;, 0) value2 = mp.Value(&apos;d&apos;, 3.14) 其中d和i参数用来设置数据类型的，d表示一个双精浮点类型，i表示一个带符号的整型。更多的形式请查看此表.| Type code | C Type | Python Type | Minimum size in bytes || --------- | ------------------ | ----------------- | --------------------- || `&apos;b&apos;` | signed char | int | 1 || `&apos;B&apos;` | unsigned char | int | 1 || `&apos;u&apos;` | Py_UNICODE | Unicode character | 2 || `&apos;h&apos;` | signed short | int | 2 || `&apos;H&apos;` | unsigned short | int | 2 || `&apos;i&apos;` | signed int | int | 2 || `&apos;I&apos;` | unsigned int | int | 2 || `&apos;l&apos;` | signed long | int | 4 || `&apos;L&apos;` | unsigned long | int | 4 || `&apos;q&apos;` | signed long long | int | 8 || `&apos;Q&apos;` | unsigned long long | int | 8 || `&apos;f&apos;` | float | float | 4 || `&apos;d&apos;` | double | float | 8 | 在Python的mutiprocessing中，有还有一个Array类，可以和共享内存交互，来实现在进程之间共享数据。array = mp.Array(&apos;i&apos;, [1, 2, 3, 4]) 这里的Array和numpy中的不同，它只能是一维的，不能是多维的。同样和Value 一样，需要定义数据形式，否则会报错。 进程锁为了解决不同进程抢共享资源的问题，我们可以用加进程锁来解决。示例代码import multiprocessing as mpimport timedef job(v, num, l): l.acquire() # 锁住 for _ in range(5): time.sleep(0.1) v.value += num # 获取共享内存 print(v.value) l.release() # 释放def multicore(): l = mp.Lock() # 定义一个进程锁 v = mp.Value(&apos;i&apos;, 0) # 定义共享内存 p1 = mp.Process(target=job, args=(v,1,l)) # 需要将lock传入 p2 = mp.Process(target=job, args=(v,3,l)) p1.start() p2.start() p1.join() p2.join()if __name__ == &apos;__main__&apos;: multicore() &quot;&quot;&quot;12345811141720&quot;&quot;&quot;# 显然，进程锁保证了进程p1的完整运行，然后才进行了进程p2的运行","path":"2019/08/05/python 多线程、多进程/","date":"08-05","excerpt":"","tags":[{"name":"python","slug":"python","permalink":"https://litianbo243.github.io/tags/python/"},{"name":"多线程","slug":"多线程","permalink":"https://litianbo243.github.io/tags/多线程/"},{"name":"多进程","slug":"多进程","permalink":"https://litianbo243.github.io/tags/多进程/"}]},{"title":"tkinter使用指南","text":"Tkinter模块元素简要说明 tkinter类 元素 Button 按钮 Canvas 画布 Checkbutton 复选框 Entry 单行文本框 Frame 框架 Label 标签 LabelFrame 容器控件 Listbox 列表框 Menu 菜单 Menubutton 菜单按钮 Message 消息框 OptionMenu 选择菜单 PanedWindow 窗口布局管理 Radiobutton 单选框 Scale 进度条 Scrollbar 滚动条 Spinbox 输入控件 Text 多行文本框 Toplevel 顶层 messageBox 消息框 Tkinter支持16个核心的窗口部件，这个16个核心窗口部件类简要描述如下：Button：一个简单的按钮，用来执行一个命令或别的操作。Canvas：组织图形。这个部件可以用来绘制图表和图，创建图形编辑器，实现定制窗口部件。Checkbutton：代表一个变量，它有两个不同的值。点击这个按钮将会在这两个值间切换。Entry：文本输入域。Frame：一个容器窗口部件。帧可以有边框和背景，当创建一个应用程序或dialog(对话）版面时，帧被用来组织其它的窗口部件。Label：显示一个文本或图象。Listbox：显示供选方案的一个列表。listbox能够被配置来得到radiobutton或checklist的行为。Menu：菜单条。用来实现下拉和弹出式菜单。Menubutton：菜单按钮。用来实现下拉式菜单。Message：显示一文本。类似label窗口部件，但是能够自动地调整文本到给定的宽度或比率。Radiobutton：代表一个变量，它可以有多个值中的一个。点击它将为这个变量设置值，并且清除与这同一变量相关的其它radiobutton。Scale：允许你通过滑块来设置一数字值。Scrollbar：为配合使用canvas, entry, listbox, and text窗口部件的标准滚动条。Text：格式化文本显示。允许你用不同的样式和属性来显示和编辑文本。同时支持内嵌图象和窗口。Toplevel：一个容器窗口部件，作为一个单独的、最上面的窗口显示。messageBox：消息框，用于显示你应用程序的消息框。(Python2中为tkMessagebox)注意在Tkinter中窗口部件类没有分级；所有的窗口部件类在树中都是兄弟关系。所有这些窗口部件提供了Misc和几何管理方法、配置管理方法和部件自己定义的另外的方法。此外，Toplevel类也提供窗口管理接口。这意味一个典型的窗口部件类提供了大约150种方法。 动手实践学习创建主窗口及Label部件的创建和使用示例代码import tkinter as tk # 使用Tkinter前需要先导入 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第4步，在图形界面上设定标签l = tk.Label(window, text=&apos;你好！this is Tkinter&apos;, bg=&apos;green&apos;, font=(&apos;Arial&apos;, 12), width=30, height=2)# 说明： bg为背景，font为字体，width为长，height为高，这里的长和高是字符的长和高，比如height=2,就是标签有2个字符这么高 # 第5步，放置标签l.pack() # Label内容content区域放置位置，自动调节尺寸# 放置lable的方法有：1）l.pack(); 2)l.place(); # 第6步，主窗口循环显示window.mainloop()# 注意，loop因为是循环的意思，window.mainloop就会让window不断的刷新，如果没有mainloop,就是一个静态的window,传入进去的值就不会有循环，mainloop就相当于一个很大的while循环，有个while，每点击一次就会更新一次，所以我们必须要有循环 测试效果 Button窗口部件示例代码import tkinter as tk # 使用Tkinter前需要先导入 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第4步，在图形界面上设定标签var = tk.StringVar() # 将label标签的内容设置为字符类型，用var来接收hit_me函数的传出内容用以显示在标签上l = tk.Label(window, textvariable=var, bg=&apos;green&apos;, fg=&apos;white&apos;, font=(&apos;Arial&apos;, 12), width=30, height=2)# 说明： bg为背景，fg为字体颜色，font为字体，width为长，height为高，这里的长和高是字符的长和高，比如height=2,就是标签有2个字符这么高l.pack() # 定义一个函数功能（内容自己自由编写），供点击Button按键时调用，调用命令参数command=函数名on_hit = Falsedef hit_me(): global on_hit if on_hit == False: on_hit = True var.set(&apos;you hit me&apos;) else: on_hit = False var.set(&apos;&apos;) # 第5步，在窗口界面设置放置Button按键b = tk.Button(window, text=&apos;hit me&apos;, font=(&apos;Arial&apos;, 12), width=10, height=1, command=hit_me)b.pack() # 第6步，主窗口循环显示window.mainloop() 测试效果 Entry窗口部件示例代码import tkinter as tk # 使用Tkinter前需要先导入 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第4步，在图形界面上设定输入框控件entry并放置控件e1 = tk.Entry(window, show=&apos;*&apos;, font=(&apos;Arial&apos;, 14)) # 显示成密文形式e2 = tk.Entry(window, show=None, font=(&apos;Arial&apos;, 14)) # 显示成明文形式e1.pack()e2.pack() # 第5步，主窗口循环显示window.mainloop() 测试效果 Text窗口部件示例代码import tkinter as tk # 使用Tkinter前需要先导入 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第4步，在图形界面上设定输入框控件entry框并放置e = tk.Entry(window, show = None)#显示成明文形式e.pack() # 第5步，定义两个触发事件时的函数insert_point和insert_end（注意：因为Python的执行顺序是从上往下，所以函数一定要放在按钮的上面）def insert_point(): # 在鼠标焦点处插入输入内容 var = e.get() t.insert(&apos;insert&apos;, var)def insert_end(): # 在文本框内容最后接着插入输入内容 var = e.get() t.insert(&apos;end&apos;, var) # 第6步，创建并放置两个按钮分别触发两种情况b1 = tk.Button(window, text=&apos;insert point&apos;, width=10, height=2, command=insert_point)b1.pack()b2 = tk.Button(window, text=&apos;insert end&apos;, width=10, height=2, command=insert_end)b2.pack() # 第7步，创建并放置一个多行文本框text用以显示，指定height=3为文本框是三个字符高度t = tk.Text(window, height=3)t.pack() # 第8步，主窗口循环显示window.mainloop() 测试效果 Listbox窗口部件示例代码import tkinter as tk # 使用Tkinter前需要先导入 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第4步，在图形界面上创建一个标签label用以显示并放置var1 = tk.StringVar() # 创建变量，用var1用来接收鼠标点击具体选项的内容l = tk.Label(window, bg=&apos;green&apos;, fg=&apos;yellow&apos;,font=(&apos;Arial&apos;, 12), width=10, textvariable=var1)l.pack() # 第6步，创建一个方法用于按钮的点击事件def print_selection(): value = lb.get(lb.curselection()) # 获取当前选中的文本 var1.set(value) # 为label设置值 # 第5步，创建一个按钮并放置，点击按钮调用print_selection函数b1 = tk.Button(window, text=&apos;print selection&apos;, width=15, height=2, command=print_selection)b1.pack() # 第7步，创建Listbox并为其添加内容var2 = tk.StringVar()var2.set((1,2,3,4)) # 为变量var2设置值# 创建Listboxlb = tk.Listbox(window, listvariable=var2) #将var2的值赋给Listbox# 创建一个list并将值循环添加到Listbox控件中list_items = [11,22,33,44]for item in list_items: lb.insert(&apos;end&apos;, item) # 从最后一个位置开始加入值lb.insert(1, &apos;first&apos;) # 在第一个位置加入&apos;first&apos;字符lb.insert(2, &apos;second&apos;) # 在第二个位置加入&apos;second&apos;字符lb.delete(2) # 删除第二个位置的字符lb.pack() # 第8步，主窗口循环显示window.mainloop() 测试效果 Radiobutton窗口控件示例代码import tkinter as tk # 使用Tkinter前需要先导入 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第4步，在图形界面上创建一个标签label用以显示并放置var = tk.StringVar() # 定义一个var用来将radiobutton的值和Label的值联系在一起.l = tk.Label(window, bg=&apos;yellow&apos;, width=20, text=&apos;empty&apos;)l.pack() # 第6步，定义选项触发函数功能def print_selection(): l.config(text=&apos;you have selected &apos; + var.get()) # 第5步，创建三个radiobutton选项，其中variable=var, value=&apos;A&apos;的意思就是，当我们鼠标选中了其中一个选项，把value的值A放到变量var中，然后赋值给variabler1 = tk.Radiobutton(window, text=&apos;Option A&apos;, variable=var, value=&apos;A&apos;, command=print_selection)r1.pack()r2 = tk.Radiobutton(window, text=&apos;Option B&apos;, variable=var, value=&apos;B&apos;, command=print_selection)r2.pack()r3 = tk.Radiobutton(window, text=&apos;Option C&apos;, variable=var, value=&apos;C&apos;, command=print_selection)r3.pack() # 第7步，主窗口循环显示window.mainloop() 测试效果 Checkbutton窗口部件示例代码import tkinter as tk # 使用Tkinter前需要先导入 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第4步，在图形界面上创建一个标签label用以显示并放置l = tk.Label(window, bg=&apos;yellow&apos;, width=20, text=&apos;empty&apos;)l.pack() # 第6步，定义触发函数功能def print_selection(): if (var1.get() == 1) &amp; (var2.get() == 0): # 如果选中第一个选项，未选中第二个选项 l.config(text=&apos;I love only Python &apos;) elif (var1.get() == 0) &amp; (var2.get() == 1): # 如果选中第二个选项，未选中第一个选项 l.config(text=&apos;I love only C++&apos;) elif (var1.get() == 0) &amp; (var2.get() == 0): # 如果两个选项都未选中 l.config(text=&apos;I do not love either&apos;) else: l.config(text=&apos;I love both&apos;) # 如果两个选项都选中 # 第5步，定义两个Checkbutton选项并放置var1 = tk.IntVar() # 定义var1和var2整型变量用来存放选择行为返回值var2 = tk.IntVar()c1 = tk.Checkbutton(window, text=&apos;Python&apos;,variable=var1, onvalue=1, offvalue=0, command=print_selection) # 传值原理类似于radiobutton部件c1.pack()c2 = tk.Checkbutton(window, text=&apos;C++&apos;,variable=var2, onvalue=1, offvalue=0, command=print_selection)c2.pack() # 第7步，主窗口循环显示window.mainloop() 测试效果 Scale窗口部件示例代码import tkinter as tk # 使用Tkinter前需要先导入 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第4步，在图形界面上创建一个标签label用以显示并放置l = tk.Label(window, bg=&apos;green&apos;, fg=&apos;white&apos;, width=20, text=&apos;empty&apos;)l.pack() # 第6步，定义一个触发函数功能def print_selection(v): l.config(text=&apos;you have selected &apos; + v)# 第5步，创建一个尺度滑条，长度200字符，从0开始10结束，以2为刻度，精度为0.01，触发调用print_selection函数s = tk.Scale(window, label=&apos;try me&apos;, from_=0, to=10, orient=tk.HORIZONTAL, length=200, showvalue=0,tickinterval=2, resolution=0.01, command=print_selection)s.pack() # 第7步，主窗口循环显示window.mainloop() 测试效果 Canvas窗口部件示例代码from PIL import Imagefrom PIL import ImageTkimport tkinter as tk # 使用Tkinter前需要先导入# 第1步，实例化object，建立窗口windowwindow = tk.Tk()# 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;)# 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x# 第4步，在图形界面上创建 500 * 200 大小的画布并放置各种元素canvas = tk.Canvas(window, bg=&apos;green&apos;, height=200, width=500)# 说明图片位置，并导入图片到画布上image_file = Image.open(&quot;/home/ltb/timg.jpeg&quot;).resize((500, 300))image_file = ImageTk.PhotoImage(image_file)# image_file = tk.PhotoImage(file=&apos;/home/ltb/timg.gif&apos;) # 图片位置（相对路径，与.py文件同一文件夹下，也可以用绝对路径，需要给定图片具体绝对路径）image = canvas.create_image(250, 0, anchor=&apos;n&apos;, image=image_file) # 图片锚定点（n图片顶端的中间点位置）放在画布（250,0）坐标处# 定义多边形参数，然后在画布上画出指定图形x0, y0, x1, y1 = 100, 100, 150, 150line = canvas.create_line(x0 - 50, y0 - 50, x1 - 50, y1 - 50) # 画直线oval = canvas.create_oval(x0 + 120, y0 + 50, x1 + 120, y1 + 50, fill=&apos;white&apos;) # 画圆 用黄色填充arc = canvas.create_arc(x0, y0 + 50, x1, y1 + 50, start=0, extent=180) # 画扇形 从0度打开收到180度结束rect = canvas.create_rectangle(330, 30, 330 + 20, 30 + 20) # 画矩形正方形canvas.pack()# 第6步，触发函数，用来一定指定图形def moveit(): canvas.move(rect, 2, 2) # 移动正方形rect（也可以改成其他图形名字用以移动一起图形、元素），按每次（x=2, y=2）步长进行移动# 第5步，定义一个按钮用来移动指定图形的在画布上的位置b = tk.Button(window, text=&apos;move item&apos;, command=moveit).pack()# 第7步，主窗口循环显示window.mainloop()# 在一个类中创建Canvas需要把要显示再画布上的图片声明为全局变量，不然画布无法显示图片。 图像锚定点位置参数图测试效果 Menu窗口部件示例代码import tkinter as tk # 使用Tkinter前需要先导入# 第1步，实例化object，建立窗口windowwindow = tk.Tk()# 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;)# 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x# 第4步，在图形界面上创建一个标签用以显示内容并放置l = tk.Label(window, text=&apos; &apos;, bg=&apos;green&apos;)l.pack()# 第10步，定义一个函数功能，用来代表菜单选项的功能，这里为了操作简单，定义的功能比较简单counter = 0def do_job(): global counter l.config(text=&apos;do &apos; + str(counter)) counter += 1# 第5步，创建一个菜单栏，这里我们可以把他理解成一个容器，在窗口的上方menubar = tk.Menu(window)# 第6步，创建一个File菜单项（默认不下拉，下拉内容包括New，Open，Save，Exit功能项）filemenu = tk.Menu(menubar, tearoff=0)# 将上面定义的空菜单命名为File，放在菜单栏中，就是装入那个容器中menubar.add_cascade(label=&apos;File&apos;, menu=filemenu)# 在File中加入New、Open、Save等小菜单，即我们平时看到的下拉菜单，每一个小菜单对应命令操作。filemenu.add_command(label=&apos;New&apos;, command=do_job)filemenu.add_command(label=&apos;Open&apos;, command=do_job)filemenu.add_command(label=&apos;Save&apos;, command=do_job)filemenu.add_separator() # 添加一条分隔线filemenu.add_command(label=&apos;Exit&apos;, command=window.quit) # 用tkinter里面自带的quit()函数# 第7步，创建一个Edit菜单项（默认不下拉，下拉内容包括Cut，Copy，Paste功能项）editmenu = tk.Menu(menubar, tearoff=0)# 将上面定义的空菜单命名为 Edit，放在菜单栏中，就是装入那个容器中menubar.add_cascade(label=&apos;Edit&apos;, menu=editmenu)# 同样的在 Edit 中加入Cut、Copy、Paste等小命令功能单元，如果点击这些单元, 就会触发do_job的功能editmenu.add_command(label=&apos;Cut&apos;, command=do_job)editmenu.add_command(label=&apos;Copy&apos;, command=do_job)editmenu.add_command(label=&apos;Paste&apos;, command=do_job)# 第8步，创建第二级菜单，即菜单项里面的菜单submenu = tk.Menu(filemenu) # 和上面定义菜单一样，不过此处实在File上创建一个空的菜单filemenu.add_cascade(label=&apos;Import&apos;, menu=submenu, underline=0) # 给放入的菜单submenu命名为Import# 第9步，创建第三级菜单命令，即菜单项里面的菜单项里面的菜单命令（有点拗口，笑~~~）submenu.add_command(label=&apos;Submenu_1&apos;, command=do_job) # 这里和上面创建原理也一样，在Import菜单项中加入一个小菜单命令Submenu_1# 第11步，创建菜单栏完成后，配置让菜单栏menubar显示出来window.config(menu=menubar)# 第12步，主窗口循环显示window.mainloop() 测试效果 Frame窗口部件示例代码import tkinter as tk # 使用Tkinter前需要先导入 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第4步，在图形界面上创建一个标签用以显示内容并放置tk.Label(window, text=&apos;on the window&apos;, bg=&apos;red&apos;, font=(&apos;Arial&apos;, 16)).pack() # 和前面部件分开创建和放置不同，其实可以创建和放置一步完成 # 第5步，创建一个主frame，长在主window窗口上frame = tk.Frame(window)frame.pack() # 第6步，创建第二层框架frame，长在主框架frame上面frame_l = tk.Frame(frame)# 第二层frame，左frame，长在主frame上frame_r = tk.Frame(frame)# 第二层frame，右frame，长在主frame上frame_l.pack(side=&apos;left&apos;)frame_r.pack(side=&apos;right&apos;) # 第7步，创建三组标签，为第二层frame上面的内容，分为左区域和右区域，用不同颜色标识tk.Label(frame_l, text=&apos;on the frame_l1&apos;, bg=&apos;green&apos;).pack()tk.Label(frame_l, text=&apos;on the frame_l2&apos;, bg=&apos;green&apos;).pack()tk.Label(frame_l, text=&apos;on the frame_l3&apos;, bg=&apos;green&apos;).pack()tk.Label(frame_r, text=&apos;on the frame_r1&apos;, bg=&apos;yellow&apos;).pack()tk.Label(frame_r, text=&apos;on the frame_r2&apos;, bg=&apos;yellow&apos;).pack()tk.Label(frame_r, text=&apos;on the frame_r3&apos;, bg=&apos;yellow&apos;).pack() # 第8步，主窗口循环显示window.mainloop() 测试效果 messageBox窗口部件示例代码import tkinter as tk # 使用Tkinter前需要先导入import tkinter.messagebox # 要使用messagebox先要导入模块 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第5步，定义触发函数功能def hit_me(): tkinter.messagebox.showinfo(title=&apos;Hi&apos;, message=&apos;你好！&apos;) # 提示信息对话窗 # tkinter.messagebox.showwarning(title=&apos;Hi&apos;, message=&apos;有警告！&apos;) # 提出警告对话窗 # tkinter.messagebox.showerror(title=&apos;Hi&apos;, message=&apos;出错了！&apos;) # 提出错误对话窗 # print(tkinter.messagebox.askquestion(title=&apos;Hi&apos;, message=&apos;你好！&apos;)) # 询问选择对话窗return &apos;yes&apos;, &apos;no&apos; # print(tkinter.messagebox.askyesno(title=&apos;Hi&apos;, message=&apos;你好！&apos;)) # return &apos;True&apos;, &apos;False&apos; # print(tkinter.messagebox.askokcancel(title=&apos;Hi&apos;, message=&apos;你好！&apos;)) # return &apos;True&apos;, &apos;False&apos; # 第4步，在图形界面上创建一个标签用以显示内容并放置tk.Button(window, text=&apos;hit me&apos;, bg=&apos;green&apos;, font=(&apos;Arial&apos;, 14), command=hit_me).pack() # 第6步，主窗口循环显示window.mainloop() 测试效果 窗口部件三种放置方式 pack/grid/placepack按上下左右的方式排列 示例代码import tkinter as tk # 使用Tkinter前需要先导入# 第1步，实例化object，建立窗口windowwindow = tk.Tk()# 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;)# 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x# 第4步，pack 放置方法tk.Label(window, text=&apos;P&apos;, fg=&apos;red&apos;).pack(side=&apos;top&apos;) # 上tk.Label(window, text=&apos;P&apos;, fg=&apos;red&apos;).pack(side=&apos;bottom&apos;) # 下tk.Label(window, text=&apos;P&apos;, fg=&apos;red&apos;).pack(side=&apos;left&apos;) # 左tk.Label(window, text=&apos;P&apos;, fg=&apos;red&apos;).pack(side=&apos;right&apos;) # 右# 第5步，主窗口循环显示window.mainloop() 测试效果 grid所有的内容会被放在这些规律的方格中 示例代码import tkinter as tk # 使用Tkinter前需要先导入# 第1步，实例化object，建立窗口windowwindow = tk.Tk()# 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;)# 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x# 第4步，grid 放置方法for i in range(3): for j in range(3): tk.Label(window, text=1).grid(row=i, column=j, padx=10, pady=10, ipadx=10, ipady=10)# 第5步，主窗口循环显示window.mainloop() 测试效果 place给精确的坐标来定位 示例代码import tkinter as tk # 使用Tkinter前需要先导入 # 第1步，实例化object，建立窗口windowwindow = tk.Tk() # 第2步，给窗口的可视化起名字window.title(&apos;My Window&apos;) # 第3步，设定窗口的大小(长 * 宽)window.geometry(&apos;500x300&apos;) # 这里的乘是小x # 第4步，place 放置方法（精准的放置到指定坐标点的位置上）tk.Label(window, text=&apos;Pl&apos;, font=(&apos;Arial&apos;, 20), ).place(x=50, y=100, anchor=&apos;nw&apos;) # 第5步，主窗口循环显示window.mainloop() 测试效果 Tkinter布局之pack我们使用pack函数的时候，默认先使用的放在上面，然后依次向下排，它会给我们的部件一个自认为合适的位置和大小，这是默认方式。 side 按钮停靠的位置left：左top：上right：右bottom：下 fill 填充x：水平方向填充y：竖直方向填充both：水平和竖直方向填充none：不填充 expand 扩展yes：扩展整个空白区no：不扩展 部件消除示例代码# 销毁上一个布局的frame及其部件# 先销毁frame下的部件，再销毁framefor widget in self.last_frame.winfo_children(): widget.destroy()self.last_frame.destroy() 多线程按钮部件示例代码# 打开摄像头button_open_camera = tk.Button(self.frame_3, text=&quot;打开摄像头&quot;, height=3, command=self.camera_threading)button_open_camera.pack(side=&quot;top&quot;, fill=&quot;x&quot;)# 关闭摄像头button_close_camera = tk.Button(self.frame_3, text=&quot;关闭摄像头&quot;, height=3, command=self.close_camera)button_close_camera.pack(side=&quot;top&quot;, fill=&quot;x&quot;)# 线程函数def camera_threading(self): self.thread = threading.Thread(target=self.open_camera) self.thread.setDaemon(True) self.thread.start() # 打开摄像头函数，被线程函数调用def open_camera(self): try: cap = cv2.VideoCapture(0) self.thread_flag = True while(self.thread_flag): ret, frame = cap.read() cv2.imshow(&quot;video&quot;, frame) cv2.imwrite(self.temporary_path, frame) if ret != True: break c = cv2.waitKey(60) if c == 27: break cv2.destroyAllWindows() cap.release() except: messagebox.showerror(title=&quot;读取视频出错&quot;, message=&quot;读取出错或未检测到usb摄像头&quot;) # 关闭摄像头函数，用全局变量控制def close_camera(self): self.thread_flag = False os._exit(0) 第一个Tkinter程序from PIL import Imagefrom PIL import ImageTkimport tkinter as tkfrom tkinter import filedialog, messageboximport threadingimport cv2import osimport shutilfrom datetime import datetimeclass AppUI(): def __init__(self): # 全局变量 self.first = True self.last_frame = None self.thread_flag = False self.temporary_path = &quot;/mnt/tkinter_pj1/temporary_picture.jpg&quot; self.database = &quot;/mnt/tkinter_pj1/Images/all&quot; # 存线程实例 self.thread_obj = [] # 创建窗口 self.window = tk.Tk() self.window.title(&quot;手术器械自动识别系统&quot;) self.window.geometry(&quot;500x200&quot;) # 参数 self.main_menu() self.main_layout() self.window.mainloop() # 主菜单 def main_menu(self): self.menu_bar = tk.Menu(self.window) self.start_menu = tk.Menu(self.menu_bar, tearoff=0) self.menu_bar.add_cascade(label=&quot;开始&quot;, menu=self.start_menu) self.start_menu.add_command(label=&quot;首页&quot;, command=self.main_layout) self.start_menu.add_command(label=&quot;单张图像预测&quot;, command=self.layout_2) self.start_menu.add_command(label=&quot;实时图像检测&quot;, command=self.layout_3) self.start_menu.add_command(label=&quot;图像采集&quot;, command=self.layout_4) self.window.config(menu=self.menu_bar) def main_layout(self): if self.first == False: # 销毁上一个layout的frame及其控件 for widget in self.last_frame.winfo_children(): widget.destroy() self.last_frame.destroy() self.window.geometry(&quot;500x200&quot;) self.first = False self.main_frame = tk.Frame(self.window) self.main_frame.pack(expand=&quot;yes&quot;, fill=&quot;both&quot;) self.last_frame = self.main_frame label_main_frame_title = tk.Label(self.main_frame, text=&apos;欢迎来到手术器械自动识别系统&apos;, font=(&apos;Arial&apos;, 17), height=3) label_main_frame_title.pack(side=&quot;top&quot;, fill=&quot;both&quot;) # 三个button button_single_predict = tk.Button(self.main_frame, text=&apos;单张图像识别&apos;, font=(&apos;Arial&apos;, 15), width=10, height=3, command=self.layout_2) button_single_predict.pack(side=&quot;left&quot;, expand=&quot;yes&quot;, fill=&quot;both&quot;) button_realtime_predict = tk.Button(self.main_frame, text=&quot;实时图像识别&quot;, font=(&apos;Arial&apos;, 15), width=10, height=3, command=self.layout_3) button_realtime_predict.pack(side=&quot;left&quot;, expand=&quot;yes&quot;, fill=&quot;both&quot;) button_catch_picture = tk.Button(self.main_frame, text=&quot;图像采集&quot;, font=(&apos;Arial&apos;, 15), width=10, height=3, command=self.layout_4) button_catch_picture.pack(side=&quot;left&quot;, expand=&quot;yes&quot;, fill=&quot;both&quot;) def layout_2(self): # 销毁上一个layout的frame及其控件 for widget in self.last_frame.winfo_children(): widget.destroy() self.last_frame.destroy() self.window.geometry(&quot;800x555&quot;) # 创建frame_2 self.frame_2 = tk.Frame(self.window) self.frame_2.pack(expand=&quot;yes&quot;, fill=&quot;both&quot;) self.last_frame = self.frame_2 label_frame_2_title = tk.Label(self.frame_2, text=&apos;单张图像识别&apos;, font=(&apos;Arial&apos;, 15), height=3, width=30) label_frame_2_title.pack(side=&quot;top&quot;) # 显示图片 self.canvas = tk.Canvas(self.frame_2, height=480, width=640) self.canvas.pack(side=&quot;left&quot;) # 选择路径 button_picture_path = tk.Button(self.frame_2, text=&quot;选择路径&quot;, height=3, command=self.select_picture_path) button_picture_path.pack(side=&quot;top&quot;, fill=&quot;x&quot;) # 路径 label_picture_path = tk.Label(self.frame_2, text=&quot;图片路径&quot;, height=3) label_picture_path.pack(side=&quot;top&quot;, fill=&quot;x&quot;) self.entry_picture_path = tk.Entry(self.frame_2, show=None) self.entry_picture_path.pack(side=&quot;top&quot;, fill=&quot;x&quot;) self.entry_predict_result = tk.Entry(self.frame_2, show=None) self.entry_predict_result.pack(side=&quot;bottom&quot;, fill=&quot;x&quot;) button_predict_picture = tk.Button(self.frame_2, text=&quot;图片预测&quot;, height=3, command=self.predict_picture) button_predict_picture.pack(side=&quot;bottom&quot;, fill=&quot;x&quot;) def layout_3(self): # 销毁上一个layout的frame及其控件 for widget in self.last_frame.winfo_children(): widget.destroy() self.last_frame.destroy() self.window.geometry(&quot;800x555&quot;) self.frame_3 = tk.Frame(self.window) self.frame_3.pack(expand=&quot;yes&quot;, fill=&quot;both&quot;) self.last_frame = self.frame_3 label_frame_3_title = tk.Label(self.frame_3, text=&apos;实时图像识别&apos;, font=(&apos;Arial&apos;, 15), height=3, width=30) label_frame_3_title.pack(side=&quot;top&quot;) # 显示图片 self.canvas = tk.Canvas(self.frame_3, height=480, width=640) self.canvas.pack(side=&quot;left&quot;) # 打开摄像头 button_open_camera = tk.Button(self.frame_3, text=&quot;打开摄像头&quot;, height=3, command=self.camera_threading) button_open_camera.pack(side=&quot;top&quot;, fill=&quot;x&quot;) # 关闭摄像头 button_close_camera = tk.Button(self.frame_3, text=&quot;关闭摄像头&quot;, height=3, command=self.close_camera) button_close_camera.pack(side=&quot;top&quot;, fill=&quot;x&quot;) self.entry_realtime_result = tk.Entry(self.frame_3, show=None) self.entry_realtime_result.pack(side=&quot;bottom&quot;, fill=&quot;x&quot;) button_realtime_prediction = tk.Button(self.frame_3, text=&quot;实时检测&quot;, height=3, command=self.realtime_prediction) button_realtime_prediction.pack(side=&quot;bottom&quot;, fill=&quot;x&quot;) def layout_4(self): # 销毁上一个layout的frame及其控件 for widget in self.last_frame.winfo_children(): widget.destroy() self.last_frame.destroy() self.window.geometry(&quot;800x555&quot;) self.frame_4 = tk.Frame(self.window) self.frame_4.pack(expand=&quot;yes&quot;, fill=&quot;both&quot;) self.last_frame = self.frame_4 label = tk.Label(self.frame_4, text=&apos;图像采集&apos;, font=(&apos;Arial&apos;, 15), width=30, height=3) label.pack(side=&quot;top&quot;) # 显示图片 self.canvas = tk.Canvas(self.frame_4, height=480, width=640) self.canvas.pack(side=&quot;left&quot;) # 打开摄像头 button_open_camera = tk.Button(self.frame_4, text=&quot;打开摄像头&quot;, height=3, command=self.camera_threading) button_open_camera.pack(side=&quot;top&quot;, fill=&quot;x&quot;) # 关闭摄像头 button_close_camera = tk.Button(self.frame_4, text=&quot;关闭摄像头&quot;, height=3, command=self.close_camera) button_close_camera.pack(side=&quot;top&quot;, fill=&quot;x&quot;) button_picture_capture = tk.Button(self.frame_4, text=&quot;图片采集&quot;, height=3, command=self.picture_capture) button_picture_capture.pack(side=&quot;bottom&quot;, fill=&quot;x&quot;) self.entry_input_label = tk.Entry(self.frame_4, show=None, text=&quot;&quot;) self.entry_input_label.pack(side=&quot;bottom&quot;, fill=&quot;x&quot;) label_input_label = tk.Label(self.frame_4, text=&quot;输入类别&quot;) label_input_label.pack(side=&quot;bottom&quot;, fill=&quot;x&quot;) def select_picture_path(self): self.pic_path = filedialog.askopenfilename() self.entry_picture_path.delete(0, &quot;end&quot;) self.entry_picture_path.insert(0, self.pic_path) self.show_picture(self.pic_path) def show_picture(self, path): try: self.picture = Image.open(path).resize((640, 480)) self.picture = ImageTk.PhotoImage(self.picture) self.canvas.create_image(0, 0, anchor=&quot;nw&quot;, image=self.picture) except: messagebox.showerror(title=&quot;路径错误&quot;, message=&quot;路径输入错误或选中文件不是图片&quot;) def open_camera(self): try: cap = cv2.VideoCapture(0) self.thread_flag = True while(self.thread_flag): ret, frame = cap.read() cv2.imshow(&quot;video&quot;, frame) cv2.imwrite(self.temporary_path, frame) if ret != True: break c = cv2.waitKey(60) if c == 27: break cv2.destroyAllWindows() cap.release() except: messagebox.showerror(title=&quot;读取视频出错&quot;, message=&quot;读取出错或未检测到usb摄像头&quot;) def close_camera(self): self.thread_flag = False os._exit(0) def realtime_prediction(self): self.show_picture(self.temporary_path) pass def predict_picture(self): self.entry_predict_result.delete(0, &quot;end&quot;) self.entry_predict_result.insert(0, 0) pass def picture_capture(self): content = self.entry_input_label.get() if len(content) &gt; 0: time_stamp = &apos;&#123;:%Y-%m-%d-%H-%M-%S&#125;&apos;.format(datetime.now()) sub_class_folder = os.path.join(self.database, str(self.entry_input_label.get())) if not os.path.exists(sub_class_folder): os.mkdir(sub_class_folder) image_store_path = os.path.join(sub_class_folder, &apos;&#123;&#125;_&#123;&#125;.jpg&apos;.format(str(self.entry_input_label.get()), time_stamp)) if self.thread_flag: self.show_picture(self.temporary_path) shutil.copy(self.temporary_path, image_store_path) messagebox.showinfo(title=&quot;成功&quot;, message=&quot;成功保存图片&quot;) else: messagebox.showwarning(title=&quot;警告&quot;, message=&quot;未打开摄像头&quot;) else: messagebox.showwarning(title=&quot;警告&quot;, message=&quot;请输入类别号&quot;) def camera_threading(self): self.thread = threading.Thread(target=self.open_camera) self.thread.setDaemon(True) self.thread.start()if __name__ == &quot;__main__&quot;: app = AppUI()pyinstaller 打包 # 安装pyinstallsudo pip3 install pyinstaller# 切换到需要打包的目录，执行pyinstaller -F -W GUI.py# -F: 只生成耦合可执行的文件# -W: 表示窗口，无控制台# 生成的可执行文件就在目录dist中# 修改图标-i icon.ico 或者 --icon.ico# 图片放在需要打包的文件同目录中 参考本文参考以下网站、博客http://www.cnblogs.com/shwee/p/9427975.htmlhttps://blog.csdn.net/yingshukun/article/details/78838395http://www.cnblogs.com/kongzhagen/p/6144588.html","path":"2019/08/05/Tkinter使用指南/","date":"08-05","excerpt":"","tags":[{"name":"python","slug":"python","permalink":"https://litianbo243.github.io/tags/python/"},{"name":"tkinter","slug":"tkinter","permalink":"https://litianbo243.github.io/tags/tkinter/"}]},{"title":"pytorch常用函数","text":"Tensor基本操作Tensor的基本数据类型torch.Tensor是一种包含单一数据类型元素的多维矩阵。Torch定义了七种CPU tensor类型和八种GPU tensor类型： Data type CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor import torchimport numpy as npt = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])print(t.dtype)print(t) Tensor数据类型的转化import torchimport numpy as np# 默认数据类型为float32t = torch.Tensor(3, 5)print(t.dtype, &quot;\\n&quot;)# t.double()将该tensor投射为double类型double_t = t.double()print(double_t.dtype)# t.float()将该tensor投射为float类型float_t = t.float()print(float_t.dtype)# t.half()将tensor投射为半精度浮点类型half_t = t.half()print(half_t.dtype)# t.long() 将tensor投射为long类型long_t = t.long()print(long_t.dtype)# t.int()将该tensor投射为int类型int_t = t.int()print(int_t.dtype)# t.short()将该tensor投射为short类型short_t = t.short()print(short_t.dtype)# t.char()将该tensor投射为char类型char_t = t.char()print(char_t.dtype)# t.byte()将该tensor投射为byte类型byte_t = t.byte()print(byte_t.dtype)# t.cuda()将tensor投射为gputensorgpu_t = t.cuda()print(gpu_t.dtype) Tensor与numpy的相互转换import torchimport numpy as npa = np.array([1, 2, 3])t = torch.from_numpy(a)print(t)t[0] = -1a = t.numpy()print(a) Tensor拼接操作import torchimport numpy as npx = torch.randn(2, 3)print(x)x0 = torch.cat((x, x, x), 0)print(x0)x1 = torch.cat((x, x, x), 1)print(x1) Tensor进行分块import torchimport numpy as npx = torch.randn(2, 3)print(x)x = torch.chunk(x, 2, 0)print(x) Tensor进行挤压import torchimport numpy as npx = torch.zeros(2, 1, 2, 1, 2)print(x.size())y = torch.squeeze(x)print(y.size())y = torch.squeeze(x, 0)print(y.size())y = torch.squeeze(x, 1)print(y.size()) Tensor进行扩增import torchimport numpy as npx = torch.zeros(2, 2, 2)print(x.size())y = torch.unsqueeze(x, 0)print(y.size())y = torch.unsqueeze(x, 1)print(y.size()) Tensor进行转置import torchimport numpy as npx = torch.randn(2, 3)print(x)x = torch.transpose(x, 0, 1)print(x) Tensor进行view操作相当于numpy中resize（）的功能，但是用法可能不太一样。 把原先tensor中的数据按照行优先的顺序排成一个一维的数据（这里应该是因为要求地址是连续存储的），然后按照参数组合成其他维度的tensor。比如说是不管你原先的数据是[[[1,2,3],[4,5,6]]]还是[1,2,3,4,5,6]，因为它们排成一维向量都是6个元素，所以只要view后面的参数一致，得到的结果都是一样的。比如，a=torch.Tensor([[[1,2,3],[4,5,6]]])b=torch.Tensor([1,2,3,4,5,6])print(a.view(1,6))print(b.view(1,6)) 得到的结果都是tensor([[1., 2., 3., 4., 5., 6.]])a=torch.Tensor([[[1,2,3],[4,5,6]]])print(a.view(3,2))&apos;&apos;&apos;tensor([[1., 2.], [3., 4.], [5., 6.]])&apos;&apos;&apos; 随机种子import torchimport numpy as nptorch.manual_seed(0)x = torch.randn(2, 3)print(x)torch.manual_seed(0)x = torch.randn(2, 3)print(x) 序列化当提到保存和加载模型时，有三个核心功能需要熟悉：1.torch.save：将序列化的对象保存到disk。这个函数使用Python的pickle实用程序进行序列化。使用这个函数可以保存各种对象的模型、张量和字典。2.torch.load：使用pickle unpickle工具将pickle的对象文件反序列化为内存。3.torch.nn.Module.load_state_dict:使用反序列化状态字典加载model’s参数字典。 什么是state_dictimport torchimport torch.nn as nnimport torch.nn.functional as F# Define modelclass TheModelClass(nn.Module): def __init__(self): super(TheModelClass, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def farward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x# Initialize modelmodel = TheModelClass()# Initialize optimizeroptimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)print(&quot;Model&apos;s state_dict:&quot;)# Print model&apos;s state_dictfor param_tensor in model.state_dict(): print(param_tensor, &quot;\\t&quot;, model.state_dict()[param_tensor].size())print(&quot;optimizer&apos;s state_dict:&quot;)# Print optimizer&apos;s state_dictfor var_name in optimizer.state_dict(): print(var_name, &quot;\\t&quot;, optimizer.state_dict()[var_name]) 保存读取模型# 只保存模型的学习参数torch.save(model.state_dict(), PATH)# 读取模型的可学习参数model = TheModelClass(*args, **kwargs)model.load_state_dict(torch.load(PATH))model.eval() # 保存整个模型torch.save(the_model, PATH)# 读取整个模型the_model = torch.load(PATH)model.eval() # 序列化字典# savetorch.save(&#123; &apos;epoch&apos;: epoch, &apos;model_state_dict&apos;: model.state_dict(), &apos;optimizer_state_dict&apos;: optimizer.state_dict(), &apos;loss&apos;: loss, ... &#125;, PATH) # loadmodel = TheModelClass(*args, **kwargs)optimizer = TheOptimizerClass(*args, **kwargs)checkpoint = torch.load(PATH)model.load_state_dict(checkpoint[&apos;model_state_dict&apos;])optimizer.load_state_dict(checkpoint[&apos;optimizer_state_dict&apos;])epoch = checkpoint[&apos;epoch&apos;]loss = checkpoint[&apos;loss&apos;]model.eval()# - or -model.train() 数学操作夹紧操作import torchimport numpy as npa = torch.randn(4)print(a)a = torch.clamp(a, min=-0.5, max=0.5)print(a) 沿维度计算和import torchimport numpy as npa = torch.randn(10)print(a)a = torch.cumsum(a, dim=0)print(a)print(a[-1]) 计算元素均值import torchimport numpy as npa = torch.randn(2, 2)print(a)b = torch.mean(a)print(b)b = torch.mean(a, 1)print(b) 计算元素标准差import torchimport numpy as npa = torch.randn(2, 2)print(a)b = torch.std(a)print(b)b = torch.std(a, dim=1)print(b) 计算所有元素和import torchimport numpy as npa = torch.randn(2, 2)print(a)b = torch.sum(a)print(b)b = torch.sum(a, dim=0)print(b) torch.nn容器torch.nn.Module是所有网络的基类。你的模型也应该继承这个类。Modules也可以包含其它Modules,允许使用树结构嵌入他们。你可以将子模块赋值给模型属性。import torch.nn as nnimport torch.nn.functional as Fclass Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) # submodule: Conv2d self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) 时序容器# Example of using Sequentialmodel = nn.Sequential( nn.Conv2d(1,20,5), nn.ReLU(), nn.Conv2d(20,64,5), nn.ReLU() ) # Example of using Sequential with OrderedDictmodel = nn.Sequential(OrderedDict([ (&apos;conv1&apos;, nn.Conv2d(1,20,5)), (&apos;relu1&apos;, nn.ReLU()), (&apos;conv2&apos;, nn.Conv2d(20,64,5)), (&apos;relu2&apos;, nn.ReLU()) ])) 卷积层Conv2dclass torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) 参数kernel_size，stride,padding，dilation也可以是一个int的数据，此时卷积height和width值相同;也可以是一个tuple数组，tuple的第一维度表示height的数值，tuple的第二维度表示width的数值。 in_channels(int) – 输入信号的通道 out_channels(int) – 卷积产生的通道 kerner_size(int or tuple) - 卷积核的尺寸 stride(int or tuple, optional) - 卷积步长 padding(int or tuple, optional) - 输入的每一条边补充0的层数 dilation(int or tuple, optional) – 卷积核元素之间的间距 groups(int, optional) – 从输入通道到输出通道的阻塞连接数 bias(bool, optional) - 如果bias=True，添加偏置import torch.nn as nnimport torchm = nn.Conv2d(16, 33, 3, stride=2)m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))input = torch.Tensor(torch.randn(20, 16, 50, 100))output = m(input)print(output) # 空洞卷积import torch.nn as nnimport torchm = nn.Conv2d(16, 33, kernel_size=3, stride=1, padding=2, dilation=2)input = torch.Tensor(torch.randn(20, 16, 100, 100))print(input.size())output = m(input)print(output.size()) ConvTranspose2dclass torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True) 注意，这上面的stride、padding是争对于与原始卷积上的stride和padding2维的转置卷积操作（transposed convolution operator，注意改视作操作可视作解卷积操作，但并不是真正的解卷积操作） 该模块可以看作是Conv2d相对于其输入的梯度，有时（但不正确地）被称为解卷积操作。 由于内核的大小，输入的最后的一些列的数据可能会丢失。因为输入和输出是不是完全的互相关。因此，用户可以进行适当的填充（padding操作）。 in_channels(int) – 输入信号的通道数 out_channels(int) – 卷积产生的通道数 kerner_size(int or tuple) - 卷积核的大小 stride(int or tuple,optional) - 卷积步长 padding(int or tuple, optional) - 输入的每一条边补充0的层数 output_padding(int or tuple, optional) - 输出的每一条边补充0的层数 dilation(int or tuple, optional) – 卷积核元素之间的间距 groups(int, optional) – 从输入通道到输出通道的阻塞连接数 bias(bool, optional) - 如果bias=True，添加偏置import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(20, 16, 64, 64))print(input.size())m = nn.ConvTranspose2d(16, 33, 3, stride=2, padding=1, output_padding=1)output = m(input)print(output.size()) 池化层Maxpool2dclass torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) kernel_size(int or tuple) - max pooling的窗口大小 stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size padding(int or tuple, optional) - 输入的每一条边补充0的层数 dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数 return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助 ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(20, 16, 64, 64))print(input.size())m = nn.MaxPool2d(3, stride=2)output = m(input)print(output.size())input = torch.Tensor(torch.randn(1, 3, 4, 4))print(input)m = nn.MaxPool2d(2, stride=2, return_indices=True)output, indices = m(input)print(indices) MaxUnpool2dclass torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0) Maxpool2d的逆过程，不过并不是完全的逆过程，因为在maxpool2d的过程中，一些最大值的已经丢失。 MaxUnpool2d的输入是MaxPool2d的输出，包括最大值的索引，并计算所有maxpool2d过程中非最大值被设置为零的部分的反向。 kernel_size(int or tuple) - max pooling的窗口大小 stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size padding(int or tuple, optional) - 输入的每一条边补充0的层数import torch.nn as nnimport torchpool = nn.MaxPool2d(2, stride=2, return_indices=True)unpool = nn.MaxUnpool2d(2, stride=2)input = torch.Tensor([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]]])output, indices = pool(input)result = unpool(output, indices)print(result) AvgPool2dclass torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) kernel_size(int or tuple) - 池化窗口大小 stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size padding(int or tuple, optional) - 输入的每一条边补充0的层数 dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数 ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 count_include_pad - 如果等于True，计算平均池化时，将包括padding填充的0import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 3, 9, 9))m = nn.AvgPool2d(3, stride=2)output = m(input)print(output.size()) AdaptiveMaxPool2dclass torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False) 对输入信号，提供2维的自适应最大池化操作 对于任何输入大小的输入，可以将输出尺寸指定为H*W，但是输入和输出特征的数目不会变化。import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 3, 9, 9))m = nn.AdaptiveMaxPool2d((3, 3))output = m(input)print(output.size()) 非线性激活函数ReLUclass torch.nn.ReLU(inplace=False) 对输入运用修正线性单元函数{ReLU}(x)= max(0, x)import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 3, 3, 3))print(input)m = nn.ReLU(inplace=True)output = m(input)print(input)print(output) ELUclass torch.nn.ELU(alpha=1.0, inplace=False) 对输入的每一个元素运用函数f(x) = max(0,x) + min(0, alpha * (e^x - 1))import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 3, 3, 3))print(input)m = nn.ELU(inplace=False)output = m(input)print(output) PReLUclass torch.nn.PReLU(num_parameters=1, init=0.25)对输入的每一个元素运用函数PReLU(x) = max(0,x) + a * min(0,x)a是一个可学习参数。当没有声明时，nn.PReLU()在所有的输入中只有一个参数a；如果是nn.PReLU(nChannels)，a将应用到每个输入。import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 3, 3, 3))print(input)m = nn.PReLU()output = m(input)print(output) LeakyReLUclass torch.nn.LeakyReLU(negative_slope=0.01, inplace=False) 对输入的每一个元素运用f(x) = max(0, x) + {negative_slope} * min(0, x)import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 3, 3, 3))print(input)m = nn.LeakyReLU()output = m(input)print(output) Thresholdclass torch.nn.Threshold(threshold, value, inplace=False) y=x,if x&gt;=threshold y=value,if x&lt;thresholdimport torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 3, 3, 3))print(input)m = nn.Threshold(0.1, 20)output = m(input)print(output) Sigmoidclass torch.nn.Sigmoidimport torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 3, 3, 3))print(input)m = nn.Sigmoid()output = m(input)print(output) Tanhclass torch.nn.Tanhimport torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 3, 3, 3))print(input)m = nn.Tanh()output = m(input)print(output) Softmaxclass torch.nn.Softmax对n维输入张量运用Softmax函数，将张量的每个元素缩放到（0,1）区间且和为1。import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 7))print(input)m = nn.Softmax(dim=1)output = m(input)print(output) 标准化层BatchNorm2dclass torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True) 对小批量(mini-batch)3d数据组成的4d输入进行批标准化(Batch Normalization)操作在每一个小批量（mini-batch）数据中，计算输入各个维度的均值和标准差。gamma与beta是可学习的大小为C的参数向量（C为输入大小）在训练时，该层计算每次输入的均值与方差，并进行移动平均。移动平均默认的动量值为0.1。在验证时，训练求得的均值/方差将用于标准化验证数据。 num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features x height x width’ eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。 momentum： 动态均值和动态方差所使用的动量。默认为0.1。 affine： 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(20, 100, 3, 3))print(input)m = nn.BatchNorm2d(100)output = m(input)print(output) 线性层Linearclass torch.nn.Linear(in_features, out_features, bias=True)对输入数据做线性变换：y=Ax+b in_features - 每个输入样本的大小 out_features - 每个输出样本的大小 bias - 若设置为False，这层不会学习偏置。默认值：Trueimport torch.nn as nnimport torchinput = torch.Tensor(torch.randn(20, 20))print(input.size())m = nn.Linear(20, 30)output = m(input)print(output.size()) Dropout层Dropoutclass torch.nn.Dropout(p=0.5, inplace=False) 随机将输入张量中部分元素设置为0。对于每次前向调用，被置0的元素都是随机的。 p - 将元素置0的概率。默认值：0.5 in-place - 若设置为True，会在原地执行操作。默认值：Falseimport torch.nn as nnimport torchinput = torch.Tensor(torch.randn(3, 6))print(input)m = nn.Dropout(p=0.5)output = m(input)print(output) Droupout2dclass torch.nn.Dropout2d(p=0.5, inplace=False) 随机将输入张量中整个通道设置为0。对于每次前向调用，被置0的通道都是随机的。import torch.nn as nnimport torchinput = torch.Tensor(torch.randn(1, 3, 3, 3))print(input)m = nn.Dropout2d(p=0.5)output = m(input)print(output) 损失函数基本用法：criterion = LossCriterion() #构造函数有自己的参数loss = criterion(x, y) #调用标准时也有参数 计算出来的结果已经对mini-batch取了平均。 MSELossclass torch.nn.MSELoss(size_average=True)创建一个衡量输入x(模型预测输出)和目标y之间均方误差标准。import torch.nn as nnimport torchinput1 = torch.Tensor(torch.randn(1, 8))print(input1)input2 = torch.Tensor(torch.rand(1, 8))print(input2)criterion = nn.MSELoss()loss = criterion(input1, input2)print(loss) CrossEntropyLossclass torch.nn.CrossEntropyLoss(weight=None, size_average=True) 此标准将LogSoftMax和NLLLoss集成到一个类中。不需要再使用softmax。 weight(tensor): 1-D tensor，n个元素，分别代表n类的权重，如果你的训练样本很不均衡的话，是非常有用的。默认值为None。 input : 包含每个类的得分，３-D tensor,shape为 batchn样例数 target: 大小为 n 的 ２-D tensor，包含类别的索引(0到 n-1)，long型。import torch.nn as nnimport torch# batchsize=3, 二分类, 8个样例input = torch.Tensor(torch.randn(2, 3, 3, 3))print(input)# target为long型target = torch.Tensor(torch.zeros(2, 3, 3)).long()target[0, 0, 0] = 2target[1, 1, 1] = 1target[1, 2, 2] = 1print(target)criterion = nn.CrossEntropyLoss()loss = criterion(input, target)print(loss) BCELossclass torch.nn.BCELoss(weight=None, size_average=True) 计算 target 与 output 之间的二进制交叉熵。import torch.nn as nnimport torch# 二进制交叉商，用0和1来表示类别criterion = nn.BCELoss(reduction=&quot;none&quot;)input = torch.Tensor(torch.randn(2, 3, 3))m = nn.Sigmoid()input = m(input)print(input)target = torch.Tensor(torch.ones(2, 3, 3))target[0, 0, 0] = 0target[1, 1, 1] = 0target[1, 2, 2] = 0print(target)loss = criterion(input, target)print(loss.mean()) 视觉函数PixelShuffleclass torch.nn.PixelShuffle(upscale_factor) 将shape为$[N, Cr^2, H, W]$的Tensor重新排列为shape为$[N, C, Hr, W*r]$的Tensor。 当使用stride=1/r 的sub-pixel卷积的时候，这个方法是非常有用的。import torch.nn as nnimport torchps = nn.PixelShuffle(3)input = torch.Tensor(torch.randn(1, 9, 4, 4))print(input.size())output = ps(input)print(output.size()) UpsamplingNearest2dclass torch.nn.UpsamplingNearest2d(size=None, scale_factor=None) 对于多channel 输入 进行 2-D 最近邻上采样。可以通过size或者scale_factor来指定上采样后的图片大小。当给定size时，size的值将会是输出图片的大小。 size (tuple, optional) – 一个包含两个整数的元组 (H_out, W_out)指定了输出的长宽 scale_factor (int, optional) – 长和宽的一个乘子import torch.nn as nnimport torchm = nn.UpsamplingNearest2d(scale_factor=2)input = torch.Tensor(torch.randn(1, 1, 2, 2))print(input)output = m(input)print(output) UpsamplingBilinear2dclass torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None) import torch.nn as nnimport torchm = nn.UpsamplingBilinear2d(scale_factor=2)input = torch.Tensor(torch.randn(1, 1, 2, 2))print(input)output = m(input)print(output) 多GPU模块并行DataParallelclass torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0) 在模块级别上实现数据并行。此容器通过将mini-batch划分到不同的设备上来实现给定module的并行。在forward过程中，module会在每个设备上都复制一遍，每个副本都会处理部分输入。在backward过程中，副本上的梯度会累加到原始module上。batch的大小应该大于所使用的GPU的数量。还应当是GPU个数的整数倍，这样划分出来的每一块都会有相同的样本数量。 module – 要被并行的module device_ids – CUDA设备，默认为所有设备。 output_device – 输出设备（默认为device_ids[0]）net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])output = net(input_var) 优化器为了构建一个Optimizer，你需要给它一个包含了需要优化的参数（必须都是Variable对象）的iterable。然后，你可以设置optimizer的参 数选项，比如学习率，权重衰减，等等。optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)optimizer = optim.Adam([var1, var2], lr = 0.0001) 当我们想指定每一层的学习率时optim.SGD([ &#123;&apos;params&apos;: model.base.parameters()&#125;, &#123;&apos;params&apos;: model.classifier.parameters(), &apos;lr&apos;: 1e-3&#125; ], lr=1e-2, momentum=0.9) 这意味着model.base的参数将会使用1e-2的学习率，model.classifier的参数将会使用1e-3的学习率，并且0.9的momentum将会被用于所有的参数。 所有的optimizer都实现了step()方法，这个方法会更新所有的参数。 optimizer.step()这是大多数optimizer所支持的简化版本。一旦梯度被如backward()之类的函数计算好后，我们就可以调用这个函数。for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() Optimizerclass torch.optim.Optimizer(params, defaults) [source] Base class for all optimizers. params (iterable) —— Variable 或者 dict的iterable。指定了什么参数应当被优化。 defaults —— (dict)：包含了优化选项默认值的字典（一个参数组没有指定的参数选项将会使用默认值）。 Adamclass torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)[source] params (iterable) – 待优化参数的iterable或者是定义了参数组的dict lr (float, 可选) – 学习率（默认：1e-3） betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999） eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8） weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）SGDclass torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)[source] 实现随机梯度下降算法（momentum可选） params (iterable) – 待优化参数的iterable或者是定义了参数组的dict lr (float) – 学习率 momentum (float, 可选) – 动量因子（默认：0） weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认：0） dampening (float, 可选) – 动量的抑制因子（默认：0） nesterov (bool, 可选) – 使用Nesterov动量（默认：False）import torch.optim as optimoptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)optimizer.zero_grad()loss_fn(model(input), target).backward()optimizer.step() 数据集抽象类torch.utils.dataclass torch.utils.data.Dataset表示Dataset的抽象类。所有其他数据集都应该进行子类化。所有子类应该overridelen和getitem，前者提供了数据集的大小，后者支持整数索引，范围从0到len(self)。 DataLoaderclass torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=, pin_memory=False, drop_last=False) 数据加载器。组合数据集和采样器，并在数据集上提供单进程或多进程迭代器。 dataset (Dataset) – 加载数据的数据集。 batch_size (int, optional) – 每个batch加载多少个样本(默认: 1)。 shuffle (bool, optional) – 设置为True时会在每个epoch重新打乱数据(默认: False). sampler (Sampler, optional) – 定义从数据集中提取样本的策略。如果指定，则忽略shuffle参数。 num_workers (int, optional) – 用多少个子进程加载数据。0表示数据将在主进程中加载(默认: 0) collate_fn (callable, optional) – pin_memory (bool, optional) – drop_last (bool, optional) – 如果数据集大小不能被batch size整除，则设置为True后可删除最后一个不完整的batch。如果设为False并且数据集的大小不能被batch size整除，则最后一个batch将更小。(默认: False)torchvisiontorchvision.datasetstorchvision.datasets中包含了以下数据集 MNIST COCO（用于图像标注和目标检测）(Captioning and Detection) LSUN Classification ImageFolder Imagenet-12 CIFAR10 and CIFAR100 STL10 Datasets 拥有以下API:getitem len由于以上Datasets都是 torch.utils.data.Dataset的子类，所以，他们也可以通过torch.utils.data.DataLoader使用多线程（python的多进程）。 MNISTfrom torchvision import datasetsroot = &quot;/home/ltb/MNIST&quot;datasets.MNIST(root, train=True, transform=None, target_transform=None, download=True) 参数说明： root : processed/training.pt 和 processed/test.pt 的主目录 train : True = 训练集, False = 测试集 download : True = 从互联网上下载数据集，并把数据集放在root目录下. 如果数据集之前下载过，将处理过的数据（minist.py中有相关函数）放在processed文件夹下。 COCO需要安装COCO API# 图像标注dset.CocoCaptions(root=&quot;dir where images are&quot;, annFile=&quot;json annotation file&quot;, [transform, target_transform]) from torchvision import datasetsimport torchvision.transforms as transformscap = datasets.CocoCaptions(root=&apos;dir where images are&apos;, annFile=&apos;json annotation file&apos;, transform=transforms.ToTensor())print(&apos;Number of samples: &apos;, len(cap))img, target = cap[3] # load 4th sampleprint(&quot;Image Size: &quot;, img.size())print(target) # 检测dset.CocoDetection(root=&quot;dir where images are&quot;, annFile=&quot;json annotation file&quot;, [transform, target_transform]) ImageFolder一个通用的数据加载器，数据集中的数据以以下方式组织。root/dog/xxx.pngroot/dog/xxy.pngroot/dog/xxz.pngroot/cat/123.pngroot/cat/nsdf3.pngroot/cat/asd932_.png 既其默认你的数据集已经自觉按照要分配的类型分成了不同的文件夹，一种类型的文件夹下面只存放一种类型的图片import torchvision.datasets as dsetdset.ImageFolder(root=&quot;root folder path&quot;, [transform, target_transform])# root ： 指定图片存储的路径，在下面的例子中是&apos;./data/dogcat_2&apos;# transform： 一个函数，原始图片作为输入，返回一个转换后的图片。# target_transform - 一个函数，输入为target，输出对其的转换。例子，输入的是图片标注的string，输出为word的索引。 import torchvision.datasets as dsetdataset = dset.ImageFolder(&apos;./data/dogcat_2&apos;) #没有transform，先看看取得的原始图像数据print(dataset.classes) #根据分的文件夹的名字来确定的类别print(dataset.class_to_idx) #按顺序为这些类别定义索引为0,1...print(dataset.imgs) #返回从所有文件夹中得到的图片的路径以及其类别&apos;&apos;&apos;[&apos;cat&apos;, &apos;dog&apos;]&#123;&apos;cat&apos;: 0, &apos;dog&apos;: 1&#125;[(&apos;./data/dogcat_2/cat/cat.12484.jpg&apos;, 0), (&apos;./data/dogcat_2/cat/cat.12485.jpg&apos;, 0), (&apos;./data/dogcat_2/cat/cat.12486.jpg&apos;, 0), (&apos;./data/dogcat_2/cat/cat.12487.jpg&apos;, 0), (&apos;./data/dogcat_2/dog/dog.12496.jpg&apos;, 1), (&apos;./data/dogcat_2/dog/dog.12497.jpg&apos;, 1), (&apos;./data/dogcat_2/dog/dog.12498.jpg&apos;, 1), (&apos;./data/dogcat_2/dog/dog.12499.jpg&apos;, 1&apos;&apos;&apos; torchvision.modelstorchvision.models模块的 子模块中包含以下模型结构。 AlexNet VGG ResNet SqueezeNet DenseNet 你可以使用随机初始化的权重来创建这些模型。from torchvision import modelsresnet18 = models.resnet18()alexnet = models.alexnet()squeezenet = models.squeezenet1_0()densenet = models.densenet161() 对于ResNet variants和AlexNet，我们也提供了预训练(pre-trained)的模型。import torchvision.models as models# pretrained=True就可以使用预训练的模型resnet18 = models.resnet18(pretrained=True)alexnet = models.alexnet(pretrained=True) tansformCompose将多个transform组合起来使用。transforms： 由transform构成的列表. 例子：from PIL import Imagefrom torchvision import transformstransforms.Compose([ transforms.CenterCrop(10), transforms.ToTensor(), ])&apos;&apos;&apos;### class torchvision.transforms.Scale(size, interpolation=2)将输入的`PIL.Image`重新改变大小成给定的`size`，`size`是最小边的边长。举个例子，如果原图的`height&gt;width`,那么改变大小后的图片大小是`(size*height/width, size)`。**用例:**​```python&apos;&apos;&apos;crop = transforms.Scale(12)img = Image.open(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;)print(type(img))print(img.size)croped_img = crop(img)print(type(croped_img))print(croped_img.size) CenterCropclass torchvision.transforms.CenterCrop(size)将给定的PIL.Image进行中心切割，得到给定的size，size可以是tuple，(target_height, target_width)。size也可以是一个Integer，在这种情况下，切出来的图片的形状是正方形。 RandomCropclass torchvision.transforms.RandomCrop(size, padding=0)切割中心点的位置随机选取。size可以是tuple也可以是Integer。 RandomHorizantalFlipclass torchvision.transforms.RandomHorizontalFlip随机水平翻转给定的PIL.Image,概率为0.5。即：一半的概率翻转，一半的概率不翻转。 RandomSizeCropclass torchvision.transforms.RandomSizedCrop(size, interpolation=2)先将给定的PIL.Image随机切，然后再resize成给定的size大小。 Padclass torchvision.transforms.Pad(padding, fill=0)将给定的PIL.Image的所有边用给定的pad value填充。 padding：要填充多少像素 fill：用什么值填充 例子：from torchvision import transformsfrom PIL import Imagepadding_img = transforms.Pad(padding=10, fill=0)img = Image.open(&apos;/home/ltb/图片/cv2-tutorial/000001.jpg&apos;)print(type(img))print(img.size)padded_img = padding_img(img)print(type(padded_img))print(padded_img.size) Normalizeclass torchvision.transforms.Normalize(mean, std)给定均值：(R,G,B) 方差：（R，G，B），将会把Tensor正则化。即：Normalized_image=(image-mean)/std。 ToTensor与ToPILImage把一个取值范围是[0,255]的PIL.Image或者shape为(H,W,C)的numpy.ndarray，转换成形状为[C,H,W]，取值范围是[0,1.0]的torch.FloadTensorimport numpy as npfrom PIL import Imageimport cv2import torchfrom torchvision import transforms# opencv和PIL读取图片img1 = Image.open(&quot;/home/ltb/图片/cv2-tutorial/000001.jpg&quot;)img2 = cv2.imread(&quot;/home/ltb/图片/cv2-tutorial/000001.jpg&quot;)img1.show()cv2.imshow(&quot;img2&quot;, img2)cv2.waitKey(0)print(img1)print(img2)img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)# ToTensor,转化为tensor,[C, H, W],RGB,[0,1.0]t = transforms.ToTensor()img1_tensor = t(img1)img2_tensor = t(img2)print(img1_tensor)print(&quot;\\n&quot;*10)print(img2_tensor)# tensor转化为PILImaget = transforms.ToPILImage()img1 = t(img1_tensor)img2 = t(img2_tensor)img1.show()img2.show()","path":"2019/08/05/pytorch常用函数/","date":"08-05","excerpt":"","tags":[{"name":"deeplearning","slug":"deeplearning","permalink":"https://litianbo243.github.io/tags/deeplearning/"},{"name":"pytorch","slug":"pytorch","permalink":"https://litianbo243.github.io/tags/pytorch/"}]}]}